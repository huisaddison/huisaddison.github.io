<!doctype html>
<html lang="">	
<head>
	<meta charset="utf-8"/>
	<title>Discussion: Asymptotic Normality and Optimalities in Estimation of Large Gaussian Graphical Models, Part 2 - huisaddison/blog</title>	
	<meta name="author" content="Addison">
	

	<meta name="description" content="Discussion of paper on asymptotic properties of an entrywise estimator for precision matrices, continued.">


	<link rel="top" href="#" /><link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600,300italic,400italic,600italic|Source+Code+Pro' rel='stylesheet' type='text/css'></link>
	<link rel="stylesheet" href="http://huisaddison.com/blog/theme/css/main.css" type="text/css" />
		

</head>
	
<body>

    <div class="container">
	  
	  <header role="banner">
	    <div class="feeds">
	    </div>
		<a href="http://huisaddison.com/blog" class="title">huisaddison/blog</a>
      </header>
	
	  <div class="wrapper">

		  <div role="main" class="content">
	<article class="full">
			
		<h1>Discussion: Asymptotic Normality and Optimalities in Estimation of Large Gaussian Graphical Models, Part 2</h1>
		
<div class="metadata">
  <time datetime="2017-02-11T00:00:00-05:00" pubdate>Sat 11 February 2017</time>
    <address class="vcard author">
      by <a class="url fn" href="http://huisaddison.com/blog/author/addison.html">Addison</a>
    </address>
  in <a href="http://huisaddison.com/blog/category/statistics.html">Statistics</a>
<p class="tags">tagged <a href="http://huisaddison.com/blog/tag/math.html">math</a>, <a href="http://huisaddison.com/blog/tag/stats.html">stats</a>, <a href="http://huisaddison.com/blog/tag/covariance.html">covariance</a>, <a href="http://huisaddison.com/blog/tag/minimax.html">minimax</a>, <a href="http://huisaddison.com/blog/tag/risk.html">risk</a>, <a href="http://huisaddison.com/blog/tag/precision.html">precision</a>, <a href="http://huisaddison.com/blog/tag/asymptotics.html">asymptotics</a>, <a href="http://huisaddison.com/blog/tag/normality.html">normality</a></p></div>		
		<div style="display:none">
    $$
    \newcommand{\norm}[1]{\left\lVert#1\right\rVert}
    \newcommand{\RR}{\mathbf{R}}
    \newcommand{\PP}{\mathbf{P}}
    \newcommand{\EE}{\mathbf{E}}
    \newcommand{\XX}{\mathbf{X}}
    \newcommand{\Nn}{\mathcal{N}}
    \{\Nn}{\mathcal{N}}
    \DeclareMathOperator{\var}{var}
    $$
</div>

<h2>Introduction</h2>
<p>As part of project I've been working on, I'm reading <a href="https://arxiv.org/abs/1309.6024">Asymptotic Normality and
Optimalities in Estimation of Large Gaussian Graphical Models</a>, a paper by Ren, Sun, Zhang, and Zhou.</p>
<p>Motivation, problem setup, and other preliminaries are addressed in a previous
<a href="http://huisaddison.com/blog/discuss-precmat-optimality-pt1.html">blog post</a>.  In this post, we
will begin discussing the statistical inference results of the paper by
walking through the proof for <strong>Theorem 2</strong>, which places bounds on the
distribution of the estimates and makes the results in <strong>Theorem 3</strong> possible.
The results of <strong>Theorem 1</strong> are simply a special case of <strong>Theorem 2</strong>.</p>
<h2>Preliminaries</h2>
<p>First, we recall the definitions of the estimators for the sub-blocks of the
precision and conditional covariance matrices.  We begin with the scaled
lasso regression parameter estimates:<br>
</p>
<div class="math">\begin{align}
\left\{\hat\beta_m, \hat\theta^{1/2}_{mm}\right\}
=
\arg\min_{b\in\RR^{p-2},\\\sigma \in \RR^+}
\left\{
\frac{\norm{\XX_m - \XX_{A^c}b}^2}{2n\sigma}
+ \frac{\sigma}{2} 
+ \lambda\sum_{k\in A^c}\frac{\norm{\XX_k}}{\sqrt{n}}|b_k|
\right\}
\end{align}</div>
<p>
which yield <span class="math">\(\hat\epsilon_A\)</span> as the residual estimates from regression <span class="math">\(\XX_A\)</span>
on <span class="math">\(\XX_{A^c}\)</span>.  Then, we define:
</p>
<div class="math">\begin{align}
\hat\Theta_{A, A}   &amp;=  \frac{\hat\epsilon_A^\top\epsilon_A}{n} \\
\hat\Omega_{A, A}   &amp;=  \hat\Theta_{A, A}^{-1}
\end{align}</div>
<p>We now define the parameter space considered.  For <span class="math">\(\lambda &gt; 0\)</span>, we
defined capped-<span class="math">\(\ell_1\)</span> balls as follows:
</p>
<div class="math">$$
\mathcal{G}^* = \left\{
\Omega: s_\lambda(\Omega) \leq s, M^{-1}
    \leq \lambda_\min(\Omega)
    \leq \lambda_\max(\Omega)
    \leq M
\right\}
$$</div>
<p>
where
</p>
<div class="math">$$
s_\lambda = s_\lambda(\Omega) = \max_j\sum_{i\neq j}
\min\left\{1, \frac{|\omega_{ij}|}{\lambda}\right\}
$$</div>
<p>
for <span class="math">\(\Omega = (\omega_{ij})_1\leq i, j\leq p\)</span>.  The authors take <span class="math">\(\lambda\)</span>
on the order <span class="math">\(\sqrt\frac{\log p}{n}\)</span> in this paper.</p>
<p>Intuitively, this parameter space is a mix of the <span class="math">\(\ell_0\)</span> norm, which
measures sparsity, and the <span class="math">\(\ell_1\)</span> norm, which is the sum of absolute
values, imposed on each row.  For <span class="math">\(\lambda\)</span> very small, we recover the pure
<span class="math">\(\ell_0\)</span> norm; this special case is the parameter space used in <strong>Theorem 1</strong>.</p>
<p>Appealing to the graphical model representation of the multivariate normal
distribution, in which a nonzero entry <span class="math">\(\omega_{ij}\)</span> in the precision matrix
implies the existence of an edge between nodes <span class="math">\(i\)</span> and <span class="math">\(j\)</span>, we may observe that
when <span class="math">\(|\omega_{ij}|\)</span> is zero or larger than <span class="math">\(\lambda\)</span>, <span class="math">\(s_\lambda\)</span> is
equivalent to the maximum node degree of the graph (the degree of each node is
equivalent to the number of nonzero entries on each row, or column).  Finally,
we note that the spectrum (eigenvalues) of the matrix are bounded, a fact upon
which the later analysis relies.</p>
<p>The authors then prove a theorem that gives an error bound on the estimates.
Their approach is to:</p>
<ol>
<li>Compare the estimates to the oracle MLE, giving a concentration bound
    on the distances between them.</li>
<li>Show that
    <div class="math">$$
    \kappa_{ij}^{ora} = \sqrt{n}
    \frac{\omega_{ij}^{ora} - \omega_{ij}}
    {\sqrt{\omega_{ii}\omega_{jj} + \omega_{ij}^2}}
    $$</div>
    is asymptotically standard normal, which implies the oracle MLE is
    asymptotically normal with mean <span class="math">\(\omega_{ij}\)</span> and variance
    <span class="math">\(n^{-1}\sqrt{\omega_{ii}\omega_{jj} + \omega_{ij}^2}\)</span>.</li>
</ol>
<p>By coupling the actual estimator to the oracle MLE and then proving nice
properties for the oracle MLE, we can work towards nice properties for
the actual estimator.</p>
<h2>Statement</h2>
<p><strong>Theorem 2.</strong> <em>Let <span class="math">\(\hat\Theta_{A, A}\)</span> and <span class="math">\(\hat\Omega_{A, A}\)</span> be estimators
defined in (2) and (3) respectively, and <span class="math">\(\lambda = (1 + \varepsilon)
\sqrt\frac{2\delta\log p}{n}\)</span> for any <span class="math">\(\delta \geq 1\)</span> and <span class="math">\(\varepsilon &gt; 0\)</span>
in equation (1).</em></p>
<ol>
<li><em>Suppose <span class="math">\(s \leq \frac{c_0n}{\log p}\)</span> for a sufficiently small constant
    <span class="math">\(c_0 &gt; 0\)</span>.  We have
    <div class="math">\begin{align}
    \max_{G^*(M, s, \lambda)}\max_{A:A=\{i, j\}}
    \PP\left\{
        \norm{\hat\Theta_{A, A} - \Theta_{A, A}^{ora}}_\infty
        &gt; C_1 s \frac{\log p}{n}
    \right\}\leq o\left(p^{-\delta + 1}\right)
    \end{align}</div>
    and
    <div class="math">\begin{align}
    \max_{G^*(M, s, \lambda)}\max_{A:A=\{i, j\}}
    \PP\left\{
        \norm{\hat\Omega_{A, A} - \Omega_{A, A}^{ora}}_\infty
        &gt; C_1' s \frac{\log p}{n}
    \right\}\leq o\left(p^{-\delta + 1}\right)
    \end{align}</div>
    where <span class="math">\(\Theta^{ora}_{A, A}\)</span> and <span class="math">\(\Omega^{ora}_{A, A}\)</span> are the oracle
    estimators and <span class="math">\(C_1\)</span> and <span class="math">\(C_1'\)</span> are positive constants depending on
    <span class="math">\(\{\varepsilon, c_0, M\}\)</span> only.</em></li>
<li><em>There exist constants <span class="math">\(D_1\)</span> and <span class="math">\(\vartheta \in (0, \infty)\)</span>, and three
    marginally standard normal random variables <span class="math">\(Z_{kl}\)</span>, where <span class="math">\(kl = ii, ij,
    jj\)</span>, such that whenever <span class="math">\(|Z_{kl}| \leq \vartheta\sqrt{n}\)</span> for all <span class="math">\(kl\)</span>,
    we have
    <div class="math">\begin{align}
    \left|\kappa_{ij}^{ora} - Z'\right|
    \leq
    \frac{D_1}{\sqrt{n}}\left(1 + Z_{ii}^2 + Z_{ij}^2 + Z_{jj}^2\right)
    \end{align}</div>
    where <span class="math">\(Z' \sim \Nn(0, 1)\)</span>, which can be defined as a linear combination
    of <span class="math">\(Z_{kl}\)</span>.</em></li>
</ol>
<p>Intuitively, statement (1) says that there is a very low probability that
the maximum entrywise deviation of the actual estimator from the oracle MLE
is larger than a constant that we can control.  Statement (2) says that
the rescaled oracle MLE behaves more or less asymptotically normally.</p>
<h2>Proof for Theorem 2.1</h2>
<p>First, we present the following lemma:</p>
<p><strong>Lemma 2.</strong> <em>Let <span class="math">\(\lambda = (1 + \varepsilon)\sqrt\frac{2\delta\log p}{n}\)</span> for
any <span class="math">\(\delta \geq 1\)</span> and <span class="math">\(\varepsilon &gt; 0\)</span>.  Define the event <span class="math">\(E_m\)</span> as follows:
<div class="math">\begin{align}
\left|
\hat\theta_{mm} - \theta^{ora}_{mm} \right|
    &amp;\leq   C_1'\lambda^2s  \\
\norm{\beta_m - \hat\beta_m}_1
    &amp;\leq C_2'\lambda s   \\
n^{-1}\norm{\XX_{A^c}\left(\beta_m - \hat\beta_m\right)}^2
    &amp;\leq C_3'\lambda^2 s   \\
\norm{n^{-1}\XX_{A^c}^\top\epsilon_m}_\infty
    &amp;\leq C_4'\lambda
\end{align}</div>
for <span class="math">\(m \in \{i, j\}\)</span> and some constants <span class="math">\(C_k', 1 \leq k \leq 4\)</span>.  Under
the assumptions of Theorem 2, we have:</em>
</p>
<div class="math">$$
\PP\left\{E_m^c\right\} \leq o\left(p^{-\delta + 1}\right)
$$</div>
<p>The proof of <strong>Lemma 2</strong> is deferred to a future post.</p>
<p>First, we consider <span class="math">\(\theta_{ii}^{ora}\)</span> and <span class="math">\(\theta_{jj}^{ora}\)</span>.  By the
definition of <span class="math">\(\lambda\)</span> and Equation (7) of <strong>Lemma 2</strong>, the concentration
bound on the deviations in (4) holds for <span class="math">\(\theta_{ii}^{ora},
\theta_{ij}^{ora}\)</span>.  We then consider the event <span class="math">\(E_i \cap E_j\)</span>:</p>
<div class="math">\begin{align*}
\left|\hat\theta_{ij} - \theta^{ora}_{ij}\right|
&amp;=  \left|
    \frac{\hat\epsilon_i^\top\hat\epsilon_j}{n}
    - 
    \frac{\epsilon_i^\top\epsilon_j}{n}
    \right|
\end{align*}</div>
<p>(To be continued!)</p>
<h2>Proof for Theorem 2.2</h2>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>	

	</article>

    <p>
	<a href="https://twitter.com/share" class="twitter-share-button" data-via="" data-lang="en" data-size="large" data-related="">Tweet</a>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
	</p>


		  </div>	
		  
		  <div class="sidebar">

	        <nav>
	          <h2>Categories</h2>
	          <ul>
	              <li ><a href="http://huisaddison.com/blog/category/pelican.html">Pelican</a></li>
	              <li ><a href="http://huisaddison.com/blog/category/photography.html">Photography</a></li>
	              <li ><a href="http://huisaddison.com/blog/category/python.html">Python</a></li>
	              <li class="active"><a href="http://huisaddison.com/blog/category/statistics.html">Statistics</a></li>
	          </ul>
	        </nav>



		  </div>

	  </div>

      <footer>
		<p role="contentinfo">
        © 2017 Addison - Proudly powered by <a href="https://github.com/getpelican/pelican">pelican</a>.  
        <a href="https://github.com/huisaddison/blog-theme">Theme</a> derived from <a href="https://github.com/fle/pelican-simplegrey">pelican-simplegrey</a>.  
        Math rendered beautifully by <a href="https://www.mathjax.org/">MathJax</a>.
    	</p>

	  </footer>	

	</div>
	

</body>
</html>