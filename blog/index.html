<!doctype html>
<html lang="">	
<head>
	<meta charset="utf-8"/>
	<title>huisaddison/blog</title>	
	<meta name="author" content="Addison">
	

	<link rel="top" href="#" /><link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600,300italic,400italic,600italic|Source+Code+Pro' rel='stylesheet' type='text/css'></link>
	<link rel="stylesheet" href="http://huisaddison.com/blog/theme/css/main.css" type="text/css" />
		

</head>
	
<body>

    <div class="container">
	  
	  <header role="banner">
	    <div class="feeds">
	    </div>
		<a href="http://huisaddison.com/blog" class="title">huisaddison/blog</a>
      </header>
	
	  <div class="wrapper">

		  <div role="main" class="content">



      <article>
        <h1><a href="http://huisaddison.com/blog/discuss-covmat-optimal-convergence-pt2.html">Discussion: Optimal Rates of Convergence for Covariance Matrix Estimation, Part 2</a></h1>
        <p>In this post, I will continue my discussion of <a href="https://arxiv.org/abs/1010.3866">Optimal Rates of Convergence
for Covariance Matrix Estimation</a>.  A
discussion of their first result (showing an upper bound on the risk of their
proposed tapering estimator) can be found in a <a href="http://huisaddison.com/blog/discuss-covmat-optimal-convergence-pt1.html">previous
post</a>.</p>
<h2>Minimax Lower Bound</h2>
<p>We will review their proof of a minimax lower bound among all estimators that
matches the upper bound previously proved for their tapering estimator.</p>
<p><strong>Theorem 3.</strong>  <em>Suppose <span class="math">\(p \leq \exp(\gamma n)\)</span> for some constant
<span class="math">\(\gamma &gt; 0\)</span>.  The minimax risk for estimating the covariance matrix
<span class="math">\(\Sigma\)</span> over <span class="math">\(\mathcal{P}_\alpha\)</span> under the operator norm satisfies</em>:
</p>
<div class="math">\begin{align*}
  \inf_{\hat\Sigma}\sup_{\mathcal{P}_\alpha}
  \mathbf{E}\Vert\hat\Sigma - \Sigma\Vert^2
  \geq
  cn^{-\frac{2\alpha}{2\alpha + 1}} + c\frac{\log p}{n} 
\end{align*}</div>
<p>In the words of the authors,</p>
<blockquote>
<p><em>The basic strategy underlying the proof of Theorem 3 is to carefully
  construct a finite collection of multivariate normal distributions and
  calculate the total variation affinity between pairs of probability measures
  in the collection.</em></p>
</blockquote>
<h2>Parameter Space Specification</h2>
<p>Oftentimes, the proof of a minimax lower bound is accompanied with the
specification of a smaller parameter space that is easier to analyze.
Intuitively, the restriction of the parameters to a subspace is permissible as
for <span class="math">\(\mathcal{F}'\subset \mathcal{F}\)</span>:
</p>
<div class="math">\begin{align*}
    \sup_{\Sigma\in\mathcal{F}'} R(\Sigma, \hat\Sigma)
    \leq
    \sup_{\Sigma\in\mathcal{F}} R(\Sigma, \hat\Sigma)
\end{align*}</div>
<p>
for all <span class="math">\(\hat\Sigma\)</span>.  It follows, then, that:
</p>
<div class="math">\begin{align*}
    \inf_{\hat\Sigma}\sup_{\Sigma\in\mathcal{F}'} R(\Sigma, \hat\Sigma)
    \leq
    \inf_{\hat\Sigma}\sup_{\Sigma\in\mathcal{F}} R(\Sigma, \hat\Sigma)
\end{align*}</div>
<p>For positive integers <span class="math">\(k, m\)</span> such that <span class="math">\(2k \leq p\)</span> and <span class="math">\(1 \leq m \leq k\)</span>, we
define the <span class="math">\(p \times p\)</span> matrix <span class="math">\(B(m, k) = (b_{ij})_{p\times{p}}\)</span> such that:
</p>
<div class="math">$$
b_{ij} = \mathbf{1} \{i = m \text{ and } m+1 \leq j\leq 2k, \text{ or } 
            j = m \text{ and } m+1 \leq i \leq 2k\}
$$</div>
<p>Setting <span class="math">\(k = n^\frac{1}{2\alpha + 1}\)</span> and <span class="math">\(a = k^{-\alpha-1}\)</span>, we then define
a collection of <span class="math">\(2^k\)</span> covariance matrices:
</p>
<div class="math">$$
\mathcal{F}_{11} = \left\{
    \Sigma(\theta):
    \Sigma(\theta) = \mathbf{I}_{p\times p}
        + \tau a\sum_{m=1}^k \theta_m B(m, k),
    \quad\theta = (\theta_m) \in \{0, 1\}^k
\right\}
$$</div>
<p>
where <span class="math">\(0 &lt; \tau &lt; 2^{-\alpha - 1}M\)</span>.  We can interpret this parameter 
space intuitively.  First, we generate a set of <span class="math">\(k\)</span> matrices <span class="math">\(B(m, k)\)</span>,
<span class="math">\(m \in [k]\)</span>, in which <span class="math">\(B(m, k)\)</span> is nonzero only on certain elements
along row or column <span class="math">\(m\)</span>.  Then, for each <span class="math">\(\theta = \{0, 1\}^k\)</span>, we add
a number of <span class="math">\(\tau a\)</span> perturbations to the identity matrix; where
we add <span class="math">\(\tau a B(m, k)\)</span> if <span class="math">\(\theta_m \neq 0\)</span>.  This gives us a set
of <span class="math">\(\Sigma(\theta)\)</span> of size <span class="math">\(2^k\)</span>.</p>
<p>We now verify that <span class="math">\(\mathcal{F}_{11} \subset \mathcal{F}(M, M_0)\)</span>.  Recall
the definition:
</p>
<div class="math">$$
\mathcal{F}(M_0, M) = \left\{
\Sigma: \max_j \sum_j \{|\sigma_{ij}|: |i-j| &gt; k\} \leq M k^{-\alpha}
\text{ for all } k, \text{and} \lambda_{\text{max}}(\Sigma)\leq M_0
\right\}
$$</div>
<p>
Let us consider the <span class="math">\(\Sigma(\theta)\)</span> where <span class="math">\(\theta\)</span> is a vector of 
<span class="math">\(k\)</span> ones and <span class="math">\(\tau = 2^{-\alpha - 1}M\)</span>.  Then
</p>
<div class="math">$$
\Sigma(\theta) = \mathbf{I}_{p\times p}
    + M (k + 2)^{-\alpha -1}\sum_{m=1}^k B(m, k)
$$</div>
<p>
We observe that <span class="math">\(\sum_{m=1}^k B(m, k)\)</span> is a matrix of ones on the
off-diagonals up to row and column <span class="math">\(2k \leq p\)</span>.  Therefore, the worst-case
sum of entries more than <span class="math">\(k\)</span> away from the diagonal is:
</p>
<div class="math">\begin{align*}
    \frac{k}{2} M k^{-\alpha - 1} 2^{- \alpha - 1}
    &amp;=  M k^{-\alpha} 2^{- \alpha - 2}  \\
    &amp;\leq M k^{-\alpha}
\end{align*}</div>
<p>
Without loss of generality the authors assume <span class="math">\(M_0 &gt; 1\)</span> and <span class="math">\(\rho &gt; 1\)</span>; 
otherwise we may replace <span class="math">\(\II_{p\times p}\)</span> with <span class="math">\(\varepsilon \II_{
p \times p}\)</span> with <span class="math">\(\varepsilon \in (0, \min\{M_0, \rho\})\)</span>.</p>
<p>To be continued!</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
      </article>

      <hr />

        <div>
          <h3>Last posts</h3>
          <ol class="archive">
    


      <li>
<a href="http://huisaddison.com/blog/discuss-covmat-optimal-convergence-pt1.html" rel="bookmark" title="Permalink to Discussion: Optimal Rates of Convergence for Covariance Matrix Estimation, Part 1">Discussion: Optimal Rates of Convergence for Covariance Matrix Estimation, Part 1</a>
  <time datetime="2017-01-26T00:00:00-05:00" pubdate>Thu 26 January 2017</time>
<p class="tags">tags: <a href="http://huisaddison.com/blog/tag/math.html">math</a>, <a href="http://huisaddison.com/blog/tag/stats.html">stats</a>, <a href="http://huisaddison.com/blog/tag/covariance.html">covariance</a>, <a href="http://huisaddison.com/blog/tag/minimax.html">minimax</a>, <a href="http://huisaddison.com/blog/tag/risk.html">risk</a></p></a>      </li>


      <li>
<a href="http://huisaddison.com/blog/my-first-post.html" rel="bookmark" title="Permalink to Post One">Post One</a>
  <time datetime="2017-01-25T00:00:00-05:00" pubdate>Wed 25 January 2017</time>
<p class="tags">tags: <a href="http://huisaddison.com/blog/tag/pelican.html">pelican</a>, <a href="http://huisaddison.com/blog/tag/python.html">python</a></p></a>      </li>


      <li>
<a href="http://huisaddison.com/blog/trying-out-mathjax.html" rel="bookmark" title="Permalink to Trying Out MathJax">Trying Out MathJax</a>
  <time datetime="2017-01-25T00:00:00-05:00" pubdate>Wed 25 January 2017</time>
<p class="tags">tags: <a href="http://huisaddison.com/blog/tag/pelican.html">pelican</a>, <a href="http://huisaddison.com/blog/tag/mathjax.html">mathjax</a>, <a href="http://huisaddison.com/blog/tag/math.html">math</a></p></a>      </li>


          </ol>
        </div>

		  </div>	
		  
		  <div class="sidebar">

	        <nav>
	          <h2>Categories</h2>
	          <ul>
	              <li ><a href="http://huisaddison.com/blog/category/pelican.html">Pelican</a></li>
	              <li ><a href="http://huisaddison.com/blog/category/python.html">Python</a></li>
	              <li ><a href="http://huisaddison.com/blog/category/statistics.html">Statistics</a></li>
	          </ul>
	        </nav>



		  </div>

	  </div>

      <footer>
		<p role="contentinfo">
        © 2017 Addison - Proudly powered by <a href="https://github.com/getpelican/pelican">pelican</a>.  
        <a href="https://github.com/huisaddison/blog-theme">Theme</a> derived from <a href="https://github.com/fle/pelican-simplegrey">pelican-simplegrey</a>.  
        Math rendered beautifully by <a href="https://www.mathjax.org/">MathJax</a>.
    	</p>

	  </footer>	

	</div>
	

</body>
</html>