var tipuesearch = {"pages":[{"title":"Fat Salmon","text":"I am by no means a strong swimmer. I had taken lessons as a child (thank you mom and dad), but refused to join the team in high school (sorry mom and dad). I got back into it a few years ago, when I began doing laps on my own to stay in shape, but my technique was still rather poor. So, when I moved to Seattle and saw that the Downtown Y offered a \"Fitness Swim\" class led by an instructor, I figured, why not give it a shot? The first day I attended Fit Swim, exactly a year ago, I was the sole participant. The instructor at the time informed me that most everyone else was at the Fat Salmon, a 5k swim in Lake Washington. I laughed at the idea of swimming a 5k. A year has now passed, and I still think the idea of swimming a 5k is ridiculous, even though I did it myself yesterday. I signed up on a whim two months ago, banking on the idea that forcing myself into the reality of participating would motivate me to improve. Apart from the distance being much farther than I'd ever swam and the fact that I hadn't participated in any open water races at that point, there existed a (loosely enforced) two-hour time limit, imposed by the organizers of the Fat Salmon in the interest of safety (in the last few years, swimmers have come in on as late as 2 hours 21 minutes). Those straggling too far behind are pulled onto a boat by the race organizers. Immediately after signing up, I doubted myself. In fact, I hesitated to inform my teammates that I had registered, reserving the right to not participate if I didn't feel ready. This stemmed from my fear of failure; a fear that is shared, I think, by many young people who are accustomed to being relatively successful in their endeavors. For me personally, I felt it preferable to not try at all than to risk the embarrassment of being pulled from the race. Absurd, right? Even though it makes little sense from an \"logical\" viewpoint to handicap myself by not trying, it's still a fear that's not always easy for me to get over. As it turned out, the participant roster was public, so my teammates knew I had registered regardless. The pressure was on. I went to my first lake practice and found the water so cold and the darkness so disorienting that for the first twenty minutes I couldn't keep my head in the water. Fortunately I was accompanied by John, an excellent swimmer and exceptional friend. The weeks passed, and I gradually improved. Two weeks before the race, our team organized a practice run of the course, and I came in at 2 hours and 17 minutes. It wasn't a great result, but knowing that I could swim the entire course calmed my nerves. For the week leading up to the Fat Salmon, I tried being too excited to be nervous about the race. That worked until the day before, when all the nerves came home to roost. I told myself that it would be great if I finished, and if I didn't, it would prove that there's no harm in trying—the harm is in not trying. In the end, I finished at just over two hours. I couldn't have done it without my team, in particular John and Juli, whose moral support carried me through; nor without Kathleen, whose coaching has done wonders for my stroke. Now, I need to find another goal that is just out of grasp, and possibly fail at that.","tags":"Personal","url":"http://huisaddison.com/blog/fat-salmon.html"},{"title":"Cycling!","text":"I recently picked up a used bicycle off Craigslist. It's a beautiful steel Schwinn from the 1980s. I take it out on joyrides pretty often, and it's an indispensable part of my daily commute now. Prior to this I've never cycled regularly. Riding along Burke Gilman My favorite ride to take on a weekend afternoon is the Burke Gilman from Fremont to its end in Bothell. 192 Brewing lies along the trail, near Kenmore. It's a nice place to have a drink and relax for a while before heading home. Sunday afternoons feature live blues. Riding along the Ship Canal Trail For a nice, quick ride after work, I head in the opposite direction, along the Ship Canal Trail towards Discovery Park. A fantastic viewpoint exists there, by the Daybreak Star Indian Cultural Center. The area is almost always unoccupied. It's particularly nice for clearing one's head. The first time I stopped by, I was reminded of a line from my favorite children's book series. \"The world is quiet here.\"","tags":"Personal","url":"http://huisaddison.com/blog/cycling-seattle.html"},{"title":"Snow Lake Hike","text":"I recently went on a hike with some friends to Snow Lake in Snolquamie Pass. It was a wonderful time. We started early and made it to Snow Lake before it became crowded. The water was clear (the smoky skies not so much) and we jumped into the water for a quick swim (it would have been nice to stay longer, but the lake was sixty degrees Fahrenheit at most, and not particularly inviting). From there, we hiked the extra mile or so to Gem Lake and made our way back. This photograph, my favorite, is of Gem Lake. Here are the other pictures from the hike. These photographs were taken with a Ricoh GR II -- the film photography habit, as one may have guessed, required too much dedication to stick :P","tags":"Photography","url":"http://huisaddison.com/blog/snow-lake-hike.html"},{"title":"Toying with Fonts in LaTeX","text":"I typeset most of my documents using some program built atop TeX. For documents where fine control over formatting is not necessarily (e.g., blog posts), I use Markdown instead. On occasion, I use LibreOffice to type out a quick sign. I'm particularly fond of LaTeX and its family because it does a fine job of separating content creation from layout, and because math is rendered beautifully (though Markdown compiled to HTML with MathJax does very well too). Up until now, I've used pdflatex to compile my tex files, but recently I've started using xelatex instead, in some cases. Fonts in LaTeX Aside from some pre-installed fonts obtained through a distribution of LaTeX, there are not many options for fonts when using pdflatex. Typically this would not be a problem, as the default font family (Computer Modern), which was last updated in 1992 1 , is quite nice. In some cases, though, it would be nice to be able to add some personal flair. fontspec I use the moderncv package to typeset my resume, but found the default sans serif font to be a bit bland. I decided to replace the section headers with Bitter and the body text with Raleway by using the fontspec package. Defining the fonts could not be easier: \\usepackage{fontspec} \\newfontfamily\\bitter[Path=fonts/Bitter/, Ligatures=TeX]{Bitter-Regular} \\newfontfamily\\raleway[Path=fonts/Raleway/, Ligatures=TeX]{Raleway-Regular} After defining the font families, I set the body font to Raleway: \\setsansfont[Path=fonts/Raleway/, BoldFont=Raleway-Bold, ItalicFont=Raleway-Italic, BoldItalicFont=Raleway-BoldItalic, Ligatures=TeX]{Raleway-Regular} and finally, set the rest of the fonts to Bitter: \\renewcommand*{\\namefont}{\\bitter\\fontsize{34}{36}\\mdseries\\upshape} \\renewcommand*{\\titlefont}{\\bitter\\LARGE\\mdseries\\slshape} \\renewcommand*{\\addressfont}{\\bitter\\small\\mdseries\\slshape} \\renewcommand*{\\quotefont}{\\bitter\\large\\slshape} \\renewcommand*{\\sectionfont}{\\bitter\\Large\\mdseries\\upshape} \\renewcommand*{\\subsectionfont}{\\bitter\\large\\mdseries\\upshape} XeLaTeX The only catch is that the fontspec package is not available when using pdflatex; the only catch is that one must use xelatex instead. xetex has a few advantages over pdftex 2 : xetex assumes the input is unicode, so characters with accents and other marks can be inserted directly into the tex source. As seen in the example above, xetex is able to use fonts located on the sytem without any issues. For now, I use xetex to typeset my resume and slides ; for other documents, I stick with pdftex. LuaTeX It's worth mentioning that LuaTeX is the anointed successor to pdftex, so I will probably be using that at some point in my life. It has ambitious goals, like introducing Lua to TeX to allow scripting within the document 3 . I haven't tried it yet as xetex does everything I need for now. http://www-cs-faculty.stanford.edu/~uno/cm.html ↩ http://tex.stackexchange.com/questions/3393/ ↩ http://tex.stackexchange.com/questions/36/ ↩","tags":"LaTeX","url":"http://huisaddison.com/blog/fonts-in-latex.html"},{"title":"Mill's Inequality","text":"$$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\one}{\\mathbf{1}} \\newcommand{\\RR}{\\mathbf{R}} \\newcommand{\\DD}{\\mathbf{D}} \\newcommand{\\PP}{\\mathbf{P}} \\newcommand{\\EE}{\\mathbf{E}} \\newcommand{\\XX}{\\mathbf{X}} \\newcommand{\\Nn}{\\mathcal{N}} \\newcommand{\\Gg}{\\mathcal{G}} \\newcommand{\\la}{\\langle} \\newcommand{\\ra}{\\rangle} \\DeclareMathOperator{\\var}{var} \\DeclareMathOperator{\\diag}{diag} \\DeclareMathOperator{\\cov}{cov} $$ Background One of the results in the paper I've been reading uses a tail bound on the max of Gaussian-distributed random variables that I was not too familiar with, so I thought I'd present and discuss it here to solidify my understanding of it. Statement Suppose \\(Z \\sim \\Nn(0, \\sigma&#94;2)\\) . Then we have the tail bound: $$ \\PP\\{|Z| > t\\} \\leq \\sqrt\\frac{2}{\\pi}\\frac{\\sigma}{t} \\exp\\left\\{ -\\frac{t&#94;2}{2\\sigma&#94;2} \\right\\} $$ Intuitively, this kind of tail bound is useful because we can get exponentially-fast decay without calculating the distribution function directly. Proof The broad strokes of the proof follow Aliyah Ahmed's response to a post on StackExchange . We begin by observing that density of \\(Z\\) is symmetric about the origin, therefore: \\begin{align*} \\PP\\{|Z| > t\\} &= 2 \\PP\\{Z > t\\} \\end{align*} We then observe that by playing with distribution functions and expectations, we get the following upper bound: \\begin{align*} t\\cdot \\PP\\{Z > t\\} &= t\\int_t&#94;\\infty dF(x) \\\\ &\\leq \\int_t&#94;\\infty x d F(x) \\\\ &= \\int_t&#94;\\infty x \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ -\\frac{x&#94;2}{2\\sigma&#94;2} \\right\\} \\\\ &= \\frac{\\sigma}{\\sqrt{2\\pi}}\\exp\\left\\{ -\\frac{t&#94;2}{2\\sigma&#94;2} \\right\\} \\end{align*} in the process using sneaky way to introduce a quantity that has a nice, clean closed-form integral. Closer examination shows that this is in fact a tighter version of Markov's Inequality; rather than taking \\(\\EE X\\) , we take \\(\\EE [X \\one\\{X > t\\}]\\) . This implies that: \\begin{align*} \\PP\\{Z > t\\} &= \\frac{\\sigma}{t\\sqrt{2\\pi}}\\exp\\left\\{ -\\frac{t&#94;2}{2\\sigma&#94;2} \\right\\} \\\\ \\Rightarrow \\PP\\{|Z| > t\\} &= \\sqrt\\frac{2}{\\pi} \\frac{\\sigma}{t}\\exp\\left\\{ -\\frac{t&#94;2}{2\\sigma&#94;2} \\right\\} \\end{align*} Extension to Sum of Random Variables This result can be extended to the maximum of \\(m\\) Gaussian random variables by way of the union bound. Suppose \\(\\{Z_i\\}_{i=1}&#94;m \\sim \\Nn(0, \\sigma&#94;2)\\) . Then the union bound implies: $$ \\PP\\left\\{ \\max_{1\\leq i\\leq m} |Z_i| > t \\right\\} \\leq m\\cdot \\sqrt\\frac{2}{\\pi} \\frac{\\sigma}{t}\\exp\\left\\{ -\\frac{t&#94;2}{2\\sigma&#94;2} \\right\\} $$ Suppose the variance of these random variables decreased with \\(n\\) , i.e., \\(\\sigma&#94;2 = \\frac{1}{n}\\) . This could happen if our \\(Z_i\\) are estimators. Then we would have the bound: \\begin{align*} \\PP\\left\\{ \\max_{1\\leq i\\leq m} |Z_i| > t \\right\\} &\\leq m\\cdot \\sqrt\\frac{2}{\\pi} \\frac{1}{\\sqrt{n}t}\\exp\\left\\{ -\\frac{nt&#94;2}{2} \\right\\} \\\\ &\\leq \\sqrt\\frac{2}{\\pi} \\frac{1}{\\sqrt{n}t}\\exp\\left\\{ -\\frac{nt&#94;2}{2} + \\log m \\right\\} \\end{align*} Suppose we wanted to get a reasonably large probability on the right hand side so that our bound is useful. A trivial way to do this is to take \\(t\\) very small, but this upper bound would be meaningless. What if we made \\(t\\) arbitrarily large? The implication in this case is not particularly useful either. To balance between these interests, we can choose \\(t\\) such that: $$ \\frac{nt&#94;2}{2} = \\log m $$ giving us a bound that adapts to \\(m\\) . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"http://huisaddison.com/blog/mills-inequality.html"},{"title":"Discussion: Asymptotic Normality and Optimalities in Estimation of Large Gaussian Graphical Models, Part 2","text":"$$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\RR}{\\mathbf{R}} \\newcommand{\\DD}{\\mathbf{D}} \\newcommand{\\PP}{\\mathbf{P}} \\newcommand{\\EE}{\\mathbf{E}} \\newcommand{\\XX}{\\mathbf{X}} \\newcommand{\\Nn}{\\mathcal{N}} \\newcommand{\\Gg}{\\mathcal{G}} \\newcommand{\\la}{\\langle} \\newcommand{\\ra}{\\rangle} \\DeclareMathOperator{\\var}{var} \\DeclareMathOperator{\\diag}{diag} \\DeclareMathOperator{\\cov}{cov} $$ Introduction As part of project I've been working on, I'm reading Asymptotic Normality and Optimalities in Estimation of Large Gaussian Graphical Models , a paper by Ren, Sun, Zhang, and Zhou. Motivation, problem setup, and other preliminaries are addressed in a previous blog post . In this post, we will begin discussing the statistical inference results of the paper by walking through the proof for Theorem 2 , which places bounds on the distribution of the estimates and makes the results in Theorem 3 possible. The results of Theorem 1 are simply a special case of Theorem 2 . Preliminaries First, we recall the definitions of the estimators for the sub-blocks of the precision and conditional covariance matrices. We begin with the scaled lasso regression parameter estimates: \\begin{align} \\left\\{\\hat\\beta_m, \\hat\\theta&#94;{1/2}_{mm}\\right\\} = \\arg\\min_{b\\in\\RR&#94;{p-2},\\\\\\sigma \\in \\RR&#94;+} \\left\\{ \\frac{\\norm{\\XX_m - \\XX_{A&#94;c}b}&#94;2}{2n\\sigma} + \\frac{\\sigma}{2} + \\lambda\\sum_{k\\in A&#94;c}\\frac{\\norm{\\XX_k}}{\\sqrt{n}}|b_k| \\right\\} \\end{align} which yield \\(\\hat\\epsilon_A\\) as the residual estimates from regression \\(\\XX_A\\) on \\(\\XX_{A&#94;c}\\) . Then, we define: \\begin{align} \\hat\\Theta_{A, A} &= \\frac{\\hat\\epsilon_A&#94;\\top\\hat\\epsilon_A}{n} \\\\ \\hat\\Omega_{A, A} &= \\hat\\Theta_{A, A}&#94;{-1} \\end{align} We now define the parameter space considered. For \\(\\lambda > 0\\) , we defined capped- \\(\\ell_1\\) balls as follows: $$ \\mathcal{G}&#94;* = \\left\\{ \\Omega: s_\\lambda(\\Omega) \\leq s, M&#94;{-1} \\leq \\lambda_\\min(\\Omega) \\leq \\lambda_\\max(\\Omega) \\leq M \\right\\} $$ where $$ s_\\lambda = s_\\lambda(\\Omega) = \\max_j\\sum_{i\\neq j} \\min\\left\\{1, \\frac{|\\omega_{ij}|}{\\lambda}\\right\\} $$ for \\(\\Omega = (\\omega_{ij})_{1\\leq i, j\\leq p}\\) . The authors take \\(\\lambda\\) on the order \\(\\sqrt\\frac{\\log p}{n}\\) in this paper. Intuitively, this parameter space is a mix of the \\(\\ell_0\\) norm, which measures sparsity, and the \\(\\ell_1\\) norm, which is the sum of absolute values, imposed on each row. For \\(\\lambda\\) very small, we recover the pure \\(\\ell_0\\) norm; this special case is the parameter space used in Theorem 1 . Appealing to the graphical model representation of the multivariate normal distribution, in which a nonzero entry \\(\\omega_{ij}\\) in the precision matrix implies the existence of an edge between nodes \\(i\\) and \\(j\\) , we may observe that when \\(|\\omega_{ij}|\\) is zero or larger than \\(\\lambda\\) , \\(s_\\lambda\\) is equivalent to the maximum node degree of the graph (the degree of each node is equivalent to the number of nonzero entries on each row, or column). Finally, we note that the spectrum (eigenvalues) of the matrix are bounded, a fact upon which the later analysis relies. The authors then prove a theorem that gives an error bound on the estimates. Their approach is to: Compare the estimates to the oracle MLE, giving a concentration bound on the distances between them. Show that $$ \\kappa_{ij}&#94;{ora} = \\sqrt{n} \\frac{\\omega_{ij}&#94;{ora} - \\omega_{ij}} {\\sqrt{\\omega_{ii}\\omega_{jj} + \\omega_{ij}&#94;2}} $$ is asymptotically standard normal, which implies the oracle MLE is asymptotically normal with mean \\(\\omega_{ij}\\) and variance \\(n&#94;{-1}\\sqrt{\\omega_{ii}\\omega_{jj} + \\omega_{ij}&#94;2}\\) . By coupling the actual estimator to the oracle MLE and then proving nice properties for the oracle MLE, we can work towards nice properties for the actual estimator. First, we state a few conditions, which will be useful in our analysis of Theorem 2 . When these conditions hold for certain fixed constant \\(C_0\\) , \\(\\varepsilon_\\Omega \\rightarrow 0\\) , and all \\(\\delta \\geq 1\\) , the asymptotic normality and efficiency properties will hold, as we will see in the analysis of Theorem 2. The first condition is: \\begin{align} \\max_{A: A = \\{i, j\\}} \\PP\\left\\{ \\norm{\\XX_{A&#94;c}\\left(\\hat\\beta_{A&#94;c, A} - \\beta_{A&#94;c, A}\\right)}&#94;2 \\geq C_0 s \\delta\\log p \\right\\} \\leq p&#94;{-\\delta + 1}\\varepsilon_\\Omega \\end{align} Observing that \\(\\XX_{A&#94;c}\\left(\\hat\\beta_{A&#94;c, A} - \\beta_{A&#94;c, A}\\right)\\) is equivalent to \\(\\norm{\\epsilon_A - \\hat\\epsilon_A}&#94;2\\) , we may interpret this as a concentration bound on the deviation of the residual estimates from the oracle residuals. The next condition is: \\begin{align} \\max_{A:A = \\{i, j\\}} \\PP\\left\\{ \\norm{ \\bar\\DD&#94;\\frac{1}{2}_{A&#94;c} \\left(\\hat\\beta_{A&#94;c, A} - \\beta_{A&#94;c, A}\\right) }_1 \\geq C_0 s \\sqrt{\\delta\\frac{\\log p}{n}} \\right\\} \\leq p&#94;{-\\delta+1}\\varepsilon_\\Omega \\end{align} with \\(\\bar \\DD = \\diag\\left(\\frac{\\XX&#94;\\top\\XX}{n}\\right)\\) . These two statements are essentially a risk bounds on the lasso estimator, which will be discussed and proved in a future post. The final condition is, for \\(\\theta_{ii}&#94;{ora} = \\frac{\\norm{\\XX_i - \\XX{A&#94;c}\\beta_{A&#94;c, i}}&#94;2}{n}\\) , \\begin{align} \\max_{A: A = \\{i, j\\}} \\PP\\left\\{ \\left| \\frac{\\hat\\theta_{ii}}{\\theta_{ii}&#94;{ora}} - 1 \\right| \\geq C_0 s \\delta \\frac{\\log p}{n} \\right\\} \\leq p&#94;{-\\delta + 1}\\varepsilon_\\Omega \\end{align} with a certain complexity measure \\(s\\) of the precision matrix \\(\\Omega\\) , assuming the spectrum of \\(\\Omega\\) is bounded, and \\(n \\geq \\frac{(s\\log p)&#94;2}{c_0}\\) for a sufficiently small \\(c_0 > 0\\) . Statement Theorem 2. Let \\(\\hat\\Theta_{A, A}\\) and \\(\\hat\\Omega_{A, A}\\) be estimators defined in (2) and (3) respectively. Let \\(\\delta \\geq 1\\) . Suppose \\(s \\leq \\frac{c_0n}{\\log p}\\) for a sufficiently small constant \\(c_0 > 0\\) . Suppose that conditions (7), (8), (9) hold with \\(C_0\\) and \\(\\varepsilon_\\Omega\\) . Then \\begin{align} \\max_{G&#94;*(M, s, \\lambda)}\\max_{A:A=\\{i, j\\}} \\PP\\left\\{ \\norm{\\hat\\Theta_{A, A} - \\Theta_{A, A}&#94;{ora}}_\\infty > C_1 s \\frac{\\log p}{n} \\right\\}\\leq 6\\varepsilon_\\Omega p&#94;{-\\delta + 1} + \\frac{4p&#94;{-\\delta + 1}}{\\sqrt{2\\log p}} \\end{align} and \\begin{align} \\max_{G&#94;*(M, s, \\lambda)}\\max_{A:A=\\{i, j\\}} \\PP\\left\\{ \\norm{\\hat\\Omega_{A, A} - \\Omega_{A, A}&#94;{ora}}_\\infty > C_1' s \\frac{\\log p}{n} \\right\\}\\leq 6\\varepsilon_\\Omega p&#94;{-\\delta + 1} + \\frac{4p&#94;{-\\delta + 1}}{\\sqrt{2\\log p}} \\end{align} where \\(\\Theta&#94;{ora}_{A, A}\\) and \\(\\Omega&#94;{ora}_{A, A}\\) are the oracle estimators and \\(C_1\\) is a positive constant depending only on \\(\\{C_0, \\max_{m\\in A = \\{i, j\\}}\\theta_{mm}\\}\\) . Let \\(\\lambda = (1 + \\varepsilon)\\sqrt{\\frac{2\\delta\\log p}{n}}\\) with \\(\\varepsilon > 0\\) be the \\(\\lambda\\) parameter in the scaled lasso estimation problem, and let \\(\\hat\\beta_{A&#94;c, A}\\) be the scaled lasso estimator, or the LSE after the scaled lasso selection. Then (4), (5), and (6), and thus (7) and (8) hold for all \\(\\Omega \\in \\mathcal{G}&#94;*(M, s, \\lambda)\\) with a certain constant \\(C_0\\) depending on \\(\\{\\varepsilon, c_0, M\\}\\) only and \\begin{align} \\max_{\\Omega \\in \\mathcal{G}&#94;*(M, s, \\lambda)} \\varepsilon_\\Omega = o(1) \\end{align} There exist constants \\(D_1\\) and \\(\\vartheta \\in (0, \\infty)\\) , and four marginally standard normal random variables \\(Z1, Z_{kl}\\) , where \\(kl = ii, ij, jj\\) , such that whenever \\(|Z_{kl}| \\leq \\vartheta\\sqrt{n}\\) for all \\(kl\\) , we have \\begin{align} \\left|\\kappa_{ij}&#94;{ora} - Z'\\right| \\leq \\frac{D_1}{\\sqrt{n}}\\left(1 + Z_{ii}&#94;2 + Z_{ij}&#94;2 + Z_{jj}&#94;2\\right) \\end{align} where \\(Z'\\) , which can be defined as a linear combination of \\(Z_{kl}\\) . Intuitively, statement (1) says that there is a very low probability that the maximum entrywise deviation of the actual estimator from the oracle MLE is larger than a constant that we can control. Statement (2) shows that the conditions are met such that statement (1) holds. Statement (3) says that the rescaled oracle MLE behaves more or less asymptotically normally. Once we show these statements about the estimator relative to oracle MLEs, we will prove statements in Theorem 3 relating the oracle MLEs to the true parameter values, and by the triangle inequality, we will have bounds on the distances between our estimates on the truth. Proof for Theorem 2(i) The values of \\(\\theta_{ii}, \\theta_{jj}\\) are uniformly bounded, which implies that the desired concentration bound (7) follows from (4) for \\(\\theta&#94;{ora}_{ii}\\) and \\(\\theta&#94;{ora}_{jj}\\) . Therefore, we only need to be concerned about bounding \\(\\theta&#94;{ora}_{ij}\\) . Recall that we define \\(\\bar \\DD = \\diag\\left(\\frac{\\XX&#94;\\top \\XX}{n}\\right)\\) and that \\(\\XX_{A&#94;c}\\) is independent of \\(\\epsilon_A\\) . First, we show the following. Claim. $$\\left(\\XX \\bar \\DD&#94;{-\\frac{1}{2}}\\right)&#94;\\top_k \\frac{\\epsilon_m}{n}\\sim \\Nn\\left(0, \\frac{\\theta_{mm}}{n}\\right)$$ for all \\(m \\in A\\) . Proof. First, we observe that \\(\\XX\\bar\\DD&#94;{-\\frac{1}{2}}\\) is essentially \\(\\XX\\) with its columns scaled to unit length in Euclidean norm. The fact that the mean of the distribution is zero follows from the fact that the columns of \\(\\XX\\) are assumed to be centered. To show the variance, we observe that we may express \\begin{align*} \\var\\left(\\left(\\XX \\bar \\DD&#94;{-\\frac{1}{2}}\\right)&#94;\\top_k\\epsilon_m\\right) &= \\var\\left(\\sum_{i=1}&#94;p \\left(\\XX\\bar\\DD&#94;{-\\frac{1}{2}}\\right)_{ik} \\epsilon_{im}\\right) \\\\ &= \\sum_{i=1}&#94;p \\left(\\XX\\bar\\DD&#94;{-\\frac{1}{2}}\\right)&#94;2_{ik} \\var\\left(\\epsilon_{im}\\right) \\\\ &= \\sum_{i=1}&#94;p \\left(\\XX\\bar\\DD&#94;{-\\frac{1}{2}}\\right)&#94;2_{ik} \\var\\left(\\epsilon_{1m}\\right) &&\\text{(Symmetry.)} \\\\ &= \\var\\left(\\epsilon_{1m}\\right) \\\\ &= \\EE \\left[\\epsilon_{1m}&#94;2\\right] &&\\text{(Errors centered at zero.)} \\\\ &= n\\theta_{mm} \\end{align*} Dividing \\(\\XX\\bar\\DD&#94;{-1/2}\\) by \\(n\\) gives the desired variance. ∎ It then follows from the union bound and Mill's Inequality that: $$ \\PP\\left\\{ \\norm{ \\left(\\XX\\bar\\DD&#94;{-1/2}\\right)&#94;\\top_{A&#94;c} \\frac{\\epsilon_m}{n} }_\\infty > \\sqrt{ 2\\delta\\theta_{mm}n&#94;{-1}\\log p } \\right\\} \\leq \\frac{p&#94;{-\\delta}(p-2)}{\\sqrt{2\\delta\\log p}} $$ Now, let's compare our covariance estimates to the oracle MLE: \\begin{align*} \\left| \\hat\\theta_{ij} - \\theta_{ij}&#94;{ora}\\right| &= \\left| \\frac{\\hat\\epsilon_i&#94;\\top\\hat\\epsilon_j}{n} - \\frac{\\epsilon_i&#94;\\top\\epsilon_j}{n} \\right| \\end{align*} Recalling that \\begin{align*} \\hat\\epsilon_A &= \\XX_A - \\XX_{A&#94;c}\\hat\\beta_{A&#94;c, A} \\\\ \\epsilon_A &= \\XX_A - \\XX_{A&#94;c}\\beta_{A&#94;c, A} \\\\ \\Rightarrow \\hat\\epsilon_A - \\epsilon_A &= \\XX_{A&#94;c}\\left(\\beta_{A&#94;c, A} - \\hat\\beta_{A&#94;c, A}\\right) \\end{align*} we have: \\begin{align*} \\left| \\hat\\theta_{ij} - \\theta_{ij}&#94;{ora}\\right| &= \\frac{1}{n}\\left| \\hat\\epsilon_i&#94;\\top\\hat\\epsilon_j - \\epsilon_i&#94;\\top\\epsilon_j \\right| \\\\ &= \\frac{1}{n}\\left| \\left(\\epsilon_i + (\\hat\\epsilon_i - \\epsilon_i) \\right)&#94;\\top \\left(\\epsilon_j + (\\hat\\epsilon_j - \\epsilon_j) \\right) - \\epsilon_i&#94;\\top\\epsilon_j \\right| \\\\ &= \\frac{1}{n}\\left| \\left(\\epsilon_i + \\XX_{A&#94;c}(\\beta_i - \\hat\\beta_i) \\right)&#94;\\top \\left(\\epsilon_j + \\XX_{A&#94;c}(\\beta_j - \\hat\\beta_j) \\right) - \\epsilon_i&#94;\\top\\epsilon_j \\right| \\\\ &= \\frac{1}{n} \\Bigg| \\epsilon_i&#94;\\top \\epsilon_j + \\left(\\beta_i - \\hat\\beta_i\\right)&#94;\\top \\XX_{A&#94;c}&#94;\\top \\epsilon_j + \\epsilon_i&#94;\\top \\left(\\beta_j - \\hat\\beta_j\\right)\\XX_{A&#94;c} \\\\ &\\qquad + \\left(\\beta_i - \\hat\\beta_i\\right)&#94;\\top \\XX_{A&#94;c}&#94;\\top \\XX_{A&#94;c}\\left(\\beta_j - \\hat\\beta_j\\right) -\\epsilon_i&#94;\\top \\epsilon_j \\Bigg| \\\\ &\\leq \\frac{1}{n}\\Bigg[ \\left| \\left(\\beta_i - \\hat\\beta_i\\right)&#94;\\top \\XX_{A&#94;c}&#94;\\top \\epsilon_j \\right| + \\left| \\epsilon_i&#94;\\top \\left(\\beta_j - \\hat\\beta_j\\right)\\XX_{A&#94;c} \\right| \\\\ &\\qquad + \\left|\\left(\\beta_i - \\hat\\beta_i\\right)&#94;\\top \\XX_{A&#94;c}&#94;\\top \\XX_{A&#94;c}\\left(\\beta_j - \\hat\\beta_j\\right) \\right|\\Bigg] \\\\ &= \\frac{1}{n}\\Bigg[ \\left| \\left(\\beta_i - \\hat\\beta_i\\right)&#94;\\top \\bar\\DD&#94;{-1/2}\\bar\\DD&#94;{1/2} \\XX_{A&#94;c}&#94;\\top \\epsilon_j \\right| \\\\ &\\qquad + \\left| \\epsilon_i&#94;\\top \\left(\\beta_j - \\hat\\beta_j\\right) \\bar\\DD&#94;{-1/2}\\bar\\DD&#94;{1/2}\\XX_{A&#94;c} \\right| \\\\ &\\qquad + \\left|\\left(\\beta_i - \\hat\\beta_i\\right)&#94;\\top \\XX_{A&#94;c}&#94;\\top \\XX_{A&#94;c}\\left(\\beta_j - \\hat\\beta_j\\right) \\right|\\Bigg]\\\\ &\\leq \\frac{1}{n}\\Bigg[ \\norm{ \\left(\\XX\\bar\\DD&#94;{-1/2}\\right)_{A&#94;c}&#94;\\top\\epsilon_i }_\\infty\\norm{ \\bar\\DD&#94;{1/2}\\left(\\beta_j - \\hat\\beta_j\\right) }_1 \\\\ &\\qquad + \\norm{ \\left(\\XX\\bar\\DD&#94;{-1/2}\\right)_{A&#94;c}&#94;\\top\\epsilon_j }_\\infty\\norm{ \\bar\\DD&#94;{1/2}\\left(\\beta_i - \\hat\\beta_i\\right) }_1 \\\\ &\\qquad + \\norm{ \\XX_{A&#94;c}\\left(\\beta_i - \\hat\\beta_i\\right) }\\cdot\\norm{ \\XX_{A&#94;c}\\left(\\beta_j - \\hat\\beta_j\\right) } \\Bigg] \\\\ &\\leq 2\\sqrt{2\\delta\\theta_{mm}n&#94;{-1}\\log p}C_0 s\\sqrt{\\delta \\frac{\\log p}{n}} + \\frac{C_0s\\delta\\log p}{n}\\\\ &= C_1 s\\frac{\\delta \\log p}{n} \\end{align*} with probability at least \\(1 - 2p&#94;{-\\delta + 1}\\epsilon_\\Omega - 2p&#94;{-\\delta + 1}(2\\log p)&#94;{-1/2}\\) by the union bound, implying (7). Given that the spectrum of \\(\\Theta_{A, A}\\) is bounded, the functional \\(\\zeta_{kl}(\\Theta_{A, A}) = \\left(\\Theta_{A, A}&#94;{-1}\\right)_{kl}\\) is Lipschitz in a neighborhood of \\(\\Theta_{A, A}\\) for \\(k, l \\in A\\) , and thus the bound on distances between the precision matrix estimates and the oracle MLE for the precision matrix in (8) follows from (7). Proof for Theorem 2(ii) The proof for part (ii), though fairly straightforward, depends on Theorem 10(i), Theorem 11(ii), and Proposition 1, and so we will return to this later. This part of the Theorem essentially gives and proves conditions under which conditions (4), (5), and (6) hold, which in turn imply (7) and (8) for all \\(\\Omega \\in \\Gg&#94;*(M, s, \\lambda)\\) up to a constant. This part of the Theorem also establishes that \\(\\varepsilon_\\Omega\\) is \\(o(1)\\) for all \\(\\Omega\\) in the parameter space, implying that it has a negligible impact on the concentration bounds (7) and (8). Proof for Theorem 2(iii) To prove the coupling inequality in (10), we first define a random vector \\(\\eta&#94;{ora} = \\left(\\eta_{ii}&#94;{ora}, \\eta_{ij}&#94;{ora}, \\eta_{jj}&#94;{ora}\\right)\\) , where $$ \\eta_{kl}&#94;{ora} = \\sqrt{n}\\frac{\\theta_{kl}&#94;{ora} - \\theta_{kl}} {\\theta_{kk}\\theta_{ll} + \\theta_{kl}&#94;2} $$ By the KMT inequality, for which the authors cite Mason and Zhou (2012) for the one-dimensional case and Einmahl (1989) for the multidimensional case, there exist constants \\(D_0, \\vartheta \\in (0, \\infty)\\) and a random Gaussian vector \\(Z = (Z_{ii}, Z_{ij}, Z_{jj}) \\sim \\Nn(0, \\breve\\Sigma)\\) where \\(\\breve\\Sigma = \\cov(\\eta&#94;{ora})\\) , such that \\(|Z_{kl}| \\leq \\vartheta\\sqrt{n}\\) for all \\(kl\\) implies $$ \\norm{\\eta&#94;{ora} - Z}_\\infty \\leq \\frac{D_0}{\\sqrt{n}} \\left( 1 + Z_{ii}&#94;2 + Z_{ij}&#94;2 + Z_{jj}&#94;2 \\right) $$ Let us now define \\(\\Theta = (\\theta_{ii}, \\theta_{ij}, \\theta_{jj})\\) , consider the function $$ \\omega_{ij}(\\Theta) = -\\frac{\\theta_{ij}}{\\theta_{ii}\\theta_{jj} - \\theta_{ij}&#94;2} $$ and take its Taylor expansion, which gives us: \\begin{align*} \\omega_{ij}&#94;{ora} - \\omega_{ij} &= \\la \\nabla\\omega_{ij}(\\Theta), \\Theta&#94;{ora} - \\Theta\\ra + \\sum_{|\\beta| = 2}R_\\beta(\\Theta&#94;{ora})(\\Theta-\\Theta&#94;{ora})&#94;\\beta \\end{align*} where \\begin{align*} |\\beta| &\\triangleq \\sum_k \\beta_k \\\\ x&#94;\\beta &\\triangleq \\prod_k x_k&#94;{\\beta_k} \\\\ D&#94;\\beta f(x)&\\triangleq \\frac{\\partial&#94;{|\\beta|} f} { \\partial x_1&#94;{\\beta_1} \\partial x_2&#94;{\\beta_2} \\partial x_3&#94;{\\beta_3} } \\end{align*} We observe that Taylor's Theorem gives us a uniform upper bound on the coefficients of the remainder terms: $$ \\left| R_\\beta\\left(\\Theta&#94;{ora}_{A, A}\\right) \\right| \\leq 2 \\max_{|\\alpha| = 2}\\max_{\\Theta_\\in B} D&#94;\\alpha\\omega_{ij}(\\Theta) \\leq C_2 $$ where \\(B\\) is a sufficiently small compact ball centered at \\(\\Theta\\) . The upper bound holds when \\(\\Theta&#94;{ora}\\) is in \\(B\\) , an assumption which can be satisfied by picking a small enough value \\(\\vartheta\\) in the assumption \\(\\norm{\\eta&#94;{ora}}_\\infty \\leq \\vartheta \\sqrt{n}\\) . Note that here \\(D&#94;\\alpha\\) is not a constant, but a second order derivative. This bound follows from the fact that equality in the Taylor expansion is satisfied when evaluating the second order term with some \\(\\xi\\) between \\(\\Theta\\) and \\(\\Theta&#94;{ora}\\) , so the coefficients are naturally bounded above by the maximum value of the second order coefficients in \\(B\\) , as it encloses both \\(\\Theta\\) and \\(\\Theta&#94;{ora}\\) . The bottom line is that when the oracle MLE's value is sufficiently close to the truth, a linear approximation is basically good enough. With this linear approximation to \\(\\omega_{ij}\\) as a function of \\(\\theta_{ij}\\) (and a quadratic correction term we can control), we can then express this relationship in terms of \\(\\kappa_{ij}\\) and \\(\\eta_{ij}\\) , which are simply \\(\\omega_{ij}, \\theta_{ij}\\) rescaled by constants, respectively: $$ \\kappa_{ij}&#94;{ora} = h_1\\eta_{ii}&#94;{ora} + h_2\\eta_{ij}&#94;{ora} + h_3\\eta_{jj}&#94;{ora} + \\sum_{|\\beta| = 2} \\frac{D_\\beta R_\\beta(\\Theta&#94;{ora})}{\\sqrt{n}}(\\eta&#94;{ora})&#94;\\beta $$ where \\(h_1, h_2, h_3, D_\\beta\\) are constants. Subtracting \\(Z' = h_1Z_1 + h_2Z_2 + h_3Z_3 \\sim \\Nn(0, 1)\\) from both sides and applying Hölder's Inequality, we obtain: $$ |\\kappa_{ij}&#94;{ora} - Z'| \\leq \\left( \\sum_{k=1}&#94;3 |h_k| \\right) \\norm{Z - \\eta&#94;{ora}}_\\infty + \\frac{C_3}{\\sqrt{n}}\\norm{\\eta&#94;{ora}}&#94;2 $$ Applying the KMT inequality and the fact that \\(\\norm{\\eta&#94;{ora}}&#94;2 \\leq C_4 (Z_{ii}&#94;2 + Z_{ij}&#94;2 + Z_{jj}&#94;2)\\) for some large constant \\(C_4\\) , we complete the proof: \\begin{align*} |\\kappa_{ij}&#94;{ora} - Z'| &\\leq \\left( \\sum_{k=1}&#94;3 |h_k| \\right) \\norm{Z - \\eta&#94;{ora}}_\\infty + \\frac{C_3}{\\sqrt{n}}\\norm{\\eta&#94;{ora}}&#94;2\\\\ &\\leq \\frac{D_1}{\\sqrt{n}} ( 1 + Z_{ii}&#94;2 + Z_{ij}&#94;2 + Z_{jj}&#94;2) \\end{align*} ∎ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"http://huisaddison.com/blog/discuss-precmat-optimality-pt2.html"},{"title":"Discussion: Asymptotic Normality and Optimalities in Estimation of Large Gaussian Graphical Models, Part 1","text":"$$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\RR}{\\mathbf{R}} \\newcommand{\\EE}{\\mathbf{E}} \\newcommand{\\XX}{\\mathbf{X}} \\newcommand{\\Nn}{\\mathcal{N}} \\{\\Nn}{\\mathcal{N}} \\DeclareMathOperator{\\var}{var} $$ Introduction As part of the project I've been working on, I'm reading Asymptotic Normality and Optimalities in Estimation of Large Gaussian Graphical Models , a paper by Ren, Sun, Zhang, and Zhou. The paper describes a technique for producing asymptotically efficient entrywise estimators for precision matrices, under the Gaussian assumption. They are able to accomplish this with a sparseness condition relative to the sample size. Intuitively, matrices that are less sparse require more samples to achieve the parametric rate. The parameter space also makes an assumption the spectrum of the matrix (range of its singular values). In this initial post, some motivation for the problem is given, and then we walk through some intuition, and then fully dissect the technique used by the author, so that later posts can be fully dedicated to the inference results and proofs of theorems. Motivation The precision matrix is of particular interest in the multivariate Gaussian setting due to the fact that the precision matrix admits, in the form of its nonzero entries, an adjacency matrix for the graphical model associated with conditional independence statements about the set of Gaussian random variables involved. This will be discussed in an upcoming blog post. Intuition The authors produce entrywise elements of the precision matrix by exploiting some neat properties about Gaussian vectors. Consider a random matrix \\(X \\in \\RR&#94;{n \\times p}\\) , \\(n\\) observations of \\(p\\) variables, drawn from a \\(\\Nn(0, \\Sigma_{p\\times p})\\) distribution. Rather than estimating \\(\\hat\\Sigma\\) directly and inverting it, we may make the following observations. Let's take a look at what happens if we take a pair of indices \\(A = \\{i, j\\} \\in [p]\\) , and regress the associated variables on all the others, \\(A&#94;c = [p]\\setminus\\{i, j\\}\\) . We would have: $$ x_A = \\beta x_{A&#94;c} + \\epsilon $$ where \\(\\epsilon\\) is a noise term, distributed normally with mean zero, and which are independent of \\(A&#94;c\\) . Note that here \\(x_A \\in \\RR&#94;{2}\\) \\(\\beta \\in \\RR&#94;{2\\times (p-2)}\\) \\(x_{A&#94;c} \\in \\RR&#94;{p-2}\\) \\(\\epsilon \\in \\RR&#94;2\\) We can multiply both sides by \\(x_{A&#94;c}\\) , and then take the expectation on both sides: \\begin{align*} x_A &= \\beta x_{A&#94;c} + \\epsilon \\\\ x_Ax_{A&#94;c}&#94;\\top &= \\beta x_{A&#94;c}x_{A&#94;c}&#94;\\top + \\epsilon x_{A&#94;c}&#94;\\top \\\\ \\EE x_Ax_{A&#94;c}&#94;\\top &= \\beta \\EE x_{A&#94;c}x_{A&#94;c}&#94;\\top \\\\ \\Sigma_{A, A&#94;c} &= \\beta \\Sigma_{A&#94;c, A&#94;c} \\\\ \\Rightarrow \\beta &= \\Sigma_{A, A&#94;c}\\Sigma_{A&#94;c, A&#94;c}&#94;{-1} \\end{align*} This immediately implies that given we observe \\(x_{A&#94;c}\\) , the mean of \\(x_A\\) is \\(\\Sigma_{A, A&#94;c}\\Sigma_{A&#94;c, A&#94;c}&#94;{-1}x_{A&#94;c}\\) . But what about the variance? Let's see how the noise is distributed: \\begin{align*} \\epsilon &= \\Sigma_{A, A&#94;c}\\Sigma_{A&#94;c, A&#94;c}&#94;{-1} x_{A&#94;c} - x_A\\\\ \\mathrm{var}(\\epsilon) &= \\Sigma_{A, A&#94;c}\\Sigma_{A&#94;c, A&#94;c}&#94;{-1}\\Sigma_{A&#94;c, A&#94;c}\\Sigma_{A&#94;c, A&#94;c}&#94;{-1}\\Sigma_{A&#94;c, A} + \\Sigma_{A, A}\\\\ &=\\Sigma_{A, A&#94;c}\\Sigma_{A&#94;c, A&#94;c}&#94;{-1}\\Sigma_{A&#94;c, A} + \\Sigma_{A, A}\\\\ \\end{align*} Then, assuming \\(x_{A&#94;c}\\) is observed, the variance of \\(x_A\\) depends only on the noise: \\begin{align*} \\mathrm{var}(x_A|x_{A&#94;c}) &= \\mathrm{var}(\\epsilon) \\\\ &=\\Sigma_{A, A&#94;c}\\Sigma_{A&#94;c, A&#94;c}&#94;{-1}\\Sigma_{A&#94;c, A} + \\Sigma_{A, A}\\\\ \\end{align*} We observe that the variance of \\(\\epsilon\\) is essentially the variance of \\(x_A\\) given \\(x_{A&#94;c}\\) , which we denote \\(\\Sigma_{A|A&#94;c} = \\Theta_{A, A}\\) . By using the fact that: $$ \\Theta_{A, A} = \\Omega_{A, A}&#94;{-1} $$ we can intuitively get estimates for blocks of the precision matrix at a time by inverting the estimates for the conditional covariance matrix. Rewriting \\(\\Sigma\\) with \\(\\Omega\\) By crunching through some identities, we may rewrite: $$ \\beta = \\Sigma_{A, A&#94;c}\\Sigma_{A&#94;c, A&#94;c}&#94;{-1} = -\\Omega_{A, A}&#94;{-1}\\Omega_{A, A&#94;c} $$ which is similar to what the authors will use later on. I can review the salient identities in a future post. Methodology Now that we've reviewed the intuition behind why this should work, let's dive into the methodology of the paper. Note that to be consistent with the convention that observations are listed in a data matrix \\(X\\) row-wise, most of the vectors in the paper's exposition are row vectors; therefore, the next few formulas will be transposes of those given in the previous section. By analogy of the intuition previously given (the only differences are that \\(\\XX\\) is now a matrix, and \\(\\beta\\) and \\(\\epsilon\\) are transposed), we have: $$ \\XX_A = \\XX_{A&#94;c}\\beta + \\epsilon_A $$ where, adhering to the authors' usage of the precision matrix, and transposing everything, we have: $$ \\beta = - \\Omega_{A&#94;c, A}\\Omega_{A, A} $$ Now, suppose we were only interested in estimating \\(\\epsilon\\) (as looking at the noise is the key to estimating the entries of the precision matrix) and we knew \\(\\beta\\) . Then what would the maximum likelihood estimator of \\(\\Theta_{A, A}\\) look like? The authors denote oracle MLE of \\(\\Theta_{A, A}\\) as $$ \\Theta&#94;{ora}_{A, A} = (\\theta&#94;{ora}_{kl})_{k, l \\in A} = \\frac{\\epsilon_A&#94;\\top\\epsilon_A}{n} $$ and so the corresponding estimates of the precision matrix would be: $$ \\Omega&#94;{ora}_{A, A} = (\\omega&#94;{ora}_{kl})_{k, l \\in A} = \\left(\\Theta&#94;{ora}_{A, A}\\right)&#94;{-1} $$ Suppose we have an adequate estimates of the regression weights \\(\\hat\\beta\\) . Based on \\(\\hat\\beta\\) , we may derive estimates \\(\\hat\\epsilon\\) of the residuals. $$ \\hat\\epsilon_A = \\XX_A - \\XX_{A&#94;c}\\hat\\beta $$ and consequently the estimate of the conditional covariance matrix: $$ \\hat\\Theta_{A, A} = \\frac{\\hat\\epsilon_A&#94;\\top\\hat\\epsilon_A}{n} $$ and invert the entries of this estimator to get the estimates for \\(\\Omega_{A, A}\\) : $$ \\hat\\Omega_{A, A} = \\hat\\Theta_{A, A}&#94;{-1} $$ To calculate the estimates \\(\\hat\\beta\\) , the authors introduce a scaled lasso regression problem. For each \\(m \\in A = \\{i, j\\}\\) , they perform the optimization: $$ \\left\\{\\hat\\beta_m, \\hat\\theta&#94;{1/2}_{mm}\\right\\} = \\arg\\min_{b\\in\\RR&#94;{p-2},\\\\\\sigma \\in \\RR&#94;+} \\left\\{ \\frac{\\norm{\\XX_m - \\XX_{A&#94;c}b}&#94;2}{2n\\sigma} + \\frac{\\sigma}{2} + \\lambda\\sum_{k\\in A&#94;c}\\frac{\\norm{\\XX_k}}{\\sqrt{n}}|b_k| \\right\\} $$ Intuitively, the scaling factor on the \\(\\ell_1\\) penalty implicitly standardizes the design vector to length \\(\\sqrt{n}\\) such that the \\(\\ell_1\\) penalty is applied to the new coefficients \\(\\frac{\\norm{\\XX_k}}{\\sqrt n}b_k\\) . The authors also consider the following least squares estimator, based on the model \\(\\hat S_{mm}\\) selected by the scaled lasso estimation problem: $$ \\left\\{\\hat\\beta_m, \\hat\\theta&#94;{1/2}_{mm}\\right\\} = \\arg\\min_{b\\in\\RR&#94;{p-2},\\\\\\sigma \\in \\RR&#94;+} \\left\\{ \\frac{\\norm{\\XX_m - \\XX_{A&#94;c}b}&#94;2}{2n\\sigma} + \\frac{\\sigma}{2} : \\mathrm{supp}(b) \\subseteq \\hat S_{mm} \\right\\} $$ Essentially, after determining the support of \\(b\\) through the scaled lasso estimation problem, we can fit least squares entries for \\(\\hat\\beta_m\\) only on the features in the support. In my upcoming posts about this paper, I will review the inference results and step through the associated proofs. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"http://huisaddison.com/blog/discuss-precmat-optimality-pt1.html"},{"title":"A Cute Proof (Product of Kernels is a Kernel)","text":"Background One of the most fundamental concepts of statistical learning theory is that of the Reproducing Kernel Hilbert Space. Recall that a kernel function on an RKHS is such that \\(K(x, y) = \\langle K_x, K_y \\rangle_\\mathcal{H}\\) . Rather than evaluating an inner product in the Hilbert space, we may simply evaluate the kernel function. More generally, we may define a positive semidefinite kernel \\(K:\\mathcal{X} \\times \\mathcal{X} \\mapsto \\mathbf{R}\\) such that \\(M = \\left(K(x_i, x_j)\\right)_{ij}\\) is positive semidefinite for all finite collections \\(\\{x_i\\}_{i=1}&#94;n\\) . To make this result more useful, it would be nice te be able to have a consistent way of discovering or constructing kernels. One such way is to take the product of two existing kernels. Statement & Proof Claim. Suppose \\(K_1, K_2: \\mathcal{X}\\times\\mathcal{X} \\mapsto \\mathbf{R}\\) are kernels. Then $$ K(x, y) = K_1(x, y) K_2(x, y) $$ is a kernel. Proof. Because \\(K_1, K_2\\) are kernels, given an arbitrary finite collection of \\(\\{x_1\\}_{i=1}&#94;n\\) , the matrix defined by evaluating the kernel function on all pairs of the collection satisfies the following: $$ \\mathbf{R}&#94;{n\\times n} \\ni K_j = \\left(K_j(x_i, x_k)\\right)_{ik} \\succeq 0 $$ for \\(j \\in \\{1, 2\\}\\) . Let \\(\\mathbf{R}&#94;n \\ni X_1 \\sim \\mathcal{N}(0, K_1), X_2 \\sim \\mathcal{N}(0, K_2)\\) be independent. Define \\(Y \\triangleq X_1 \\circ X_2\\) the elementwise product of \\(X_1\\) and \\(X_2\\) . Then: \\begin{align*} \\mathrm{cov}(Y_i, Y_j) &= \\mathbf{E} Y_i Y_j \\\\ &= \\mathbf{E} X_{1i}X_{2i}X_{1j}X_{2j} \\\\ &= \\mathbf{E} X_{1i}X_{1j}\\mathbf{E}X_{2i}X_{2j} \\\\ &= \\mathrm{cov}(X_{1i}, X_{1j})\\mathrm{cov}(X_{2i}, X_{2j}) \\\\ &= K_1(x_i, x_j)K_2(x_i, x_j) \\\\ &= K(x_i, x_j) \\end{align*} and we know covariance matrices in general are positive semidefinite. Discussion I'm particularly fond of this proof because it elegantly connects two facts: Kernel functions evaluated on finite sets yield positive semidefinite matrices by definition. Covariance matrices are by definition positive semidefinite. We begin with two kernel functions, and show that the product of their outputs may be considered valid output for an other kernel; but along the way, we treat the matrices created by evaluating the two known kernel functions on finite collections as covariance matrices, and construct a new covariance matrix that coincides with the desired matrix associated with the function we sought to prove is a kernel. With the multivariate normal distribution as our aid, we step in to and back out of the world of probability to prove a result in the field of learning theory. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"http://huisaddison.com/blog/cute-proof-about-kernels.html"},{"title":"Le Cam's Method","text":"$$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\EE}{\\mathbf{E}} \\newcommand{\\Pp}{\\mathcal{P}} \\newcommand{\\Dd}{\\mathcal{D}} $$ Le Cam's two-point method is a fundamental tool in the establishment of minimax lower bounds. After identifying suitable subsets of the parameter space, the key to the argument is a lower bound of the maximum by an average. Le Cam's method can be found at the heart of other minimax lower bound arguments, such as Assouad's Lemma . For this discussion, we will follow the exposition from Bin Yu's Chapter 29 of Festschrift for Le Cam . Statement Suppose \\(\\Pp\\) is a family of probability measures, with each probability measure \\(P\\) parameterized by a \\(\\theta(P)\\) which lives in a pseudo-metric space \\((\\Dd, d)\\) . Remark. Yu notes that we use pseudo-metric spaces as requiring \\(d(\\theta, \\theta') \\Leftrightarrow \\theta = \\theta'\\) would be troublesome. We define \\(\\hat\\theta = \\hat\\theta(X)\\) to be the estimator of \\(\\theta(P)\\) based on \\(X\\) drawn from a distribution \\(P\\) . Finally, we denote the convex hull of \\(\\Pp\\) by \\(co(\\Pp)\\) . Lemma. Let \\(\\hat\\theta\\) be an estimator of \\(\\theta(P)\\) on \\(\\Pp\\) taking values in a metric space \\((\\Dd, d)\\) . Suppose that there are subsets \\(\\Dd_1\\) and \\(\\Dd_2\\) of \\(\\Dd\\) that are \\(2\\delta\\) -separated; that is, \\(d(s_1, s_2) \\geq 2\\delta\\) for all \\(s_1\\in \\Dd_1, s_2\\in\\Dd_2\\) . Suppose further that \\(\\Pp_1, \\Pp_2\\) are subsets of \\(\\Pp\\) , such that \\(\\theta(P)\\in\\Dd_1\\) for \\(P \\in \\Pp_1\\) < and \\(\\theta(P)\\in\\Dd_2\\) for \\(P\\in\\Pp_2\\) . Then $$ \\sup_{P\\in\\Pp}\\EE_Pd(\\hat\\theta, \\theta(P)) \\geq \\delta \\cdot \\sup_{P_1\\in co(\\Pp_1),\\\\ P_2\\in co(\\Pp_2)} \\norm{P_1\\wedge P_2} $$ where \\(\\norm{P_1\\wedge P_2}\\) is the total variation affinity. Proof. We begin by observing that we may bound the supremem from below by an average over an arbitrary pair of \\(P_1 \\in \\Pp_1\\) and \\(P_2 \\in \\Pp_2\\) : \\begin{align*} \\sup_{P\\in\\Pp}\\EE_Pd(\\hat\\theta, \\theta(P)) &\\geq\\frac{1}{2}\\left[ \\EE_{P_1} d(\\hat\\theta, \\theta(P_1)) + \\EE_{P_2} d(\\hat\\theta, \\theta(P_2)) \\right] \\end{align*} We are there able to lower bound the distances to \\(\\theta(P_1) \\in \\Dd_1\\) and \\(\\theta(P_2) \\in \\Dd_2\\) by the distances to arbitrary members of \\(\\Dd_1\\) and \\(\\Dd_2\\) -- say, the elements in each subset closest to \\(\\hat\\theta\\) . \\begin{align*} \\frac{1}{2}\\left[ \\EE_{P_1} d(\\hat\\theta, \\theta(P_1)) + \\EE_{P_2} d(\\hat\\theta, \\theta(P_2)) \\right] &\\geq \\frac{1}{2}\\left[ \\EE_{P_1} d(\\hat\\theta, \\Dd_1) + \\EE_{P_2} d(\\hat\\theta, \\Dd_2) \\right] \\end{align*} At this point, we may note that each expectation is an integration with respect to a probability measure. Therefore, we may interpret each one as a weighted sum, and lower bound the sum of the two by integrating with respect to the pointwise minimum of the two measures. \\begin{align*} \\frac{1}{2}\\left[ \\EE_{P_1} d(\\hat\\theta, \\Dd_1) + \\EE_{P_2} d(\\hat\\theta, \\Dd_2) \\right] &\\geq \\frac{1}{2}\\left[ \\int \\left(d(\\hat\\theta, \\Dd_1) + d(\\hat\\theta, \\Dd_2)\\right) \\min\\{p_1, p_2\\}d\\mu \\right] \\end{align*} Applying the triangle inequality completes the proof: \\begin{align*} \\frac{1}{2}\\left[ \\int \\left(d(\\hat\\theta, \\Dd_1) + d(\\hat\\theta, \\Dd_2)\\right) \\min\\{p_1, p_2\\}d\\mu \\right] &\\geq \\frac{1}{2}\\left[ \\int d(\\Dd_1, \\Dd_2) \\min\\{p_1, p_2\\}d\\mu \\right] \\\\ &\\geq \\frac{1}{2}\\left[ \\int 2\\delta \\min\\{p_1, p_2\\}d\\mu \\right] \\\\ &= \\delta \\int \\min\\{p_1, p_2\\}d\\mu \\\\ &= \\delta \\norm{P \\wedge Q} \\end{align*} Discussion Le Cam's method gives a simple, intuitive way to give a lower bound on the maximum risk associated with estimating a parameter by considering two subsets of the parameter space with a suitable degree of separation. The true challenge is identifying these subsets of the parameter space in a way that gives a useful bound. It's worth noting that intuitively, the search for a useful bound involves a balancing act between the separation distance distance \\(\\delta\\) and the total variation affinity between \\(\\mathcal{P}_1\\) and \\(\\mathcal{P}_2\\) . We could maximize the total variation affinity by taking \\(P_1 = P_2\\) , but then \\(\\delta = 0\\) . Conversely, choosing \\(\\mathcal{P}_1, \\mathcal{P}_2\\) such that \\(\\delta\\) is large may cause \\(\\norm{P_1 \\wedge P_2}\\) to vanish. Better rates may be achieved by using Assouad's Lemma, which applies Le Cam's method in a multiple comparisons setting. Bin Yu also remarks that \"Le Cam's method often gives the optimal rate when a real function is estimated... [whereas Assouad and Fano] seem to be effective when the whole unknown function is being estimated.\" if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"http://huisaddison.com/blog/lecams-method.html"},{"title":"Discussion: Optimal Rates of Convergence for Covariance Matrix Estimation, Part 2","text":"$$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} $$ In this post, I will continue my discussion of Optimal Rates of Convergence for Covariance Matrix Estimation . A discussion of their first result (showing an upper bound on the risk of their proposed tapering estimator) can be found in a previous post . Minimax Lower Bound We will review their proof of a minimax lower bound among all estimators that matches the upper bound previously proved for their tapering estimator. Theorem 3. Suppose \\(p \\leq \\exp(\\gamma n)\\) for some constant \\(\\gamma > 0\\) . The minimax risk for estimating the covariance matrix \\(\\Sigma\\) over \\(\\mathcal{P}_\\alpha\\) under the operator norm satisfies : \\begin{align*} \\inf_{\\hat\\Sigma}\\sup_{\\mathcal{P}_\\alpha} \\mathbf{E}\\Vert\\hat\\Sigma - \\Sigma\\Vert&#94;2 \\geq cn&#94;{-\\frac{2\\alpha}{2\\alpha + 1}} + c\\frac{\\log p}{n} \\end{align*} In the words of the authors, The basic strategy underlying the proof of Theorem 3 is to carefully construct a finite collection of multivariate normal distributions and calculate the total variation affinity between pairs of probability measures in the collection. Parameter Space Specification Oftentimes, the proof of a minimax lower bound is accompanied with the specification of a smaller parameter space that is easier to analyze. Intuitively, the restriction of the parameters to a subspace is permissible as for \\(\\mathcal{F}'\\subset \\mathcal{F}\\) : \\begin{align*} \\sup_{\\Sigma\\in\\mathcal{F}'} R(\\Sigma, \\hat\\Sigma) \\leq \\sup_{\\Sigma\\in\\mathcal{F}} R(\\Sigma, \\hat\\Sigma) \\end{align*} for all \\(\\hat\\Sigma\\) . It follows, then, that: \\begin{align*} \\inf_{\\hat\\Sigma}\\sup_{\\Sigma\\in\\mathcal{F}'} R(\\Sigma, \\hat\\Sigma) \\leq \\inf_{\\hat\\Sigma}\\sup_{\\Sigma\\in\\mathcal{F}} R(\\Sigma, \\hat\\Sigma) \\end{align*} For positive integers \\(k, m\\) such that \\(2k \\leq p\\) and \\(1 \\leq m \\leq k\\) , we define the \\(p \\times p\\) matrix \\(B(m, k) = (b_{ij})_{p\\times{p}}\\) such that: $$ b_{ij} = \\mathbf{1} \\{i = m \\text{ and } m+1 \\leq j\\leq 2k, \\text{ or } j = m \\text{ and } m+1 \\leq i \\leq 2k\\} $$ Setting \\(k = n&#94;\\frac{1}{2\\alpha + 1}\\) and \\(a = k&#94;{-\\alpha-1}\\) , we then define a collection of \\(2&#94;k\\) covariance matrices: $$ \\mathcal{F}_{11} = \\left\\{ \\Sigma(\\theta): \\Sigma(\\theta) = \\mathbf{I}_{p\\times p} + \\tau a\\sum_{m=1}&#94;k \\theta_m B(m, k), \\quad\\theta = (\\theta_m) \\in \\{0, 1\\}&#94;k \\right\\} $$ where \\(0 < \\tau < 2&#94;{-\\alpha - 1}M\\) . We can interpret this parameter space intuitively. First, we generate a set of \\(k\\) matrices \\(B(m, k)\\) , \\(m \\in [k]\\) , in which \\(B(m, k)\\) is nonzero only on certain elements along row or column \\(m\\) . Then, for each \\(\\theta = \\{0, 1\\}&#94;k\\) , we add a number of \\(\\tau a\\) perturbations to the identity matrix; where we add \\(\\tau a B(m, k)\\) if \\(\\theta_m \\neq 0\\) . This gives us a set of \\(\\Sigma(\\theta)\\) of size \\(2&#94;k\\) . We now verify that \\(\\mathcal{F}_{11} \\subset \\mathcal{F}(M, M_0)\\) . Recall the definition: $$ \\mathcal{F}(M_0, M) = \\left\\{ \\Sigma: \\max_j \\sum_j \\{|\\sigma_{ij}|: |i-j| > k\\} \\leq M k&#94;{-\\alpha} \\text{ for all } k, \\text{and} \\lambda_{\\text{max}}(\\Sigma)\\leq M_0 \\right\\} $$ Let us consider the \\(\\Sigma(\\theta)\\) where \\(\\theta\\) is a vector of \\(k\\) ones and \\(\\tau = 2&#94;{-\\alpha - 1}M\\) . Then $$ \\Sigma(\\theta) = \\mathbf{I}_{p\\times p} + M (2k)&#94;{-\\alpha -1}\\sum_{m=1}&#94;k B(m, k) $$ We observe that \\(\\sum_{m=1}&#94;k B(m, k)\\) is a matrix of ones on the off-diagonals up to row and column \\(2k \\leq p\\) . Therefore, the worst-case sum of entries more than \\(k\\) away from the diagonal is: \\begin{align*} k\\cdot M k&#94;{-\\alpha - 1} 2&#94;{- \\alpha - 1} &= M k&#94;{-\\alpha} 2&#94;{- \\alpha - 1} \\\\ &\\leq M k&#94;{-\\alpha} \\end{align*} Without loss of generality the authors assume \\(M_0 > 1\\) and \\(\\rho > 1\\) ; otherwise we may replace \\(\\mathbf{I}_{p\\times p}\\) with \\(\\varepsilon \\mathbf{I}_{ p \\times p}\\) with \\(\\varepsilon \\in (0, \\min\\{M_0, \\rho\\})\\) . It follows that \\(\\mathcal{F_{11}} \\subset \\mathcal{F}(M, M_0)\\) . We now construct a second parameter space \\(\\mathcal{F}_{12} \\subset \\mathcal{F}\\) as follows: $$ \\mathcal{F}_{12} = \\left\\{ \\Sigma_m: \\Sigma_m =\\mathbf{I}_{p\\times p} + \\left( \\sqrt{\\frac{\\tau}{n}}\\mathbf{1}\\{i = j = m\\}\\right)_{p \\times p}, 0 \\leq m \\leq p_1 \\right\\} $$ where \\(p_1 = \\min\\{p, e&#94;\\frac{n}{2}\\}\\) and \\(0 < \\tau < \\min\\{(M_0 - 1)&#94;2, (\\rho - 1)&#94;2, 1\\}\\) . Because the \\(\\Sigma_m\\) is this parameter are diagonal matrices, the bandability condition is satisfied trivially, and because the greatest diagonal entry is \\(1 + \\sqrt{\\frac{\\tau}{n}}\\) , the condition that the spectral norm be less than \\(M_0\\) is easily verified. Denote \\(\\mathcal{F}_1 = \\mathcal{F}_{11} \\cup \\mathcal{F}_{12}\\) . Observe that \\(\\mathcal{F}_1 \\subset \\mathcal{F}_\\alpha(M_0, M)\\) We will now prove that: $$ \\inf_{\\hat\\Sigma}\\sup_{\\mathcal{F}_{11}} \\mathbf{E} \\norm{\\hat\\Sigma - \\Sigma}&#94;2 \\geq cn&#94;{-\\frac{2\\alpha}{2\\alpha+1}} $$ and $$ \\inf_{\\hat\\Sigma}\\sup_{\\mathcal{F}_{12}} \\mathbf{E} \\norm{\\hat\\Sigma - \\Sigma}&#94;2 \\geq c\\frac{\\log p}{n} $$ for some constant \\(c > 0\\) . Taken together, we have: $$ \\inf_{\\hat\\Sigma}\\sup_{\\mathcal{F}_1} \\mathbf{E} \\norm{\\hat\\Sigma - \\Sigma}&#94;2 \\geq \\frac{c}{2}\\left(n&#94;{-\\frac{2\\alpha}{2\\alpha+1}} + \\frac{\\log p}{n}\\right) $$ which proves Theorem 3 . Lower Bound by Assouad's Lemma Now that we have our machinery set up, we can move on the meat of our proof. The goal of this section is to establish: $$ \\inf_{\\hat\\Sigma}\\sup_{\\mathcal{F}_{11}} \\mathbf{E} \\norm{\\hat\\Sigma - \\Sigma}&#94;2 \\geq cn&#94;{-\\frac{2\\alpha}{2\\alpha+1}} $$ Suppose we are estimating an arbitrary quantity \\(\\psi(\\theta)\\) within a metric space with metric \\(d\\) , over a set of parameters \\(\\Theta = \\{0, 1\\}&#94;k\\) . We denote the Hamming distance on \\(\\{0, 1\\}&#94;k\\) by \\(H(\\theta, \\theta') = \\sum_{i = 1}&#94;k |\\theta_i - \\theta_i'|\\) . We also define the total variation affinity between two probability measures \\(P\\) and \\(Q\\) with densities \\(p, q\\) with respect to measure \\(\\mu\\) by \\(\\norm{P \\wedge Q} = \\int \\min\\{p, q\\} d\\mu\\) . Under these assumptions, Assoud's Lemma gives the following lower bound on the maximum risk of estimating \\(\\psi(\\theta)\\) : Lemma (Assouad). Let \\(\\Theta = \\{0, 1\\}&#94;k\\) and let \\(T\\) be an estimator based on an observation from a distribution in the collection \\(\\{P_\\theta, \\theta \\in \\Theta\\}\\) . Then for all \\(s > 0\\) : $$ \\max_{\\theta\\in\\Theta} 2&#94;s \\mathbf{E}_\\theta d&#94;s(T, \\psi(\\theta)) \\geq \\min_{H(\\theta, \\theta') \\geq 1} \\frac{ d&#94;s(\\psi(\\theta), \\psi(\\theta')) }{ H(\\theta, \\theta') }\\cdot\\frac{k}{2}\\cdot \\min_{H(\\theta, \\theta')=1}\\norm{\\mathbf{P}_\\theta \\wedge\\mathbf{P}_{\\theta'}} $$ The authors give a natural interpretation of Assouad's Lemma in terms of multiple comparisons: The first factor is the minimum cost of making a mistake per comparison; that is, it is a lower bound on the distance between the distance between two parameters in the parameter space. The last factor (total variation affinity) measures the overlap between the two probability measures indexed by \\(\\theta\\) and \\(\\theta'\\) ; intuitively, it gives a lower bound on the total probability of making type I and type II errors for each comparison. \\(\\frac{k}{2}\\) is the expected number of mistakes made when \\(\\mathbf{P}_\\theta\\) and \\(\\mathbf{P}_{\\theta'}\\) are indistinguishable from each other when \\(H(\\theta, \\theta') = 1\\) . Assouad's Lemma can be unpacked as an extension of Le Cam's Method. A companion blog post discussing Assouad's Lemma with guidance from Yu's Assoud, Fano, and Le Cam and van der Vaart's Asymptotic Statistics can be found here . Suppose we draw \\(\\mathbf{X}_1, \\dotsc, \\mathbf{X}_n \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\Sigma(\\theta))\\) with \\(\\Sigma(\\theta) \\in \\mathcal{F}_{11}\\) . We denote the joint distribution for these random vectors by \\(\\mathbf{P}_\\theta\\) . The authors give two lemmas to complete the proof: Lemma 5. Let \\(\\Sigma(\\theta)\\in\\mathcal{F}_{11}\\) . Then for some constant \\(c > 0\\) : $$ \\min_{H(\\theta, \\theta') \\geq 1} \\frac{ \\norm{\\Sigma(\\theta) - \\Sigma(\\theta')}&#94;2 }{ H(\\theta, \\theta') } \\geq cka&#94;2 $$ Lemma 6. Suppose we draw \\(\\mathbf{X}_1, \\dotsc, \\mathbf{X}_n \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\Sigma(\\theta))\\) with \\(\\Sigma(\\theta) \\in \\mathcal{F}_{11}\\) . Denote the joint distribution by \\(\\mathbf{P}_\\theta\\) . Then for some constant \\(c > 0\\) : $$ \\min_{H(\\theta, \\theta') = 1} \\norm{\\mathbf{P}_\\theta \\wedge \\mathbf{P}_{\\theta'}} > c $$ By Lemmata 5 and 6, and taking \\(k = n&#94;\\frac{1}{2\\alpha+1}\\) , we have the desired bound: $$ \\sup_{\\mathcal{F}_{11}} \\mathbf{E} \\norm{\\hat\\Sigma - \\Sigma}&#94;2 \\geq \\frac{c&#94;2}{2}k&#94;2a&#94;2 \\geq c_1n&#94;{-\\frac{2\\alpha}{2\\alpha+1}} $$ for some \\(c_1 > 0\\) . As the bound is for an arbitrary \\(\\hat\\Sigma\\) , it follows that: $$ \\inf_{\\hat\\Sigma}\\sup_{\\mathcal{F}_{11}} \\mathbf{E} \\norm{\\hat\\Sigma - \\Sigma}&#94;2 \\geq cn&#94;{-\\frac{2\\alpha}{2\\alpha+1}} $$ Lower Bound by Le Cam's Method We now establish a lower bound on the second subparameter space \\(\\mathcal{F}_{12}\\) . We first begin with a statement of Le Cam's method. Suppose \\(X\\) is drawn from a distribution belonging to the collection \\(\\{ P_\\theta: \\theta \\in \\Theta\\}\\) where \\(\\Theta = \\{\\theta_0, \\dotsc, \\theta_{p_1}\\) . We further define \\(r(\\theta_0, \\theta_m) = \\inf_t [L(t, \\theta_0) + L(t, \\theta_m)]\\) , and \\(r_\\mathrm{min}(\\theta_0, \\theta_m)\\) . Finally, we denote \\(\\mathbf{\\bar P} = \\frac{1}{p_1} \\sum_{m=1}&#94;{p_1}\\mathbf{P}_{\\theta_m}\\) . Lemma 7. Let \\(T\\) be an estimator of \\(\\theta\\) based on an observation from a distribution in the collection \\(\\{\\mathbf{P}_\\theta: \\theta \\in \\Theta\\}\\) , then $$ \\sup_\\theta \\mathbf{E} L(T, \\theta) \\geq \\frac{1}{2}r_\\mathrm{min}\\norm{\\mathbf{P}_{\\theta_0}\\wedge \\mathbf{\\bar P}} $$ Justification for this lemma is addressed on another blog post focusing exclusively on Le Cam's two-point method, with guidance from Bin Yu's excellent Chapter 23 in Festschrift for Le Cam . To apply Le Cam's method, the authors construct a parameter set as follows: for \\(1 \\leq m \\leq p_1\\) , let \\(\\Sigma_m\\) be a diagonal covariance matrix with \\(\\sigma_{mm} = 1 + \\sqrt{\\tau\\frac{\\log p_1}{n}}\\) , \\(\\sigma_{ii} = 1\\) for \\(i \\neq m\\) , and let \\(\\Sigma_0\\) be the identity matrix. Suppose for each \\(m\\) we draw \\(X_1, \\dotsc, X_n \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(0, \\Sigma_m)\\) . Then the joint density \\(f_m\\) of \\(X_1, \\dotsc, X_n\\) is given by: $$ f_m = \\prod_{1\\leq i\\leq n, 1\\leq j\\leq p, j \\neq m} \\phi_1(x_j&#94;i) \\prod_{1 \\leq i \\leq n} \\phi_{\\sigma_{mm}}(x_m&#94;i) $$ where \\(\\phi_\\sigma\\) is the normal density with mean zero and variance \\(\\sigma\\) . We can see that the statement of the joint density follows naturally from the fact that each random vector is drawn independently from a normal density with diagonal covariance matrix (no correlation implies independence in the Gaussian case). First, we establish the bound on \\(\\norm{\\mathbf{P}_{\\theta_0} \\wedge \\mathbf{\\bar P}}\\) . Note that for two arbitrary densities \\(q_0, q_1\\) , we may rewrite the total variation affinity as one minus the total variation distance: $$ \\int q_0 \\wedge q_1 d\\mu = 1 - \\frac{1}{2}\\int |q_0-q_1|d\\mu $$ Then, we may free ourselves of the absolute value by writing the integral as though we are integrating with respect to \\(q_1\\) , and then apply Jensen's inequality: \\begin{align} \\left[\\int|q_0 - q_1|d\\mu\\right]&#94;2 &= \\left[\\int\\left(\\frac{|q_0 - q_1|}{q_1}\\right)q_1d\\mu\\right]&#94;2 \\\\ &\\leq\\int\\left(\\frac{|q_0 - q_1|}{q_1}\\right)&#94;2q_1d\\mu \\\\ &= \\int \\frac{q_0&#94;2-2q_0q_1+q_1&#94;2}{q_1}d\\mu \\\\ &= \\int \\frac{q_0&#94;2}{q_1} - 2q_0 + q_1d\\mu \\\\ &= \\int \\frac{q_0&#94;2}{q_1}d\\mu - 1 \\end{align} The \\(q_1&#94;2\\) in the denominator allows us to get rid of the \\(q_1\\) outside the fraction that we treated as our measure of integration when applying Jensen's inequality in line (2). This allows us to go on to establish a bound on the total variation affinity: $$ \\norm{\\mathbf{P}_{\\theta_0} \\wedge \\mathbf{\\bar P}} \\geq 1 - \\frac{1}{2}\\left(\\int \\frac{(\\frac{1}{p_1}\\sum f_m)&#94;2}{f_0}d\\mu - 1 \\right)&#94;\\frac{1}{2} $$ we need only show that \\(\\int \\frac{(\\frac{1}{p_1}\\sum f_m)&#94;2} {f_0}d\\mu - 1\\rightarrow 0\\) . Let's open up this term and see what we find. \\begin{align*} \\int \\frac{(\\frac{1}{p_1}\\sum f_m)&#94;2} {f_0}d\\mu - 1 &= \\int \\frac{(\\frac{1}{p_1}\\sum f_m)&#94;2} {f_0}d\\mu - 1 \\\\ &= \\frac{1}{p_1&#94;2} \\int \\frac{ \\sum_{m=1}&#94;{p_1} f_m&#94;2 + \\sum_{m\\neq j} f_m f_j } {f_0}d\\mu - 1 \\end{align*} Let's focus on the cross terms first. We can directly evaluate, for all \\(j, m\\) : \\begin{align*} \\int \\frac{f_jf_m}{f_0} d\\mu &= \\int \\frac{ \\prod_{1\\leq i \\leq n\\\\1 \\leq k \\leq p_1\\\\k\\neq j} \\phi_1(x_k&#94;i) \\prod_{1\\leq i \\leq n\\\\1 \\leq k \\leq p_1\\\\k\\neq m} \\phi_1(x_k&#94;i) \\prod_{1 \\leq i \\leq n} \\phi(x_m&#94;i)\\phi(x_j&#94;i) }{ \\prod_{1 \\leq i \\leq n\\\\1\\leq k \\leq p_1} \\phi_1(x_k&#94;i) }d\\left\\{x_k&#94;i\\right\\}_{1 \\leq i \\leq n\\\\1\\leq k \\leq p_1}\\\\ &\\text{(Independence.)} \\\\ &= \\prod_{1\\leq i \\leq n}\\int \\frac{ \\left[\\prod_{1 \\leq k \\leq p_1\\\\k\\neq j} \\phi_1(x_k&#94;i)\\right] \\left[\\prod_{1 \\leq k \\leq p_1\\\\k\\neq m} \\phi_1(x_k&#94;i)\\right] \\phi(x_m&#94;i)\\phi(x_j&#94;i) }{ \\prod_{1\\leq k \\leq p_1} \\phi_1(x_k&#94;i) }d\\left\\{x_k&#94;i\\right\\}_{1\\leq k \\leq p_1} \\\\ &= \\prod_{1\\leq i\\leq n}\\int \\left[\\prod_{1 \\leq k \\leq p_1\\\\k \\not\\in\\{j, m\\}} \\phi_1(x_k&#94;i)\\right] \\phi(x_m&#94;i)\\phi(x_j&#94;i) d\\left\\{x_k&#94;i\\right\\}_{1\\leq k \\leq p_1} \\\\ &\\text{(Independence.)} \\\\ &= \\prod_{1\\leq i\\leq n} \\left[ \\left[ \\prod_{1 \\leq k \\leq p_1\\\\k \\not\\in\\{j, m\\}} \\int \\phi_1(x_k&#94;i)d x_k&#94;i \\right] \\int\\phi(x_m&#94;i)d x_m&#94;i \\int\\phi(x_j&#94;i)d x_j&#94;i \\right] \\\\ &= \\prod_{1\\leq i\\leq n} 1 \\\\ &= 1 \\end{align*} Now, we'll look at the squared terms. We have: \\begin{align*} \\int \\frac{f_m&#94;2}{f_0} d\\mu &= \\int \\frac{ \\prod_{1\\leq i \\leq n\\\\1 \\leq j \\leq p_1\\\\j\\neq m} \\phi_1(x_j&#94;i)&#94;2 \\prod_{1 \\leq i \\leq n} \\phi(x_m&#94;i)&#94;2 }{ \\prod_{1 \\leq i \\leq n\\\\1\\leq j \\leq p_1} \\phi_1(x_j&#94;i) }d\\left\\{x_j&#94;i\\right\\}_{1 \\leq i \\leq n\\\\1\\leq j \\leq p_1}\\\\ &\\text{(Independence.)} \\\\ &= \\prod_{1\\leq i \\leq n}\\int \\frac{ \\left[\\prod_{1 \\leq j \\leq p_1\\\\j\\neq m} \\phi_1(x_j&#94;i)&#94;2\\right] \\phi(x_m&#94;i)&#94;2 }{ \\prod_{1\\leq j \\leq p_1} \\phi_1(x_j&#94;i) }d\\left\\{x_j&#94;i\\right\\}_{1\\leq j \\leq p_1} \\\\ &= \\prod_{1\\leq i \\leq n}\\int \\frac{ \\left[\\prod_{1 \\leq j \\leq p_1\\\\j\\neq m} \\phi_1(x_j&#94;i)\\right] \\phi(x_m&#94;i)&#94;2 }{ \\phi_1(x_m&#94;i) }d\\left\\{x_j&#94;i\\right\\}_{1\\leq j \\leq p_1} \\\\ &= \\prod_{1\\leq i \\leq n} \\left[ \\prod_{1 \\leq j \\leq p_1\\\\j\\neq m} \\int \\phi_1(x_j&#94;i)dx_j&#94;i \\right] \\int \\frac{ \\phi(x_m&#94;i)&#94;2 }{ \\phi_1(x_m&#94;i) }dx_m&#94;i \\\\ &= \\prod_{1\\leq i \\leq n} \\int \\frac{ \\phi_{\\sigma_{mm}}(x_m&#94;i)&#94;2 }{ \\phi_1(x_m&#94;i) }dx_m&#94;i \\\\ \\end{align*} We now substitute in the form of the density functions and move the normalization terms out of the integral: \\begin{align*} \\prod_{1\\leq i \\leq n} \\int \\frac{ \\phi_{\\sigma_{mm}}(x_m&#94;i)&#94;2 }{ \\phi_1(x_m&#94;i) }dx_m&#94;i &= \\frac{ (\\sqrt{2\\pi\\sigma_{mm}})&#94;{-2n} }{ (\\sqrt{2\\pi})&#94;{-n} } \\prod_{1\\leq i \\leq n} \\int \\exp\\left\\{-2\\cdot\\frac{(x_m&#94;i)&#94;2}{2\\sigma_{mm}}\\right\\} \\exp\\left\\{\\frac{(x_m&#94;i)&#94;2}{2}\\right\\}\\\\ &= \\frac{ (\\sqrt{2\\pi\\sigma_{mm}})&#94;{-2n} }{ (\\sqrt{2\\pi})&#94;{-n} } \\prod_{1\\leq i \\leq n} \\int \\exp\\left\\{ -\\frac{1}{2}\\left[ \\frac{(x_m&#94;i)&#94;2}{\\frac{\\sigma_{mm}}{2 - \\sigma_{mm}}} \\right] \\right\\} \\\\ &= \\frac{ (\\sqrt{2\\pi\\sigma_{mm}})&#94;{-2n} }{ (\\sqrt{2\\pi})&#94;{-n} } \\left( \\frac{2\\pi\\sigma_{mm}}{2 - \\sigma_{mm}} \\right)&#94;\\frac{n}{2} \\\\ &= (\\sqrt{\\sigma_{mm}})&#94;{-2n} \\left( \\frac{\\sigma_{mm}}{2 - \\sigma_{mm}} \\right)&#94;\\frac{n}{2} \\\\ &= (\\sqrt{\\sigma_{mm}})&#94;{-n}(\\sqrt{2 - \\sigma_{mm}})&#94;{-n} \\\\ &= \\left[ 2\\sigma_{mm} - \\sigma_{mm}&#94;2 \\right]&#94;{-\\frac{n}{2}} \\\\ &= \\left[ 1 - (1 - \\sigma_{mm})&#94;2 \\right]&#94;{-\\frac{n}{2}} \\\\ &= \\left( 1 - \\tau\\frac{\\log p_1}{n}\\right)&#94;{-\\frac{n}{2}} \\end{align*} Let us take \\(0 < \\tau < 1\\) . Then we have: \\begin{align*} \\frac{1}{p_1&#94;2}\\sum_{m=1}&#94;{p_1} \\left(\\frac{f_m&#94;2}{f_0}d\\mu - 1\\right) &\\leq \\frac{1}{p_1}\\left(1 - \\tau\\frac{\\log p_1}{n}\\right)&#94;{-\\frac{n}{2}} - \\frac{1}{p_1} \\\\ &= \\exp\\left\\{-\\log p_1 - \\frac{n}{2}\\log\\left( 1 - \\tau\\frac{\\log p_1}{n}\\right)\\right\\} -\\frac{1}{p_1} \\\\ &\\rightarrow 0 \\end{align*} where we exploit the fact that \\(\\log(1-x)\\geq -2x\\) for \\(0 < x < \\frac{1}{2}\\) . Combined with the previously proved fact that: $$ \\int \\frac{f_mf_j}{f_0}d\\mu -1 = 0 $$ We can conclude that: $$ \\frac{1}{p_1&#94;2}\\sum_{m=1}&#94;{p_1}\\int \\frac{f_m&#94;2}{f_0} d\\mu + \\frac{1}{p_1&#94;2}\\sum_{m \\neq j}\\int \\frac{f_mf_j}{f_0} d\\mu \\rightarrow 0 $$ allowing us to bound: $$ \\norm{\\mathbf{P}_{\\theta_0} - \\mathbf{\\bar P}} \\geq c $$ Finally, we give a bound on \\(r_\\mathrm{min}\\) . Let \\(\\theta_m = \\Sigma_m\\) for \\(0 \\leq m \\leq p_1\\) , and let the loss function \\(L\\) be the squared operator norm. Then we see that: \\begin{align*} r(\\theta_0, \\theta_m) &= r(\\Sigma_0, \\Sigma_m) \\\\ &= \\inf_t \\left[L(t, \\Sigma_0) + L(t, \\Sigma_m)\\right] \\end{align*} Observe that the operator norm in \\(\\ell_2\\) distance on a diagonal matrix is simply the largest element. Then, we may minimize the above quantity with \\(t_{ii} = 1 + \\frac{1}{2}\\sqrt{\\tau\\frac{\\log p_1}{n}}\\mathbf{1}\\{i = m\\}\\) . This gives us: \\begin{align*} r(\\theta_0, \\theta_m) &= 2\\cdot \\frac{1}{4}\\tau\\frac{\\log p_1}{n} \\\\ &= \\frac{1}{2}\\tau\\frac{\\log p_1}{n} \\end{align*} for \\(1 \\leq m \\leq p_1\\) , implying that \\(r_\\mathrm{min} = \\frac{1}{2}\\tau\\frac{\\log p_1}{n}\\) . Substituting this result back into the lower bound given by Le Cam's Method, we have: \\begin{align*} \\sup_\\theta \\mathbf{E} L(T, \\theta) &\\geq \\frac{1}{2}r_\\mathrm{min} \\norm{\\mathbf{P}_{\\theta_0} \\wedge \\mathbf{\\bar P}} \\\\ &= \\frac{c}{4}\\tau\\frac{\\log p_1}{n} \\\\ &\\leq c\\frac{\\log p_1}{n} \\end{align*} where \\(p_1 = \\max\\{p, \\exp\\{\\frac{n}{2}\\}\\}\\) . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"http://huisaddison.com/blog/discuss-covmat-optimal-convergence-pt2.html"},{"title":"Photography","text":"I recently picked up a new hobby: film photography. I've been shooting on Kodak Portra (400 speed) film with an old Rolleicord Twin Lens Reflex. Aside from being fun, the activity has guided me through thinking about different factors that affect an image; e.g., latent light, depth of field, exposure times. And the cost of each photograph has taught me to be more deliberate about what to attempt capturing. After having the film developed at a mom and pop shop down the street from my parents' house, I digitized the photographs with a Hasselblad film scanner graciously made available by the Yale Digital Media Center for the Arts . The film scanner costs upwards of ten thousand dollars (!), according to B&H Photo and Video. I uploaded a subset of the scans (jpeg-compressed) to Flickr . So far the results look pretty good. This being my first time, I didn't handle the negatives very well, and there are more than a few artifacts on the scans. For that, I apologize. Later on, I'll try my hand at developing black and white film.","tags":"Photography","url":"http://huisaddison.com/blog/photography-intro.html"},{"title":"Assouad's Lemma","text":"$$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\EE}{\\mathbf{E}} $$ Assouad's Lemma is a common tool for proving lower bounds on the maximum risk associated with an estimation problem, and its core it may be interpreted as a multiple comparisons version of Le Cam's Method. This post will first review the version discussed by Bin Yu in her chapter of Festschrift for Lucien Le Cam . After that, we will discuss a variation found in Aad van der Vaart's Asymptotic Statistics . We conclude with a brief discussion of its use. Assouad à la Bin Yu Preliminaries Assume that \\(\\mathcal{P}\\) is a family of probability measures and \\(\\theta(P)\\) is the parameter of interest with values in the pseudo-metric space \\((\\mathcal{D}, d)\\) . Let \\(\\hat\\theta = \\hat\\theta(X)\\) be an estimator based on an \\(X\\) drawn from a distribution \\(P\\) . Statement Bin Yu gives the following statement of Assouad's Lemma in Chapter 29: Lemma 2 (Assouad's Lemma) Let \\(m\\geq 1\\) be an integer and let \\(\\mathcal{F}_m = \\{P_\\tau: \\tau \\in \\{-1, +1\\}&#94;m\\}\\) contain \\(2&#94;m\\) probability measures. Write \\(\\tau \\sim \\tau'\\) if \\(\\tau\\) and \\(\\tau'\\) differ in only one coordinate, and write \\(\\tau \\sim_j \\tau'\\) when that coordinate is the jth. Suppose that there are \\(m\\) pseudo-distances on \\(\\mathcal{D}\\) such that for any \\(x, y \\in \\mathcal{D}\\) $$ d(x, y) = \\sum_{j=1}&#94;m d_j(x, y) $$ and further, that if \\(\\tau\\sim_j\\tau'\\) , $$ d_j(\\theta(P_\\tau), \\theta(P_{\\tau'})) \\geq \\alpha_m $$ Then $$ \\max_{P_\\tau\\in\\mathcal{F}_m} \\mathbf{E}_\\tau d(\\hat\\theta, \\theta(P_\\tau)) \\geq m\\cdot \\frac{\\alpha_m}{2}\\min\\{\\norm{P_\\tau\\wedge P_{\\tau'}}: \\tau\\sim\\tau'\\} $$ Proof We now discuss the proof provided. For each tuple \\(\\tau = (\\tau_1, \\dotsc, \\tau_m)\\) , let \\(\\tau&#94;j\\) denote the \\(m\\) -tuple that differs only at the jth index. Then \\(d(\\theta(\\tau), \\theta(P_{\\tau&#94;j}))\\geq \\alpha_m\\) by assumption. The key idea in this proof that we may bound the maximum risk over \\(\\tau\\) from below by the average over the risk for each \\(\\tau\\) , and then apply Le Cam's Method many times to get our desired bound. But first, we use the assumption that we may decouple our pseudo-distance metric across the members of \\(\\tau\\) : \\begin{align*} \\max_\\tau \\EE_\\tau d(\\theta(P_\\tau), \\hat\\theta) &= \\max_\\tau\\sum_{j=1}&#94;m \\EE_{\\tau_j} d(\\theta(P_\\tau), \\hat\\theta) \\end{align*} after which we use the average to lower bound and rearrange the summations: \\begin{align*} \\max_\\tau \\EE_\\tau d(\\theta(P_\\tau), \\hat\\theta) &= \\max_\\tau\\sum_{j=1}&#94;m \\EE_{\\tau_j} d(\\theta(P_\\tau), \\hat\\theta)\\\\ &\\geq 2&#94;{-m}\\sum_\\tau \\sum_{j=1}&#94;m \\EE_\\tau d_j(\\theta(P_\\tau), \\hat\\theta) \\\\ &= \\sum_{j=1}&#94;m2&#94;{-m}\\sum_\\tau \\EE_\\tau d_j(\\theta(P_\\tau), \\hat\\theta) \\end{align*} afterwards, Yu cleverly rearranges the terms so that each \\(\\tau\\) is matched up with a \\(\\tau&#94;j\\) by strategically adding a copy of each term and then dividing through by a half: \\begin{align*} \\sum_{j=1}&#94;m2&#94;{-m}&\\sum_\\tau \\EE_\\tau d_j(\\theta(P_\\tau), \\hat\\theta) \\\\ &= \\sum_{j=1}&#94;m2&#94;{-m}\\sum_\\tau \\frac{1}{2} \\left( \\EE_\\tau d_j(\\theta(P_\\tau), \\hat\\theta) + \\EE_{\\tau&#94;j} d_j(\\theta(P_{\\tau&#94;j}), \\hat\\theta) \\right) \\end{align*} For each \\(\\tau\\) and \\(j\\) , consider the pair of associated hypotheses \\(P_\\tau\\) and \\(P_{\\tau&#94;j}\\) . By using Le Cam's Method, we may bound the average estimation error for each of these pairs from below: \\begin{align*} \\max_\\tau \\EE_\\tau d(\\theta(P_\\tau), \\hat\\theta) &\\geq \\sum_{j=1}&#94;m 2&#94;{-m} \\sum_\\tau \\frac{\\alpha_m}{2} \\norm{P_\\tau \\wedge P_{\\tau&#94;j}} \\\\ &\\geq m \\frac{\\alpha_m}{2} \\min\\{\\norm{P_\\tau \\wedge P_{\\tau&#94;j}}: \\tau\\sim\\tau'\\} \\end{align*} giving our desired bound. Assouad à la Aad van der Vaart The paper that I've been reading lately by Zhou et al uses a slightly different version of Assouad's Lemma, elaborated by van der Vaart in his book Asymptotic Statistics. My post about lower bounds discussed in that paper (which takes advantage of Assouad's Lemma) may be found here . The version discussed by van der Vaart relaxes the assumption that the distance metric decouple across \\(j\\) . Preliminaries Suppose we have a parameter set \\(\\Theta = \\{0, 1\\}&#94;r\\) and are estimating an arbitrary quantity \\(\\psi(\\theta)\\) , belonging to a metric space with metric \\(d\\) . Statement 24.3 Lemma (Assouad). For any estimator \\(T\\) based on an observation in the experiment ( \\(P_\\theta: \\theta\\in\\{0, 1\\}&#94;r\\) ), and any \\(p > 0\\) , $$ \\max_\\theta 2&#94;p \\EE_\\theta d&#94;p(T, \\psi(\\theta)) \\geq \\min_{H(\\theta, \\theta') \\geq 1} \\frac{ d&#94;p(\\psi(\\theta), \\psi(\\theta')) }{ H(\\theta, \\theta') } \\frac{r}{2} \\min_{H(\\theta, \\theta') = 1} \\norm{P_\\theta \\wedge P_{\\theta'}} $$ We see that the first term in the lower bound, which gives the minimum distance between two parameters indexed by the vertices of a hypercube, is normalized by the Hamming distance between the vertex indices, whereas in Yu's version of Assouad's Lemma the assumption that we could decompose the distance metric allowed us to only look at parameters corresponding to neighboring vertices. Proof We define an estimator \\(S\\) as follows: $$ S \\in\\arg\\min_{\\{0, 1\\}&#94;r} d(T, \\psi(S)) $$ For any \\(\\theta\\) , we have the lower bound: \\begin{align*} d(\\psi(S), \\psi(\\theta) &\\leq d(\\psi(S), T) + d(\\psi(\\theta), T) &&\\text{(Triangle Inequality.)} \\\\ &\\leq 2d(\\psi(\\theta), T) &&\\text{(By definition of $S$)} \\end{align*} Pick \\(\\alpha\\) such that for all \\(\\theta \\neq \\theta'\\) : $$ d&#94;p(\\psi(\\theta), \\psi(\\theta')) \\geq \\alpha H(\\theta, \\theta') $$ Equivalently, $$ \\alpha = \\min_{\\theta \\neq \\theta'} \\frac{d&#94;p(\\psi(\\theta), \\psi(\\theta'))}{H(\\theta, \\theta')} $$ Therefore, we may bound: \\begin{align*} 2&#94;p\\EE_\\theta d&#94;p(T, \\psi(\\theta)) &\\geq 2&#94;p \\EE_\\theta \\frac{1}{2&#94;p}d&#94;p(\\psi(S), \\psi(\\theta)) \\\\ &\\geq \\alpha \\EE_\\theta H(S, \\theta) \\end{align*} Let's focus on the expectation term and bound the maximum by the average. \\begin{align*} \\max_\\theta \\EE_\\theta H(S, \\theta) &\\geq \\frac{1}{2&#94;r}\\sum_\\theta \\EE_\\theta H(S, \\theta) \\\\ & \\frac{1}{2&#94;r}\\sum_\\theta \\sum_{j=1}&#94;r \\EE_\\theta|S_j-\\theta_j| \\\\ & \\frac{1}{2&#94;{r-1}} \\sum_{j=1}&#94;r \\sum_{\\theta: \\theta_j \\in\\{0, 1\\}} \\frac{1}{2} \\left( \\EE_{\\theta: \\theta_j = 0} |S_j-\\theta_j| + \\EE_{\\theta: \\theta_j = 1} |S_j-\\theta_j| \\right) \\\\ &= \\frac{1}{2&#94;{r-1}} \\sum_{j=1}&#94;r \\sum_{\\theta: \\theta_j \\in\\{0, 1\\}} \\frac{1}{2} \\left( \\int (1 - S_j) p_{\\theta:\\theta_j = 0}\\;d\\mu + \\int S_j p_{\\theta: \\theta_j = 1}\\;d\\mu \\right) \\\\ &\\geq \\frac{1}{2&#94;{r-1}} \\sum_{j=1}&#94;r \\sum_{\\theta: \\theta_j \\in\\{0, 1\\}} \\frac{1}{2} \\Bigg( \\int (1 - S_j) \\min\\{p_{\\theta:\\theta_j = 0}, p_{\\theta: \\theta_j = 1}\\}\\;d\\mu \\\\ &\\qquad\\qquad\\qquad\\qquad\\qquad+ \\int S_j \\min\\{p_{\\theta:\\theta_j = 0}, p_{\\theta: \\theta_j = 1}\\}\\;d\\mu \\Bigg) \\\\ &= \\frac{1}{2} \\sum_{j=1}&#94;r \\frac{1}{2&#94;{r-1}} \\sum_{\\theta: \\theta_j \\in\\{0, 1\\}} \\int \\min\\{p_{\\theta:\\theta_j = 0}, p_{\\theta: \\theta_j = 1}\\}\\;d\\mu \\\\ &\\geq \\frac{1}{2} \\sum_{j=1}&#94;r \\frac{1}{2&#94;{r-1}} \\sum_{\\theta: \\theta_j \\in\\{0, 1\\}} \\norm{P_{\\theta:\\theta_j = 0} \\wedge P_{\\theta: \\theta_j = 1}}\\\\ &\\geq \\frac{1}{2} \\sum_{j=1}&#94;r \\min_{H(\\theta, \\theta') = 1} \\norm{P_{\\theta} \\wedge P_{\\theta'}} \\\\ &= \\frac{r}{2} \\min_{H(\\theta, \\theta') = 1} \\norm{P_{\\theta} \\wedge P_{\\theta'}} \\end{align*} Appending the factor of \\(\\alpha\\) previously calculated, we have our desired bound: \\begin{align*} \\max_\\theta 2&#94;p \\EE_\\theta d&#94;p(T, \\psi(\\theta)) &\\geq \\alpha \\frac{r}{2} \\min_{H(\\theta, \\theta') = 1} \\norm{P_\\theta \\wedge P_{\\theta'}} \\\\ &\\geq \\min_{H(\\theta, \\theta') \\geq 1} \\frac{ d&#94;p(\\psi(\\theta), \\psi(\\theta')) }{ H(\\theta, \\theta') } \\frac{r}{2} \\min_{H(\\theta, \\theta') = 1} \\norm{P_\\theta \\wedge P_{\\theta'}} \\end{align*} Discussion As previously observed, Assouad's Lemma may be decomposed into a set of two-point comparisons for which Le Cam's Method may be (and is) applied. So why bother with Assouad? By simultaneously considering, say, \\(m\\) two-point comparisons, we are able to push up our lower bound by a factor of \\(m\\) corresponding to the dimensionality of the associated hypercube, which can be convenient or necessary to establish the optimality of an estimator; it certainly doesn't hurt to have a tighter bound. As Yu remarks in her note, \"it is known that Assouad's Lemma (Lemma 2) gives very effective lower bounds for many global estimation problems.\" Of course, we must satisfy the assumptions on the distance between parameters in our space, as well as work to construct an intelligent mapping from the vertices of the hypercube to the parameters themselves so that the pairwise application of Le Cam succeeds. Regardless, it's delightful to see that Le Cam's Method can be found at the heart of such a useful tool. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"http://huisaddison.com/blog/assouads-lemma.html"},{"title":"Discussion: Optimal Rates of Convergence for Covariance Matrix Estimation, Part 1","text":"For the past couple of days, I have been reading Optimal Rates of Convergence for Covariance Matrix Estimation . To aid my digestion of this paper, I will be writing about it on this blog. This post covers the authors' justification of Theorem 2 . I highly recommend referring to the paper; I will try to fill in gaps of the proof, but must leave out some details for brevity. Background The authors in this paper establish optimal rates of convergence for estimating the covariance matrix under the operator norm (which, in Euclidean distance, coincides with the spectral norm) and the Frobenius norm; i.e., they provide lower bounds on the max risk and show that the minimax upper bound of their tapering estimators achieves this rate. First, they construct the following parameter space of \\(p\\times{p}\\) covariance matrices: \\begin{equation}\\label{eq:parameter-space} \\mathcal{F}(M_0, M) = \\left\\{ \\Sigma: \\max_j \\sum_j \\{|\\sigma_{ij}|: |i-j| > k\\} \\leq M k&#94;{-\\alpha} \\text{ for all } k, \\text{and} \\lambda_{\\text{max}}(\\Sigma)\\leq M_0 \\right\\} \\end{equation} This definition states that for the covariance matrices in this space, the sum of the absolute values of the entries \\(k\\) away from the diagonal decay with \\(k&#94;{-\\alpha}\\) , where \\(\\alpha\\) is a smoothing parameter. The bound will play an important proof in the later proofs. Theorem. The minimax risk of estimating the covariance matrix \\(\\Sigma\\) over the class \\(\\mathcal{P}_\\alpha\\) given above satisfies $$ \\inf_{\\hat\\Sigma}\\sup_{\\mathcal{P}_\\alpha} \\mathbf{E}\\Vert \\hat\\Sigma - \\Sigma\\Vert&#94;2 \\asymp \\min\\left\\{n&#94;{-\\frac{2\\alpha}{2\\alpha+1}}+\\frac{\\log p}{n}, \\frac{p}{n}\\right\\} $$ where \\(\\mathcal{P}_\\alpha = \\mathcal{P}_\\alpha(M_0, M, \\rho)\\) is the set of all distributions of \\(X\\) that satisfies both Equations \\eqref{eq:parameter-space} and \\eqref{eq:subgaussian} \\(\\Sigma\\) is then estimated by tapering the maximum likelihood estimator: \\begin{equation}\\label{eq:tapering-estimator} \\hat\\Sigma = \\hat\\Sigma_k = \\left(w_{ij}\\sigma&#94;*_{ij}\\right)_{p\\times p} \\end{equation} where \\(\\sigma&#94;*_{ij}\\) are the entries in the maximum likelihood estimator \\(\\Sigma&#94;*\\) and the weights are given by: \\begin{align*} w_{ij} = \\begin{cases} 1 &\\text{ for } |i - j| \\leq \\frac{k}{2} \\\\ 2 - 2\\frac{|i-j|}{k} &\\text{ for } \\frac{k}{2} \\leq |i - j| \\leq k \\\\ 0 &\\text{ otherwise } \\end{cases} \\end{align*} The bandwidth is \\(k\\) on either side along the diagonal; shrinkage begins on each side at \\(k/2\\) . As a technical note, such a tapering estimator may be rewritten as a sum of block matrices along the diagonal; this is used in the proofs to attain concentration bounds via random matrix theory. Minimax Upper Bound under the Operator Norm The authors assume that the \\(X_i\\) 's are subgaussian; that is, there exists \\(\\rho > 0\\) such that: \\begin{equation}\\label{eq:subgaussian} \\mathbf{P}\\{|v&#94;T(X_1 - \\mathbf{E} X_1)| > t\\} \\leq \\exp\\left\\{ -\\frac{t&#94;2\\rho}{2}\\right\\} \\end{equation} for all \\(t > 0\\) and \\(\\Vert{v}\\Vert_2=1\\) . Theorem. The tapering estimator \\(\\hat\\Sigma_k\\) defined in Equation \\eqref{eq:tapering-estimator} with \\(p \\geq n&#94;{\\frac{1}{2\\alpha+1}}\\) satisfies \\begin{equation} \\sup_{\\mathcal{P}_\\alpha}\\mathbf{E}\\Vert \\hat\\Sigma_k - \\Sigma\\Vert&#94;2 \\leq C \\frac{k+\\log p}{n} + Ck&#94;{-2\\alpha} \\end{equation} for \\(k = O(n), \\log p = O(n)\\) and some constant \\(C > 0\\) . To prove this, the authors assume \\(\\mu = 0\\) and analyze $$ \\tilde \\Sigma = \\frac{1}{n}\\sum_{i=1}&#94;n X_l X_l&#94;\\top $$ rather than the maximum likelihood estimator $$ \\Sigma&#94;* = \\frac{1}{n}\\sum_{i=1}&#94;n X_lX_l&#94;\\top - \\bar X \\bar X&#94;\\top $$ as \\(\\bar X \\bar X&#94;\\top\\) is a higher order term and can be neglected in the analysis of the rate. As such, we defined a new tapering estimator: $$ \\breve\\Sigma = \\left(\\breve\\sigma_{ij}\\right)_{1 \\leq i,j \\leq p} = \\left(w_{ij}\\tilde\\sigma_{ij}\\right)_{1 \\leq i,j \\leq p} $$ We observe that we may bound the risk from above by the bias and variance: \\begin{align*} \\mathbf{E} \\Vert \\breve\\Sigma - \\Sigma\\Vert&#94;2 &\\leq 2\\mathbf{E}\\Vert \\breve\\Sigma - \\mathbf{E} \\breve\\Sigma\\Vert&#94;2 + 2\\Vert\\mathbf{E}\\breve\\Sigma- \\Sigma\\Vert&#94;2 \\end{align*} This inequality can easily be derived by thinking about how to bound \\((a + b)&#94;2\\) . Bias To prove the bound on the bias, the authors note that the operator norm of a symmetric matrix is upper bounded by its \\(\\ell_1\\) norm. Therefore, we have: \\begin{equation}\\label{eq:operator-upperbound-bias} \\Vert \\mathbf{E}\\breve\\Sigma - \\Sigma\\Vert&#94;2 \\leq \\left[\\max_{i=1, \\dotsc, p} \\sum_{j: |i-j| > k} |\\sigma_{ij}|\\right]&#94;2 \\leq M&#94;2 k&#94;{-2\\alpha} \\end{equation} This inequality holds by construction (see Equation \\eqref{eq:parameter-space}). Variance The authors rely on random matrix theory to bound the variance. Of particular note is Lemma 2. Define the submatrix \\(M_l&#94;{(m)}\\) : $$ M_l&#94;{(m)} = \\left( \\tilde\\sigma_{ij}\\mathbf{1}\\{l \\leq i < l + m, l \\leq j < l + m\\} \\right)_{p\\times p} $$ then we have the following bound: $$ \\Vert \\breve\\Sigma - \\mathbf{E}\\breve\\Sigma\\Vert \\leq 3 N&#94;{(m)} = \\max_{1 \\leq l \\leq p - m + 1}\\Vert M_l&#94;{(m)} - \\mathbf{E} M_l&#94;{(m)}\\Vert $$ They also provide a concentration bound on the operator norm of this submatrix. Lemma 3. There is a constant \\(\\rho_1 > 0\\) such that $$ \\mathbf{P}\\left\\{N_l&#94;{(m)} > x\\right\\} \\leq 2p5&#94;m\\exp(-nx&#94;2\\rho_1) $$ for all \\(0 < x < \\rho_1\\) and \\(1-m \\leq l \\leq p\\) . Therefore, by Lemma 2, we have: \\begin{align*} \\mathbf{E}\\Vert \\breve\\Sigma - \\mathbf{E}\\breve\\Sigma\\Vert&#94;2 &\\leq 9 \\mathbf{E}\\left(N&#94;{(m)}\\right)&#94;2 \\\\ &= 9 \\mathbf{E}\\left(N&#94;{(m)}\\right)&#94;2[\\mathbf{1}(N&#94;{(m)} \\leq x) + \\mathbf{1}(N&#94;{(m)} > x)] \\\\ &\\leq 9 [x&#94;2 +\\mathbf{E} \\left(N&#94;{(m)}\\right)&#94;2\\mathbf{1}(N&#94;{(m)} > x)] \\end{align*} The next few steps are quite tricky to understand. I believe that we can recall the definition of \\(N&#94;{(m)}\\) : $$ N&#94;{(m)} = \\Vert M_l&#94;{(m)} - \\mathbf{E} M_l&#94;{(m)}\\Vert $$ for some \\(l\\) . We note that we may bound the operator norm of a submatrix by the operator norm of the full matrix by recalling the definition of an operator norm $$ \\Vert A \\Vert_{\\mathrm{op}} = \\sup\\{\\Vert{Av}\\Vert: v \\in V, \\Vert v \\Vert = 1\\} $$ and seeing that for any vector \\(v\\) , \\(\\Vert A_\\mathrm{sub} v \\Vert \\leq \\Vert Av\\Vert\\) . Therefore, we may establish that: $$ \\Vert M_l&#94;{(m)} - \\mathbf{E} M_l&#94;{(m)}\\Vert \\leq \\Vert \\breve\\Sigma - \\mathbf{E}\\breve\\Sigma\\Vert $$ at this point, we may split the norm by the triangle inequality: $$ \\Vert \\breve\\Sigma - \\mathbf{E}\\breve\\Sigma\\Vert \\leq \\Vert \\breve\\Sigma\\Vert + \\Vert\\mathbf{E}\\breve\\Sigma\\Vert $$ and bound the operator norm by the Frobenius norm: $$ \\Vert \\breve\\Sigma\\Vert + \\Vert\\mathbf{E}\\breve\\Sigma\\Vert \\leq \\Vert\\breve\\Sigma\\Vert_F + C $$ Finally, we may apply Cauchy-Schwarz: \\begin{align*} \\mathbf{E}\\Vert \\breve\\Sigma - \\mathbf{E}\\breve\\Sigma\\Vert&#94;2 &\\leq C_1 \\left[x&#94;2 + \\mathbf{E}\\left(\\Vert\\breve\\Sigma\\Vert_F + C\\right)&#94;2 \\mathbf{1}(N&#94;{(m)} > x)\\right] \\\\ &\\leq C_1 \\left[x&#94;2 + \\sqrt{\\mathbf{E}\\left(\\Vert\\breve\\Sigma\\Vert_F + C\\right)&#94;4} \\sqrt{\\mathbf{P}(N&#94;{(m)} > x)}\\right] \\end{align*} We now bound the \\(\\sqrt{\\mathbf{P}(N&#94;{(m)} > x)}\\) term. By setting \\(x=4\\sqrt{\\frac{\\log{p}+m}{n\\rho_1}}\\) , and recalling Lemma 3 , we have: \\begin{align*} \\sqrt{\\mathbf{P}(N&#94;{(m)} > x)} &\\leq \\sqrt{2p5&#94;m \\exp\\{-nx&#94;2\\rho_1\\}} \\\\ &\\leq \\sqrt{2p5&#94;m \\exp\\{-16\\log p + -16m\\}} \\\\ &\\leq \\sqrt{2p5&#94;m \\cdot p&#94;{-16}\\exp\\{-16m\\}} \\\\ \\end{align*} We are able to bound the Frobenius norm: $$ \\sqrt{\\mathbf{E}\\left(\\Vert\\breve\\Sigma\\Vert_F + C\\right)&#94;4} \\leq Cp&#94;2 $$ by observing that the squared Frobenius norm decouples the entries of the matrix, and we are able to bound each of the \\(p&#94;2\\) entries by a constant, which hides among the other constants. With all these pieces together, we may conclude: \\begin{align*} \\mathbf{E}\\Vert \\breve\\Sigma - \\mathbf{E}\\breve\\Sigma\\Vert&#94;2 &\\leq C\\left[ \\frac{\\log p + m}{n} + \\underbrace{ p&#94;2\\cdot(p5&#94;m)&#94;\\frac{1}{2}\\cdot p&#94;{-8} \\exp\\{-8m\\} }_\\text{Lower Order Term} \\right] \\\\ &\\leq C_1 \\left(\\frac{\\log p + m}{n}\\right) \\end{align*} where \\(C\\) is an evolving constant. Putting It All Together Having walked through the steps of bounding the bias: $$ \\Vert \\mathbf{E}\\breve\\Sigma - \\Sigma\\Vert&#94;2 \\leq M&#94;2 k&#94;{-2\\alpha} = Ck&#94;{-2\\alpha} $$ and variance: $$ \\mathbf{E}\\Vert \\breve\\Sigma - \\mathbf{E}\\breve\\Sigma\\Vert&#94;2 \\leq C \\left(\\frac{\\log p + m}{n}\\right) $$ for arbitrary \\(\\Sigma\\) , we can put these terms together to establish the bound on the worse case risk of the tapering estimator ( Theorem 2 ): \\begin{equation} \\sup_{\\mathcal{P}_\\alpha}\\mathbf{E}\\Vert \\hat\\Sigma_k - \\Sigma\\Vert&#94;2 \\leq C \\frac{k+\\log p}{n} + Ck&#94;{-2\\alpha} \\end{equation} for \\(k = O(n), \\log p = O(n)\\) and some constant \\(C > 0\\) . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Statistics","url":"http://huisaddison.com/blog/discuss-covmat-optimal-convergence-pt1.html"},{"title":"Post One","text":"This is my first post in my new blog, powered by Pelican. I choose Pelican primarily because it is written in Python, a versatile language that I enjoy working with, and because it plays well with MathJax via the Math Render plugin. Please look forward to posts about statistics, cooking, programming, and amateur photography. And expect the theme of to change within the coming days (or weeks!)","tags":"Python","url":"http://huisaddison.com/blog/my-first-post.html"},{"title":"Trying Out MathJax","text":"Pelican has the ability to render mathematics using MathJax by way of the Math Render plugin. Here, we show the Cauchy-Schwarz Inequality: $$ \\langle f, g\\rangle&#94;2 \\leq \\langle f, f\\rangle \\cdot \\langle g, g\\rangle $$ And here is an align environment: \\begin{align*} w_{ij} = \\begin{cases} 1 &\\text{ for } |i - j| \\leq \\frac{k}{2} \\\\ 2 - 2\\frac{|i-j|}{k} &\\text{ for } \\frac{k}{2} \\leq |i - j| \\leq k \\\\ 0 &\\text{ otherwise } \\end{cases} \\end{align*} if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Pelican","url":"http://huisaddison.com/blog/trying-out-mathjax.html"}]}