<!DOCTYPE html>
<html>
<head>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link rel="Stylesheet" type="text/css" href="style.css">
<title>math300</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>

<div id="Topics in Analysis"><h1 id="Topics in Analysis">Topics in Analysis</h1></div>
<div id="Topics in Analysis-Quick links"><h2 id="Quick links">Quick links</h2></div>
<ul>
<li>
<a href="&#47;home&#47;addison&#47;Dropbox&#47;wiki&#47;archive&#47;math300_syllabus.pdf">Syllabus</a>

</ul>
<div id="Topics in Analysis-Homeworks"><h2 id="Homeworks">Homeworks</h2></div>
<ul>
<li>
<a href="&#47;home&#47;addison&#47;Dropbox&#47;wiki&#47;archive&#47;math300_pset01.pdf">Problem Set 1.</a>  <em>Due Friday, January 29th</em>

<li>
<a href="&#47;home&#47;addison&#47;Dropbox&#47;wiki&#47;archive&#47;math300_pset02.pdf">Problem Set 2.</a>  <em>Due Friday, February 5th</em>

<li>
<a href="&#47;home&#47;addison&#47;Dropbox&#47;wiki&#47;archive&#47;math300_pset03.pdf">Problem Set 3.</a>  <em>Due Friday, February 12th</em>

<li>
<a href="&#47;home&#47;addison&#47;Dropbox&#47;wiki&#47;archive&#47;math300_pset04.pdf">Problem Set 4.</a>  <em>Due Friday, February 19th</em>

<li>
<a href="&#47;home&#47;addison&#47;Dropbox&#47;wiki&#47;archive&#47;math300_pset05.pdf">Problem Set 5.</a>  <em>Due Friday, February 26th</em>

<li>
<a href="&#47;home&#47;addison&#47;Dropbox&#47;wiki&#47;archive&#47;math300_pset06.pdf">Problem Set 6.</a>  <em>Due Monday, March 7th</em>

<li>
<a href="&#47;home&#47;addison&#47;Dropbox&#47;wiki&#47;archeve&#47;math300_pset07.pdf">Problem Set 7.</a>  <em>Due Friday, March 11th</em>

</ul>
<div id="Topics in Analysis-Notes"><h2 id="Notes">Notes</h2></div>
<ul>
<li>
<a href="math300.html# Lecture 1; Introduction and Proof Techniques">Lecture 1; Introduction and Proof Techniques</a>    

<li>
<a href="math300.html#Lecture 2; Induction, function classes, infinity">Lecture 2; Induction, function classes, infinity</a>  

<li>
<a href="math300.html#Lecture 3; Countably infinite sets">Lecture 3; Countably infinite sets</a>  

<li>
<a href="math300.html#Lecture 4; Set theory, sequences">Lecture 4; Set theory, sequences</a>   

<li>
<a href="math300.html#Lecture 5; Convergence">Lecture 5; Convergence</a>

<li>
<a href="math300.html#Lecture 6; Markov chains, square roots">Lecture 6; Markov chains, square roots</a>

<li>
<a href="math300.html#Lecture 7; Cauchy sequences, series">Lecture 7; Cauchy sequences, series</a>

<li>
<a href="math300.html#Lecture 8; Supremum and Infimum">Lecture 8; Supremum and Infimum</a>

<li>
<a href="math300.html#Lecture 9; Supremum, infimum, Bolzano-Weierstrass">Lecture 9; Supremum, infimum, Bolzano-Weierstrass</a>

<li>
<a href="math300.html#Lecture 10; Continuity">Lecture 10; Continuity</a>

<li>
<a href="math300.html#Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem">Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem</a>

<li>
<a href="math300.html#Lecture 12; Intermediate value theorem, uniform continuity">Lecture 12; Intermediate value theorem, uniform continuity</a>

<li>
<a href="math300.html#Lecture 13; Lipschitz continuity, Riemann sums">Lecture 13; Lipschitz continuity, Riemann sums</a>

<li>
<a href="math300.html#Lecture 15; Riemann sums, improper integrals">Lecture 15; Riemann sums, improper integrals</a>

<li>
<a href="math300.html#Lecture 16; Improper integrals, cont., differentiability">Lecture 16; Improper integrals, cont., differentiability</a>

<li>
<a href="math300.html#Lecture 17; Differentiation, cont., mean value theorem">Lecture 17; Differentiation, cont., mean value theorem</a>

<li>
<a href="math300.html#Lecture 18; Fundamental theorem of Calculus">Lecture 18; Fundamental theorem of Calculus</a>

<li>
<a href="math300.html#Lecture 19; Approximating area, Taylor and MacLaurin">Lecture 19; Approximating area, Taylor and MacLaurin</a>

</ul>
<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques"><h3 id="Lecture 1; Introduction and Proof Techniques">Lecture 1; Introduction and Proof Techniques</h3></div>
<p>
<em>Wednesday, January 20th</em>
</p>
<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Introduction"><h4 id="Introduction">Introduction</h4></div>
<ul>
<li>
Stefan's office hours will be voted upon after shopping period

<li>
Grading scheme will be voted upon after shopping period

</ul>
<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Proof Techinques"><h4 id="Proof Techinques">Proof Techinques</h4></div>
<p>
A proof is a formal argument that would <em>convince your enemy</em>.
Types of proofs:
</p>
<ol>
<li>
Direct

<li>
Indirect (assume false, and follow logic until arriving at nonsense)

<li>
Induction

</ol>
<p>
The direct and indirect methods have been known since antiquity.
</p>
<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Direct Proofs"><h4 id="Direct Proofs">Direct Proofs</h4></div>
<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Direct Proofs-Example: Direct Proof"><h5 id="Example: Direct Proof">Example: Direct Proof</h5></div>
<p>
Statement: For all \(x&gt;0\), \(x + \frac{1}{x} \geq 2\).
</p>
<ul>
<li>
 Because \(x\) is non-negative, \(\Leftrightarrow x^2 + 1 \geq 2x\)

<li>
\(\Leftrightarrow x^2 - 2x + 1 \geq 0\)

<li>
\(\Leftrightarrow (x-1)^2 \geq 0\)

<li>
The square of any number is always non-negative.  \(\square\)

</ul>
<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Direct Proofs-Example: Flawed Proof"><h5 id="Example: Flawed Proof">Example: Flawed Proof</h5></div>
<p>
If \(n\) is the largest possible integer, then \(n=1\).
<span id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Direct Proofs-Example: Flawed Proof-Proof."></span><strong id="Proof.">Proof.</strong>  Let \(n\) be the largest possible integer. The square of a \(n\) is greater than or equal to \(n\).  However, because \(n\) is the largest possible integer, we then have \(n^2 = n\).  It follows that:
</p>
\begin{align}
n^2 &amp;= n \\
n   &amp;= 1 \\
\end{align}
<p>
The flaw in the reasoning lies in the statement of what we wish to prove.  We decompose the statement into two parts:
</p>
<ol>
<li>
There is a largest possible integer.

<li>
That integer \(n=1\).

</ol>
<p>
The first part is nonsense.  <em>ex false sequitur quodlibet.</em>
</p>
<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Indirect Proofs"><h4 id="Indirect Proofs">Indirect Proofs</h4></div>
<p>
Steps:
</p>
<ol>
<li>
We want to show (p).

<li>
We assume (p) to be false.

<li>
Attempt to derive nonsense.

</ol>
<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Indirect Proofs-Example: Indirect Proof"><h5 id="Example: Indirect Proof">Example: Indirect Proof</h5></div>
<p>
<em>History</em>.  This proof was originally presented by Hippasus, a disciple of Pythagorus.  Unable to accept that not all numbers can be expressed as integer relations, Pythagorus murdered Hippasus.
</p>

<p>
We want to show that two is irrational.  In order words, \(\sqrt{2} \neq \frac{a}{b}\) for \(a, b \in \mathbb{N}\).
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Indirect Proofs-Example: Indirect Proof-Proof by contradiction."></span><strong id="Proof by contradiction.">Proof by contradiction.</strong>
</p>

<p>
<em>Step 1.</em> Assume there exists \(a, b \in \mathbb{N}\) such that \(\sqrt{2} = \frac{a}{b}\).
</p>
\begin{align}
2    &amp;= \frac{a^2}{b^2} \\
2b^2 &amp;= a^2             \\
\end{align}
<p>
<em>Step 2.</em> We can then assume that either a or b is not divisible by two.  If both of them are, then simplify \(\frac{a}{b}\) and start again from the beginning.
</p>

<p>
<em>Step 3.</em> Note that even numbers have even squares, and odd numbers have odd squares.  Because \(a^2 = 2b^2\), \(a^2\) must be even, implying that \(a\) is even as well.
</p>

<p>
<em>Step 4.</em>  From (Step 2), \(b\) must be odd.
</p>

<p>
<em>Step 5.</em>  From (Step 3), we know that \(a\) is even.  We rewrite \(a = 2(\frac{a}{2})\), where \(\frac{a}{2}\) is an integer.
</p>

<p>
<em>Step 6.</em> 
</p>
\begin{align}
2b^2            &amp;= \left(2\left(\frac{a}{2}\right)\right)^2         \\
                &amp;= 4 \left(\frac{a}{2}\right) ^2                    \\
b^2             &amp;= 2 \left(\frac{a}{2}\right) ^2                    \\
\end{align}
<p>
<em>Step 7.</em> Thus, \(b^2\) is even. \(\square\)
</p>

<div id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Indirect Proofs-Example: Flawed Indirect Proof"><h5 id="Example: Flawed Indirect Proof">Example: Flawed Indirect Proof</h5></div>
<p>
<em>Theorem:</em> All numbers are definable in under even words.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 1; Introduction and Proof Techniques-Indirect Proofs-Example: Flawed Indirect Proof-Proof:"></span><strong id="Proof:">Proof:</strong> Suppose not.  There exists some number \(n\) that cannot be defined in under eleven words.
</p>

<p>
Let n = smallest number not definable with less than eleven words.
</p>

<p>
The smallest of these numbers was defined in under eleven words, therefore none exist.  If any existed, it would be the smallest by default.
</p>

<p>
<em>The error in this proof lies in "definable".  It is not well-specified, making the entire proof bullshit.  The "theorem" is meaningless.</em>
</p>

<p>
To learn more, read about the verification principle, which was emerged in 1929.
</p>

<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity"><h3 id="Lecture 2; Induction, function classes, infinity">Lecture 2; Induction, function classes, infinity</h3></div>
<p>
<em>Friday, January 22nd</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Administrative"><h4 id="Administrative">Administrative</h4></div>
<ul>
<li>
Homework will be submitted Fridays in class

<li>
Office hours: Monday 4pm - 6pm at DL456 (non-permanent)

</ul>
<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-An Aside: Epimenides Paradox"><h4 id="An Aside: Epimenides Paradox">An Aside: Epimenides Paradox</h4></div>
<p>
Epimenides, a man from Crete, stated: "All Cretans are liars."  The issue of his statement being self-referential is immediately clear.  Centuries later, mathematics would wrestle with this idea after the introduction of <a href="https:&#47;&#47;www.wikiwand.com&#47;en&#47;Russell%27s_paradox">set theory</a>.  
</p>

<p>
It is worth noting, however, that in the case of Epimenides, the paradox is resolved in Titus 1:12-13.
</p>
<blockquote>
12 One of themselves, a prophet of their own, said "Cretans are always liars, evil beasts, lazy gluttons." <br>
13 This testimony is true. [...]
</blockquote>

<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Proofs: A Review"><h4 id="Proofs: A Review">Proofs: A Review</h4></div>
<p>
Recall the three types of proofs introduced last lecture:
</p>
<ul>
<li>
Direct: "conjure p".

<ul>
<li>
Example: If a &lt; b, then a + 1 &lt; b + 1

</ul>
<li>
Indirect: Assume (not p), and derive nonsense from there.

<li>
Induction

</ul>

<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Mathematical Induction"><h4 id="Mathematical Induction">Mathematical Induction</h4></div>
<p>
P(n) denotes a statement about a number p.  For example:
</p>
<blockquote>
P(n): n is divisible by 2.
</blockquote>

<p>
Suppose we want to prove a statement about all n.  Induction is an excellent tool for this task.  There are two major steps:
</p>
<ol>
<li>
Base case: Show P(1) is true

<li>
Inductive step: Show that \(\forall n \in \mathbb{N}\), \(P(n) \Rightarrow P(n+1)\)

</ol>
<p>
This allows us to start from a simple case and move forward.  More importantly, we are allowed to <em>assume truth</em> for some n, and move forward just one step to n+1.
</p>

<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Mathematical Induction-Example: Bernoulli&#39;s Inequality"><h6 id="Example: Bernoulli&#39;s Inequality">Example: Bernoulli's Inequality</h6></div>
<p>
Bernoulli's Inequality places an upper bound on \((1+x)^n\).
</p>
\[
(1+x)^n \geq 1 + nx
\]

<p>
<span id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Mathematical Induction-Example: Bernoulli&#39;s Inequality--Claim:"></span><strong id="Claim:">Claim:</strong> P(n): \(\forall\) \(x \geq 0\), \((1+x)^n \geq 1 + nx\).
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Mathematical Induction-Example: Bernoulli&#39;s Inequality--Base case."></span><strong id="Base case.">Base case.</strong>  For all \(x \geq 0\), \((1+x)^1 \geq 1 + nx\).
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Mathematical Induction-Example: Bernoulli&#39;s Inequality--Inductive step."></span><strong id="Inductive step.">Inductive step.</strong>  Assuming P(n) is true, show that this implies P(n+1) is also true.
</p>
\begin{align}
(1+x)^{n+1}     &amp;=      (1+x)(1+x)^n        \\
                &amp;\geq   (1+x)(1+nx)         \\
                &amp;=      1+x+nx+nx^2         \\
                &amp;\geq   1+(n+1)x            \qquad\qquad\qquad\qquad \square
\end{align}
<p>
Note that in the third line of the proof, we used the fact that \(nx^2\) is always possible for \(n \in \mathbb{N}\).
</p>

<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Functions and function classes"><h4 id="Functions and function classes">Functions and function classes</h4></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Functions and function classes-Definition: A function"></span><strong id="Definition: A function">Definition: A function</strong> is an assignment rule from set to set; \(f: A \rightarrow B\), for every \(a \in A\), \(f(a) \in B\).
</p>

<p>
Examples:
</p>
\begin{align}
f(\mathbb{R})\,\,\, &amp;\rightarrow        \mathbb{R}      \\
f(\mathbb{R}^n)     &amp;\rightarrow        \mathbb{R}      \\
f(\mathbb{C})\,\,\, &amp;\rightarrow        \mathbb{C}      \\
f(\mathbb{R}^2)     &amp;\rightarrow        \{0, 1\}        \\
\end{align}

<p>
There are three main function classes:
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Functions and function classes-Definition: injective."></span><strong id="Definition: injective.">Definition: injective.</strong>  A function \(f\) is injective if:
</p>
\[
a_1 \neq a_2        \Rightarrow     f(a_1) \neq f(a_2)
\]
<p>
For example, \(x \rightarrow x^3\) is injective while \(x \rightarrow x^4\) is not.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Functions and function classes-Definition: surjective."></span><strong id="Definition: surjective.">Definition: surjective.</strong>  A function \(f\) is surjective if for every \(b \in B\) \(\exists\) \(a \in A\) such that \(f(a) = b\).  That is, "you cover everything".
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Functions and function classes-Definition: bijective."></span><strong id="Definition: bijective.">Definition: bijective.</strong>  A function is bijective if it is both injective and surjective.  
</p>

<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Infinity"><h4 id="Infinity">Infinity</h4></div>
<p>
The notion of "infinity" does not exist in math; it is simply shorthand for "larger than any number.
</p>

<p>
For example, saying that there are "infinitely many primes" simply means that for any finite number \(n \in \mathbb{N}\), it can be shown that there are \(n+1\) primes.
</p>

<p>
<em>Remark.</em>  If I have two finite sets A, B, then they have the same number of elements if and only if there is a bijection.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Infinity-Definition."></span><strong id="Definition.">Definition.</strong>  Two sets have the same cardinality if there exists a bijection \(f: A \rightarrow B\).  Note that here we used "if" instead of "iff" because it is a definition.
</p>

<div id="Topics in Analysis-Notes-Lecture 2; Induction, function classes, infinity-Infinity-Galileo&#39;s Squares"><h5 id="Galileo&#39;s Squares">Galileo's Squares</h5></div>
<p>
Galileo wrote a list of positive integers and their squares.  Note that there is a bijection between these two sets.
</p>
\[
f: \mathbb{N} \rightarrow \{n^2 | n \in \mathbb{N}\}
\]
<p>
However, the set of squares is a strict subset of the set of natural numbers.  So how could be the same size?
</p>

<p>
To answer this question, we must look towards <a href="https:&#47;&#47;www.wikiwand.com&#47;en&#47;Georg_Cantor">Georg Cantor</a>, who single-handedly established set theory.
</p>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets"><h3 id="Lecture 3; Countably infinite sets">Lecture 3; Countably infinite sets</h3></div>
<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Administrative"><h4 id="Administrative">Administrative</h4></div>
<ul>
<li>
Correction of previous lecture: bijective is <em>not</em> synonymous with one-to-one

<li>
Professor Rachh will sub for Stefan on Wednesday

</ul>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Infinity and sets"><h4 id="Infinity and sets">Infinity and sets</h4></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Infinity and sets-Definition."></span><strong id="Definition.">Definition.</strong>  Two sets A, B have the <span id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Infinity and sets-same cardinality"></span><strong id="same cardinality">same cardinality</strong> if there exists a bijection \(f: A \rightarrow B\).
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Infinity and sets-Definition."></span><strong id="Definition.">Definition.</strong>  A set is <span id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Infinity and sets-countable"></span><strong id="countable">countable</strong> if it has the same cardinality as \(\mathbb{N} = \{1, 2, ...\}\).
</p>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Infinity and sets-Examples"><h5 id="Examples">Examples</h5></div>
<ul>
<li>
The natural numbers \(\mathbb{N}\).  The mapping is \(f(n) \in \mathbb{N}\)

<li>
Squares of natural numbers \(\{1, 4, 16, ...\}\).  The mapping is \(f(n) = \sqrt{n}\)

<li>
\(\{2, 3, 4...\}\).  The mapping is \(f(n) = n - 1\)

<li>
The even numbers \(\{2, 4, 6, ...\}\).  The mapping is \(f(n) = \frac{n}{2}\)

</ul>
<p>
Note that the last three examples are strict subsets of the natural numbers, yet they have the same cardinality as the natural numbers.  
</p>

<p>
<em>Remark.</em>  The last two examples comprise Hilbert's Hotel.
</p>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Infinity and sets-Hilbert&#39;s Hotel"><h5 id="Hilbert&#39;s Hotel">Hilbert's Hotel</h5></div>
<ul>
<li>
Consider a hotel with countably infinite rooms that are all occupied.

<li>
If a single guest arrives and asks for a room, how do you accomodate her?

<ul>
<li>
Ask each person to shift over one room, and then place her in room number 1.

</ul>
<li>
What if a countably infinite number of guests arrive?

<ul>
<li>
Ask each current guest to move into room \(2k\) where \(k\) is the number of their current room.

<li>
Then, have the new guests stay in the odd-numbered rooms.

</ul>
</ul>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Mappings of countable sets"><h4 id="Mappings of countable sets">Mappings of countable sets</h4></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Mappings of countable sets-Statement."></span><strong id="Statement.">Statement.</strong>  If A, B are both countable, then there exists a bijection between the two \(h: A \rightarrow B\).
</p>

<p>
<em>insert drawing number 1 from notes here</em>
</p>

<p>
Define a bijection \(f: A \rightarrow \mathbb{N}\) to the natural numbers.  In doing so, we have 'numbered' each element of the set \(A\).  Then, define a bijection \(g: B \rightarrow \mathbb{N}\) which maps the set \(B\) to the natural numbers.  By doing this, we can refer to the elements in \(A\) and \(B\) as, for example, 'the element mapped to 17'.  With these two functions, we can map from set \(A\) to \(B\) with \(g^{-1} \circ f\), where \(g^{-1}: \mathbb{N} \rightarrow B\) mapps from the natural numbers to \(B\).
</p>

<p>
<em>Remark.</em>  Note that like \(g\), \(g^{-1}\) is also bijective.
</p>

<p>
In summary:
</p>
<ul>
<li>
\(f\) moves us from \(A\) to \(\mathbb{N}\)

<li>
\(g^{-1}\) move us from \(\mathbb{N}\)

</ul>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Mappings of countable sets-General remarks"><h5 id="General remarks">General remarks</h5></div>
<ul>
<li>
\(\mathbb{N}\) is countable

<li>
Any infinite increasing sequence is also countable, although it is a strict subset of \(\mathbb{N}\)

<li>
The integers \(\mathbb{Z}\) are also countable.  (Hilbert's Hotel)

<li>
Prime numbers are countable; Define \(f(p) = k\) where \(k\) is the integer that amkes the following statement true: "p is the k<sup><small>th</small></sup> prime number"

<li>
In fact, a set is countable so long as you canmake a list.

</ul>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Mappings of countable sets-Higher dimensions"><h5 id="Higher dimensions">Higher dimensions</h5></div>
<p>
Sets occupying higher dimensions are countable as well, even though the infinity is of "higher dimensionality".  Take \(\mathbb{Z}^2\), for example.
</p>

<p>
<em>Claim:</em> There is a bijection between \(\mathbb{Z}^2\) and \(\mathbb{N}\).
</p>

<p>
To prove this is true, we just need to create a mapping rule.  Do so by starting at the origin (or any arbitrary point) and draw a spiral outwards.
</p>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Mappings of countable sets-Smallest infinity"><h5 id="Smallest infinity">Smallest infinity</h5></div>
<p>
Are countable sets the smallest type of infinity can find?
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Mappings of countable sets-Smallest infinity-Theoroem."></span><strong id="Theoroem.">Theoroem.</strong>  If \(S\) is a countable set, and \(T \subset S\) is infinite, then there is a bijection \(f: S \rightarrow T\).  Thus, they are the same "size" of infinity.
</p>

<p>
Seeing this is easy.  Create a mapping \(g: S \rightarrow \mathbb{N}\).  Use \(g^{-1}\) to map the elements of \(S\) to the natural numbers.  Then, create a new mapping \(h\) from the natural numbers to \(T \subset S\).
</p>

<div id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Cantor&#39;s Diagonal Argument"><h4 id="Cantor&#39;s Diagonal Argument">Cantor's Diagonal Argument</h4></div>
<p>
<em>Claim:</em> The set of real numbers \((0, 1)\) is <em>not</em> countable.
</p>

<p>
<em>Proof.</em> By way of contradiction, assume that there exists a bijective mapping \(f: \mathbb{N} \rightarrow (0, 1)\).
</p>

<p>
If it were <a href="bijective.html">bijective</a>, we would be able to list every real number.  We just need to find something not in the listing.
</p>

<p>
<em>Remark.</em>  Decimal expansions are not unique (why is this important?)
</p>

<p>
Cantor's argument:  create a list of real numbers.  Then, go down along the diagonal (take the number in the tenth's place for the first number, hundredth's place for the second number, ...).  Replace every digit along the diagonal with <em>another</em> digit that is not 9.  Use these digits to create a real number.
</p>

<p>
This new number is <span id="Topics in Analysis-Notes-Lecture 3; Countably infinite sets-Cantor&#39;s Diagonal Argument-not"></span><strong id="not">not</strong> in the list.  It can't be the first, can't be the second... \(\square\)
</p>

<p>
As we can see, we now have two types of infinity:
</p>
<ul>
<li>
Countable infinities

<li>
Uncountable infinities, such as the real numbers between 0 and 1 (aleph-one, \(\aleph_1\)).

</ul>
<p>
Is there an infinity "in between"?  The answer to this question is independent of math; the question is ill-posed.  Paul Cohen received the Field's Medal in 1966 for this result.
</p>

<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences"><h3 id="Lecture 4; Set theory, sequences">Lecture 4; Set theory, sequences</h3></div>
<p>
<em>Wednesday, January 27th</em>
</p>
<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Cantor&#39;s diagonal argument, rehashed"><h4 id="Cantor&#39;s diagonal argument, rehashed">Cantor's diagonal argument, rehashed</h4></div>
<p>
Here we present a proof that follows the style of Cantor's diagonal argument from last lecture, but leads to a different result.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Cantor&#39;s diagonal argument, rehashed-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If \(A\) is a countable set, then the set of all subsets is uncountable.  
</p>

<p>
<em>Proof.</em>  Assume, without loss of generality, that \(A = \mathbb{N} = \{1, 2, 3, ...\}\).  If A is not the set of natural numbers, you can always create a bijection \(f: A \rightarrow \mathbb{N}\), as \(A\) is countable.  Now, list the subsets of \(A\).  A few are given below:
</p>

<table>
<tr>
<td>
Subsets \ Digit
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
5
</td>
<td>
6
</td>
<td>
Identifier
</td>
</tr>
<tr>
<td>
1
</td>
<td>
x
</td>
<td>
o
</td>
<td>
x
</td>
<td>
o
</td>
<td>
x
</td>
<td>
o
</td>
<td>
Even integers
</td>
</tr>
<tr>
<td>
2
</td>
<td>
x
</td>
<td>
o
</td>
<td>
o
</td>
<td>
x
</td>
<td>
o
</td>
<td>
x
</td>
<td>
Prime numbers
</td>
</tr>
<tr>
<td>
3
</td>
<td>
x
</td>
<td>
x
</td>
<td>
o
</td>
<td>
x
</td>
<td>
x
</td>
<td>
o
</td>
<td>
Multiples of 3
</td>
</tr>
<tr>
<td>
4
</td>
<td>
o
</td>
<td>
o
</td>
<td>
o
</td>
<td>
x
</td>
<td>
o
</td>
<td>
x
</td>
<td>
Fibonacci numbers
</td>
</tr>
</table>

<p>
In the above table, 'o' indicates that the number belongs to that subset, and 'x' indicates that it does not belong to that subset.  Now, we will build a new subset.
</p>

<p>
<em>Step 1.</em>  We assume, by way of contradiction, that the set of all subsets is actually countable.
</p>

<p>
<em>Step 2.</em> Then, we can put them into a list, like the table above.  Every conceivable subset should be in this list.
</p>

<p>
<em>Step 3.</em> We now build a specific subset of the integers \(S\).  How?  \(n \in S\) if and only if \(n \notin A_n\).  For example, we would not include 3 is \(S\) because 3 is in \(A_3\).
</p>

<p>
<em>Step 4.</em>  \(S\) is somewhere in the list, because every conceivable subset is in the list.
</p>

<p>
<em>Step 5.</em>  However, \(S\) cannot be in position \(k\), because if the k<sup><small>th</small></sup> number of the subset in position k is in \(A_k\), then it is not in \(S\), and vice-versa.
</p>

<p>
<em>Step 6.</em>  \(S\) is not in the list.
</p>

<p>
This argument of "going down the diagonal" is sometimes referred to as "condensation of singularities".  
</p>

<p>
This concludes Stefan's introduction to set theory.
</p>

<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences"><h4 id="Mathematical Foundations of Calculus: Sequences">Mathematical Foundations of Calculus: Sequences</h4></div>
<p>
What are sequences?  Consider this:
</p>

\[
    a_n: \{1, 2, 3, ...\} \rightarrow \mathbb{R}
\]

<p>
So, a sequence is just a mapping of the natural numbers to the real numbers.
</p>

<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Convergence"><h5 id="Convergence">Convergence</h5></div>
<p>
The German textbook from which Stefan learned convergence termed it "the second most important invention, after soap".  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Convergence-Definition."></span><strong id="Definition.">Definition.</strong>  A sequence converges if:
</p>

\[
    \lim_{n \rightarrow \infty} a_n = a
\]

<p>
But what does this mean?  Put another way, a sequence converges if for every \(\varepsilon &gt; 0\)  there exists \(N \in \mathbb{N}\) such that for all \(n \geq N\), \(|a_n - a| &lt; \varepsilon\).  Basically, for any arbitrarily small number that you pick, there is some term in the sequence after which the successive terms in the sequence are close to that number.
</p>

<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Convergence-Example"><h6 id="Example">Example</h6></div>
<p>
Why do we do this rather than comparing digits?  Presumably, we can say that numbers are close if they have a very long succession of digits that are the same.  Consider the following:
</p>
\[
    \lim_{n \rightarrow \infty} 1 + \frac{1}{n} = 1
\]

<p>
Our definition of convergence allows us to show this easily.  Observe:
</p>

<p>
For an arbitrarily small \(\varepsilon &gt; 0\), if \(n \geq N\), then we have:
</p>
\begin{align}
    |(1+\frac{1}{n}) - 1| &amp;&lt; \varepsilon            \\
    \frac{1}{n}           &amp;&lt; \varepsilon            \\
    \frac{1}{\varepsilon} &amp;&lt; n
\end{align}

<p>
The statement is true, and we can even say:
</p>
\[
    N = \left\lceil \frac{1}{\varepsilon} \right\rceil &gt; \frac{1}{\varepsilon}
\]

<p>
This \(n\) does the job.  Often, we don't need to find \(n\), we just need to show its existence.  
</p>

<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Convergence-Example"><h6 id="Example">Example</h6></div>
<p>
Suppose we have a sequence where \(a_1 = 5\); \(a_{n+1} = a_n \cdot q\); for \(|q| &lt; 1\).
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Convergence-Example--Claim."></span><strong id="Claim.">Claim.</strong>  \(a_n = 5q^{n-1}\)
</p>

<p>
This claim can be proven by induction.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Convergence-Example--Base case."></span><strong id="Base case.">Base case.</strong>  \(a_1 = 5\)
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Convergence-Example--Inductive step."></span><strong id="Inductive step.">Inductive step.</strong>
</p>
\[
    a_{n+1} = a_n q = (5q^{n-1})q = 5q^n    \qquad \qquad \qquad \square
\]

<p>
For every \(\varepsilon &gt; 0\) there exists an \(N \in \mathbb{N}\) such that for all \(n \geq N\),
</p>
\begin{align}
    |5q^{n-1} - 0|          &amp;&lt; \varepsilon                  \\
    5|q|^{n-1}              &amp;&lt; \varepsilon                  \\
     |q|^{n-1}              &amp;&lt; \frac{\varepsilon}{5}        \\
    (n-1)\log(|q|)          &amp;&lt; \log(\frac{\varepsilon}{5})   \\
    n-1                     &amp;&lt; \frac{\log(\frac{\varepsilon}{5})}{\log(|q|)}
\end{align}

<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Divergence"><h5 id="Divergence">Divergence</h5></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Divergence-Definition."></span><strong id="Definition.">Definition.</strong>  A sequence "diverges" if it does not converge.  Some examples include \(a_n: (-1)^n\) and \(a_n: \mathbb{I}\{\text{n is prime}\}\).
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Divergence-Definition."></span><strong id="Definition.">Definition.</strong>  A sequence "goes to infinity" or "diverges to infinity" if for any arbitrarily large integer \(M \in \mathbb{N}\) there exists some index \(N \in \mathbb{N}\) such that for all \(n &gt; N\), \(a_n &gt; M\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Divergence-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If you have a sequence of real numbers that converges to a \(a_n \rightarrow a\), then the sequence is bounded, which means that there exists an \(M\) such that \(|a_n &lt; M\).  
</p>

<p>
<em>Proof.</em>  
</p>

<p>
Pick \(\varepsilon = 1\).  Because the sequence converges, there exists \(N \in \mathbb{N}\) such that for all \(n \geq N\), \(|a_n - a| &lt; 1\).
</p>

\begin{align}
    |a_n - 1|           &amp;&lt; 1                \\
    \Rightarrow a_n     &amp;&lt; a + 1            \\
    \Rightarrow a_n     &amp;&gt; a - 1
\end{align}

<p>
Outside this bound \((a-1, a+1)\), they may just around like crazy, but it is still a finite sequence.  The statement holds for \(M = \max\{|a_1|, |a_2|, ..., |a_N|, |a-1|, |a+1|\}\).
</p>

<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Divergence-Example"><h6 id="Example">Example</h6></div>
<p>
Consider \(a_n = \frac{5}{n}\).
</p>

<p>
Since the sequence converges to zero, pick \(\varepsilon = 1\).  We know that after a while, the sequence is trapped, but not at first.
</p>

<p>
So, we take the max over the absolute values to get a bound for all \(n\).  
</p>

<div id="Topics in Analysis-Notes-Lecture 4; Set theory, sequences-Mathematical Foundations of Calculus: Sequences-Divergence-Example"><h6 id="Example">Example</h6></div>
<p>
What about \(a_n = \frac{1}{n-1}\)?  It goes to zero, but it isn't bounded!
</p>

<p>
That's because it is <em>not a sequence.</em>  A sequence maps from the natural numbers to the real numbers, but \(a_n\) is not well-defined for \(a_1\).  Infinity is not a real number.
</p>

<div id="Topics in Analysis-Notes-Lecture 5; Convergence"><h3 id="Lecture 5; Convergence">Lecture 5; Convergence</h3></div>
<p>
<em>Friday, January 29th</em>
</p>

<p>
Recall that a sequence is a mapping \(a_n : \mathbb{N} \rightarrow \mathbb{R}\).  The limits we saw in last lecture are very helpful for proofs.  
</p>

<div id="Topics in Analysis-Notes-Lecture 5; Convergence-Convergence of the sum"><h4 id="Convergence of the sum">Convergence of the sum</h4></div>

<p>
<span id="Topics in Analysis-Notes-Lecture 5; Convergence-Convergence of the sum-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If two sequences both converge, then the sequence of the sum of the terms converges.  More concisely, if \(a_n \rightarrow a\) and \(b_n \rightarrow b\), then \(a_n + b_n \rightarrow a + b\).  
</p>

<p>
<em>Proof.</em>  The basic notion of this proof: if two real numbers are really close to two corresponding numbers, then their sum is really close to the sum of those two corresponding numbers.  
</p>

<p>
We know that because \(a_n \rightarrow a\), for all \(\varepsilon &gt; 0\), there exists an \(N_a\) such that for all \(n \geq N_a\), \(|a_{N_a} - a| \leq \frac{\varepsilon}{2}\).  We can say the same about the existence of an \(N_b\).  
</p>

<p>
We want to discern something about \(|a_n + b_n - (a + b)|\).  We begin by rewriting it:
</p>
\begin{align}
    | (a_n + b_n) - (a + b) |            &amp;= |(a_n - a) + (b_n - b)|             \\
                                         &amp;\leq |a_n - a| + |b_n - b|               \\
\end{align}
<p>
And we know that the first term in the last statement is less than \(\frac{\varepsilon}{2}\) if \(n &gt; N_a\), and the second term is less than \(\frac{\varepsilon}{2}\) if \(n &gt; N_b\).  As long as \(n\) is larger than both these numbers, we are good.  If \(n\geq N_a \and n \geq N_b\), then \(|a_n + b_n - (a + b)| \leq \varepsilon\).  
</p>

<div id="Topics in Analysis-Notes-Lecture 5; Convergence-Sandwich lemma"><h4 id="Sandwich lemma">Sandwich lemma</h4></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 5; Convergence-Sandwich lemma-Theorem."></span><strong id="Theorem.">Theorem.</strong>  Suppose we have two sequences that converge to the same limit, and there is another sequence between the two:
</p>
<ol>
<li>
\(a_n \rightarrow a\), \(b_n \rightarrow a\)

<li>
\(a_n \leq c_n \leq b_n\)

</ol>
<p>
Then we know that the sequence between the two also converges to the same limit: \(c_n \rightarrow a\).
</p>

<p>
This is especially useful if we want to, for example, show that \(1 + \frac{\cos(n^5) e^{-1}}{n^2 + \sqrt{\pi}}\) converges.  We can bound it from below with \(1\) and bound it from above with \(a + \frac{1}{n^2}\).
</p>

<p>
<em>Proof.</em>  We must verify that \(c_n\) converges.  
</p>

<p>
For all \(\varepsilon &gt; 0\), we have to find an \(N\) such that for all \(n \geq N\), \(|c_n - a| \leq \varepsilon\).  We can find an \(N_a\) such that for all \(n \geq N_a\), \(|a_{N_a} - a | \leq \varepsilon\), and a corresponding \(N_b\).  So, the \(N\) that we wish to find is \(N = \max\{N_a, N_b\}\).  
</p>

<p>
We know that \(b_n \geq a_n\).  So, they are always trapped in some interval, and jumping around, but so that \(b_n\) is greater than or equal to \(a_n\).  
</p>

<p>
Suppose by way of contradiction that \(n &gt; \max\{N_a, N_b\}\), but \(c_n &lt; a - \varepsilon\).  Then proposition (2) says that \(a_n \leq c_n &lt; a - \varepsilon\).  
</p>
\begin{align}
    a_n         &amp;&lt; a - \varepsilon              \\
    a_n - a     &amp;&gt; \varepsilon                  \\
    |a_n - a |  &amp;&gt; \varepsilon
\end{align}

<p>
This says that \(a_n\) is more that \(\varepsilon\) away from \(a\) for some \(n \geq N_c \geq N_a\), but \(N_a\) is the number after which \(a_n\) is always within \(\varepsilon\) of \(a\).  Contradiction.  \(\square\)
</p>

<div id="Topics in Analysis-Notes-Lecture 5; Convergence-Two-stage Markov chains"><h4 id="Two-stage Markov chains">Two-stage Markov chains</h4></div>
<p>
This is an interesting example of sequences.  Suppose you check somebody every minute, and see whether or not they are currently on a phone call.  Keep a log.  So, if <code>x</code> denotes not in a call and <code>o</code> denotes in a phone call, we might have:
</p>
<blockquote>
<code>x</code><code>x</code><code>x</code><code>o</code><code>o</code><code>o</code><code>o</code><code>o</code><code>o</code><code>x</code><code>x</code><code>x</code><code>x</code>
</blockquote>
<p>
This can be model by a fair coin toss, but it would not be very accurate.  We could take the overall proportion of times they are in a phone call when we check, and model it as a Bernoulli(p).  Still, this would not be accurate.  As we see above, the states tend to cluster together.  That's because if you're not in a call, you it's relatively rare that you enter a call; and if you're in a call, you tend to stay in the call.  So the states tend to appear like this:
</p>


<p>
Where, perhaps, we could have \(p\) close to 1, and \(q\) high, but not as close -- perhaps 0.85.
</p>

<p>
It happens that this is rather good at modeling the weather and traversal through the internet.  It also gives rise to very nice sequences.
</p>

<div id="Topics in Analysis-Notes-Lecture 5; Convergence-Two-stage Markov chains-Convergence of Markov chains"><h5 id="Convergence of Markov chains">Convergence of Markov chains</h5></div>
<p>
Propositions:
</p>
<ol>
<li>
We start at stage <code>x</code>.

<li>
Let \(p_n\) be the likelihood of not being on the phone at minute \(n\), and let \(q_n\) be the likelihood of being on the phone at minute \(n\).  

</ol>

<table>
<tr>
<td>
Iteration
</td>
<td>
p
</td>
<td>
q
</td>
</tr>
<tr>
<td>
1
</td>
<td>
\(1\)
</td>
<td>
\(0\)
</td>
</tr>
<tr>
<td>
2
</td>
<td>
\(p\)
</td>
<td>
\(1-p\)
</td>
</tr>
<tr>
<td>
3
</td>
<td>
\(p_2p + q_2(1-q)\)
</td>
<td>
$p_2(1-p) + q_2q
</td>
</tr>
</table>

<p>
At iteration 4, the same reasoning applies.  So, in general, we can say by induction that:
</p>
<ul>
<li>
\(p_{n+1} = p_np + q_n(1-q)\)

<li>
\(q_{n+1} = p_n(1-p) + q_nq\)

</ul>

<p>
At this point, we should check that \(p_n + q_n = 1\), to ensure that our equations are correct.  The proof (by induction) plays out beautifully, and is left as an exercise for the reader.
</p>

<p>
Finally, we want to see whether this series (\(p_n\)) converges.  What is \(\lim_{n \rightarrow \infty} p_n\)?
</p>

<div id="Topics in Analysis-Notes-Lecture 6; Markov chains, square roots"><h3 id="Lecture 6; Markov chains, square roots">Lecture 6; Markov chains, square roots</h3></div>
<p>
<em>Monday, January 29th</em>
</p>
<div id="Topics in Analysis-Notes-Lecture 6; Markov chains, square roots-Markov chains, continued"><h4 id="Markov chains, continued">Markov chains, continued</h4></div>
<p>
Recall the two-state Markov chain from last class.  We want to prove some sort of convergence.  Recall the definition of convergence: if \(a_n \rightarrow a\), then \(\lim_{n \rightarrow \infty} a_n - a = 0\).  
</p>

<p>
Assuming \(p_n\) and \(q_n\) converge, we can compute:
</p>

\[
\lim_{n \rightarrow \infty} p_{n+1} - p_n = 0
\]
\[
\lim_{n \rightarrow \infty} q_{n+1} - q_n = 0
\]
\[
\begin{bmatrix}
    p_{n+1}     \\
    q_{n+1}     
\end{bmatrix}
-
\begin{bmatrix}
    p_{n}     \\
    q_{n}     
\end{bmatrix}
=
\begin{bmatrix}
    1-q     &amp;   p       \\
    q       &amp;   1-p     
\end{bmatrix}
\begin{bmatrix}
    p_n     \\
    q_n     
\end{bmatrix}
-
\begin{bmatrix}
    1   &amp;   0       \\
    0   &amp;   1       \\
\end{bmatrix}
\approx
\begin{bmatrix}
    0       \\
    0
\end{bmatrix}
\]

<div id="Topics in Analysis-Notes-Lecture 6; Markov chains, square roots-Square roots"><h4 id="Square roots">Square roots</h4></div>
<p>
Consider computing the following:
$\( \sqrt{1 + \sqrt{1 + \sqrt{1 + ...}}} \)$
</p>

<p>
We can view this as a sequence, where \(x_1 = 1\) and \(x_{n+1} = \sqrt{1 + \sqrt{x_n}}\).  Assuming the limit exists, we can compute it.
</p>

<p>
\(x_n\) and \(x_{n+1}\) should be close to each other.  The only possible limit would satisfy:
$\(x = \sqrt{1+\sqrt{x}}\)$
</p>

<p>
<em>Aside.</em>  Convergence is like asking for directions to Toads.  The limit is the place where people stop telling you to "go over there", because you have arrived.
</p>

<p>
<em>Remark.</em>  When considering two-state Markov chains, the only possible limit is at the eigenvector with eigenvalue 1.
</p>

<div id="Topics in Analysis-Notes-Lecture 6; Markov chains, square roots-More square roots"><h4 id="More square roots">More square roots</h4></div>
<p>
Consider computing \(\sqrt{a}\) for some \(a &gt; 0\).  A nice method is to observe that a square of area \(a\) has sides of length \(\sqrt{a}\).  
</p>

<p>
Therefore, a reasonable way of computing \(\sqrt{a}\) is to guess a value \(x_n\), compute \(\frac{a}{x_n}\), and update our estimate of \(sqrt{a}\) to be \(x_{n+1} = \frac{x_n + \frac{a}{x_n}}{2}\).  This is known as <a href="https:&#47;&#47;www.wikiwand.com&#47;en&#47;Methods_of_computing_square_roots#&#47;Babylonian_method">Heron's method</a>.  As it turns out, this sequence converges to \(\sqrt{a}\), and it does so extremely quickly.  Every iteration of the algorithm doubles the number of digits of precision.
</p>

<div id="Topics in Analysis-Notes-Lecture 6; Markov chains, square roots-More square roots-Argument"><h5 id="Argument">Argument</h5></div>
<p>
If there is a limit to the sequence \(x_n\) defined above, it is \(sqrt{a}\).  Furthermore, it satisfies:
</p>
\begin{align}
        x       &amp;=      \frac{x + \frac{a}{x}}{2}       \\
\Leftrightarrow 2x &amp;=   x^2 + a                         \\
\Leftrightarrow x^2 &amp;= a
\end{align}

<p>
Now, we show two elementary facts.  First, the sequence will converge from above.  That is, \(x_{n+1} \geq \sqrt{a}\).
</p>

\begin{align}
x_{n+1} = \frac{x_n + \frac{a}{x_n}}{2}     &amp;\geq       \sqrt{a}        \\
x^2_n + a                                   &amp;\geq       2\sqrt{a}x_n    \\
(x_n  \sqrt{a})^2                           &amp;\geq       0
\end{align}

<p>
The second elementary fact is that if \(x_n &gt; \sqrt{a}, then \)x_{n+1} &lt; x_n$.  
</p>

\begin{align}
x_{n+1} = \frac{x_n + \frac{a}{x_n}}{2}     &amp;&lt; x_n                      \\
x^2_n + a                                   &amp;&lt; 2x^2_n                   \\
a                                           &amp;&lt; x^2_n                    \\
x_n                                         &amp;&gt; \sqrt{a}
\end{align}

<p>
Thus, the sequence has the following properties:
</p>
<ol>
<li>
\(\forall\) \(n\in\mathbb{N}\), \(x_n \geq \sqrt{a}\)

<li>
\(\forall\) \(n\in\mathbb{N}\), \(x_{n+1} \leq x_n\)

</ol>
<p>
So, does it converge?  
</p>

<p>
Yes, but the proof is difficult.  We must assume an axiom about the real numbers.  
</p>

<p>
<em>Remark</em>.  When we have an estimate \(x_n\), we know that \(a \leq x_m \leq x_n\) for 
\(m &gt; n\).  However, we cannot be sure that the sequence converges to \(a\) rather than 
some other number within those bounds.  To find it, we can conduct binary search.
</p>

<div id="Topics in Analysis-Notes-Lecture 7; Cauchy sequences, series"><h3 id="Lecture 7; Cauchy sequences, series">Lecture 7; Cauchy sequences, series</h3></div>
<p>
<em>Wednesday, February 3rd</em>
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 7; Cauchy sequences, series-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If a sequence \(x_n\) satisfies:
</p>
<ol>
<li>
\(x_{n+1} \leq x_n\) (always decreasing

<li>
\(c \leq x_n\) (bounded from below by some constant)

</ol>
<p>
then \(x_n\) converges.  
</p>

<div id="Topics in Analysis-Notes-Lecture 7; Cauchy sequences, series-Cauchy sequences"><h4 id="Cauchy sequences">Cauchy sequences</h4></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 7; Cauchy sequences, series-Cauchy sequences-Definition."></span><strong id="Definition.">Definition.</strong>  \(a_n: \mathbb{N} \rightarrow \mathbb{R}\) is Cauchy if for all \(\varepsilon &gt; 0\) there exists an \(N\) such that for all \(n, m &gt; N\), \(|a_n - a_m| &lt; \varepsilon\).  Note that this is a statement about the <em>sequence</em>, not the <em>limit</em>.
</p>

<p>
<em>Proof.</em>  
</p>
<ol>
<li>
Cauchy sequences converge.

<li>
If a sequence converges, it is Cauchy.

</ol>

<p>
We will prove the second statement first, directly.
</p>

<p>
Suppose \(a_n\) converges.  Then, for all \(\varepsilon &gt; 0\), there exists an \(N\) such that for all \(n &gt; N\), \(|a_n - a| &lt; \varepsilon\).  In plain English, this means that for every \(\varepsilon\), we can cut off an initial part of he sequence and have the remainder \(a_{n+1}, a_{n+2}, ...\) be in the interval \((a - \varepsilon, a + \varepsilon)\).  Because they are all in there, each point is close to the other points.
</p>

\begin{align}
|a_n - a_m| = |a_n - a + a - a_m| &amp;\leq |a_n - a| + |a - a_m|           \\
                                  &amp;\leq \varepsilon + \varepsilon       \\
                                  &amp;\leq 2\varepsilon
\end{align}

<p>
What about a proof of statement one?  This proof is hard.  It requires the <em>Axiom of Completeness</em>, which depends on how you define the real numbers.  The book does not touch it.
</p>

<p>
<em>Proof of Theorem.</em>  Show that \(x_n\) is Cauchy.  
We use iterative refinement.  At each point in time, we have an upper bound (the value of the sequence at that time) and a lower bound (constant \(c\)).  So, we divide that interval into two parts.  There are two cases:
</p>
<ol>
<li>
If there is an element of the sequence in the left part, all but finitely many are in there.

<li>
There is <em>no</em> element in the left part.  

</ol>
<p>
Then, we repeat this process.  This is sufficient to verify the Cauchy property.  After repeating this process a few times, you have a tiny interval that has all the future elements inside.
</p>

<p>
<em>Remark.</em>  A Cauchy sequence from elements in \(\mathbb{Q}\) need not have a limit in \(\mathbb{Q}\).  Take, for example, this sequence, whose limit is \(\sqrt{2}\): \(\frac{1}{1}, \frac{14}{10}, \frac{141}{100}, ...\)
</p>

<div id="Topics in Analysis-Notes-Lecture 7; Cauchy sequences, series-Series"><h4 id="Series">Series</h4></div>
<p>
If we have \(a_n\), a sequence of real numbers, then we have have a series \(b_n = \sum_{k=1}^n a_k\).  Note that this is just a special type of sequence.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 7; Cauchy sequences, series-Series-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If \(\sum_{n=1}^\infty a_n\) converges, then \(a_n \rightarrow 0\).  
</p>

<p>
<em>Proof.</em>  If \(b_n \rightarrow b\), then \(b_{n + 1} - b_n \rightarrow 0\).  Then, we have:
</p>
\[
b_{n+1} - b_n = \sum_{k=1}^{n+1} a_k - \sum_{k=1}^n a_k = a_n+1
\]
<p>
Note that we are adding up infinitely many things to get a finite sum.
</p>

<div id="Topics in Analysis-Notes-Lecture 7; Cauchy sequences, series-Series-Divergence of the harmonic series"><h5 id="Divergence of the harmonic series">Divergence of the harmonic series</h5></div>
<p>
Oresme gave an extremely elegant proof of the divergence of the harmonic series.  Note that the first term, \(1 \geq \frac{1}{2}\).  Then, note that the second term, \(\frac{1}{2} \geq \frac{1}{2}\).  The third and fourth terms, \(\frac{1}{3} + \frac{1}{4} \geq \frac{1}{2}\).  As you go down the series, you find infinitely many one-halves, so the sum goes to infinity.
</p>

<div id="Topics in Analysis-Notes-Lecture 7; Cauchy sequences, series-Series-More interesting series"><h5 id="More interesting series">More interesting series</h5></div>
<p>
Leibniz also gave a very elegant proof:
</p>
\[
\frac{1}{1 \cdot 2} + \frac{1}{2 \cdot 3} + \frac{1}{3 \cdot 4} + ... = 1
\]
<p>
How?  Note that this series can equivalently be written as:
</p>
\[
\left(\frac{1}{1} - \frac{1}{2}\right) + \left(\frac{1}{2} - \frac{1}{3}\right) + \left(\frac{1}{3} - \frac{1}{4}\right) + ...
\]

<p>
More mathematically, we would like to show that:
</p>
\[
\sum_{k=1}^n \frac{1}{k(k+1)} = 1 - \frac{1}{n+1}
\]
<p>
which may be done by induction.
</p>

<div id="Topics in Analysis-Notes-Lecture 8; Supremum and Infimum"><h3 id="Lecture 8; Supremum and Infimum">Lecture 8; Supremum and Infimum</h3></div>
<p>
<em>Friday, February 5th</em>
<span id="Topics in Analysis-Notes-Lecture 8; Supremum and Infimum-Theorem."></span><strong id="Theorem.">Theorem.</strong>  There are infinitely many prime numbers.  
</p>

<p>
<em>Proof.</em>  Suppose not.  Then we can take the largest prime number and call it \(p_n\).  Compute the product of all the prime numbers, and then add 1 to that product. 
</p>

\[
P = p_1 p_2 p_3 ... p_n + 1
\]

<p>
Now, there are two cases for \(P\).  If it is prime, then \(P\) is a prime that was not on our list.  If \(P\) is not prime, then it is divisible by some prime \(p'\).  However, \(p'\) is not any of the \(p_k\) for \(1 \leq k \leq n\),  otherwise \(p'\) would be a divisor of \(1\), which is impossible.  Either way, our original list was not comprehensive.  Contradiction.  \(\square\)
</p>

<p>
<em>Proof.</em>  Suppose not.  List the prime numbers \(p_1, ..., p_n\).  
</p>

\begin{align}
\infty &gt; \prod_{k = 1}^n \frac{1}{1 - \frac{1}{p_k}} &amp;= \prod_{k = 1}^n\left(1 + \frac{1}{p_k} + \frac{1}{p_k^2} + \frac{1}{p_k^3} + ... \right)        \\
                        &amp;= \left(1 + \frac{1}{2} + \frac{1}{4} + ...\right) \left(1 + \frac{1}{3} + \frac{1}{9} + ... \right) \left(1 + \frac{1}{5} + \frac {1}{25} + ...\right) ...  \\
                        &amp;= 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \frac{1}{6} + ...
\end{align}

<p>
The last line is the harmonic sequence, which diverges to infinity.  Therefore, \(p_n\) has to be infinite.
</p>

<div id="Topics in Analysis-Notes-Lecture 8; Supremum and Infimum-Supremum and Infimum"><h4 id="Supremum and Infimum">Supremum and Infimum</h4></div>
<p>
Suppose we have \(A \in \mathbb{R}\).  A number \(b\) is an upper bound for a set \(A\) if \(x \leq b\) for all \(x \in A\).  \(b_0\) is the least upper bound of \(A \) if \(b_0\) is an upper bound and \(b_0 \leq b\) for any other upper bound \(b\).  Then we may say that \(b_0 := \sup A\).  
</p>

<div id="Topics in Analysis-Notes-Lecture 8; Supremum and Infimum-Supremum and Infimum-Example"><h5 id="Example">Example</h5></div>
<p>
Suppose we have \(A = \{0, \frac{1}{2}, \frac{2}{3}, \frac{3}{4}, ... \}\).  For all \(a \in A\), \(a \leq 1\).  
</p>

<p>
<em>Remark.</em>  One is the smallest upper bound.  
</p>

<p>
Suppose \(1 - \varepsilon\) were an upper bound as well.  For all \(n\), 
</p>
\begin{align}
\frac{n}{n + 1}     &amp;\leq       1 - \varepsilon             \\
1 - \frac{n}{n + 1} &amp;\leq       1 - \varepsilon             \\
\varepsilon         &amp;\leq       \frac{n}{n + 1}
\end{align}
<p>
This fails for \(n\) big enough.  
</p>

<div id="Topics in Analysis-Notes-Lecture 8; Supremum and Infimum-Supremum and Infimum-Existence"><h5 id="Existence">Existence</h5></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 8; Supremum and Infimum-Supremum and Infimum-Existence-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If \(A\) is a set bounded above (\(\exists c \in \mathbb{R}\) \(\forall\) \(a \in A\) such that \(a \leq c\)), then \(\sup A\) exists.  The argument here is very similar to the proof of the convergence of Cauchy sequences.  Essentially, we take a point \(a_1 \in A\), and divide the interval between \(a_1\) and \(c\) in half.  If there is an element in the right-hand interval, continue the process in that interval.  If there are no elements, continue the search in the left-hand interval.
</p>

<div id="Topics in Analysis-Notes-Lecture 8; Supremum and Infimum-Supremum and Infimum-Example"><h5 id="Example">Example</h5></div>
<p>
Consider the set \(A = \{0, \frac{1}{2}, \frac{2}{3}, \frac{3}{4}, ...\}\).  We know that for all \(a \in A\), \(a \leq 1\).  We claim that this is the smallest upper bound.  
</p>

<p>
<em>Proof.</em>  By contradiction.
</p>

<p>
Suppose that \(1 - \epsilon\) is a smaller upper bound.  This would mean that for all \(n\), we would have:
</p>
\begin{align}
\frac{n}{n + 1}     &amp;\leq       1 - \varepsilon         \\
1 - \frac{n}{n+1}   &amp;\leq       1 - \varepsilon         \\
    \varepsilon     &amp;\leq       \frac{n}{n + 1}
\end{align}

<p>
And this fails for \(n\) big enough.
</p>

<div id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass"><h3 id="Lecture 9; Supremum, infimum, Bolzano-Weierstrass">Lecture 9; Supremum, infimum, Bolzano-Weierstrass</h3></div>
<p>
<em>Monday, February 8th</em>
</p>
<div id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass-Supremum and Infimum, continued"><h4 id="Supremum and Infimum, continued">Supremum and Infimum, continued</h4></div>
<p>
Last time, we established an upper bound \(c\) for a set \(A\) by specifying that if we had a set \(A \subset R\), then for all \(a \in A\), we have \(a \leq c\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass-Supremum and Infimum, continued-Definition: Supremum."></span><strong id="Definition: Supremum.">Definition: Supremum.</strong>  A supremum is the <em>smallest</em> upper bound.  This replaces the maximum for infinite sets.  
</p>

<p>
We require the concept of a supremum because with an infinite set, there is no largest element.  Consider the set \(A = \{\frac{1}{2}, \frac{2}{3}, \frac{3}{4}, ...\}\).  In this case, we have \(\sup A = 1\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass-Supremum and Infimum, continued-Definition: Infimum."></span><strong id="Definition: Infimum.">Definition: Infimum.</strong>  An infimum is the largest lower bound.  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass-Supremum and Infimum, continued-Claim."></span><strong id="Claim.">Claim.</strong>  If we have \(A \subset \mathbb{R}\), and \(-A = \{-a: a \in A\}\), then \(-\inf A = \sup (-A)\).  
</p>

<p>
Just draw a diagram.  We reverse the order of the points.  The great thing about this proof is that if we have proofs for the supremum, the same follows for the infimum.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass-Supremum and Infimum, continued-Theorem."></span><strong id="Theorem.">Theorem.</strong>  For \(A, B \in \mathbb{R}\),  for all \(a \in A\), \(b \in B\), if we have \(a \leq b\), then \(\sup(A) \leq \inf(B)\).  
</p>

<p>
<em>Proof.</em>  From our assumption, we have for all \(a, b\), \(a \leq b\).  
</p>

<p>
Let us fix \(b^\star \in B\).  That means that for all \(a \in A\), \(a \leq b^\star\).  
</p>

<p>
That means that \(b^\star\) is an upper bound for \(A\).  Then, this upper bound \(b^\star\) is by definition greater than or equal to the smallest upper bound.  So, \(\sup(A) \leq b^\star\).  
</p>

<p>
Now, note that the choice of \(b^\star\) was arbitrary.  This argument works for all \(b^\star\).  So, the supremum of \(A\) is a lower bound for \(B\); hence, it is certainly smaller than the largest possible lower bound.  \(\square\)
</p>

<div id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass-Bolzano-Weierstrass Theorem"><h4 id="Bolzano-Weierstrass Theorem">Bolzano-Weierstrass Theorem</h4></div>
<p>
Recall that a sequence is a mapping \(a: \mathbb{N} \rightarrow \mathbb{R}\).  Let us define a subsequence \(a_{n_k}\).  
</p>

<p>
\(n_k = n(k): \mathbb{N} \rightarrow \mathbb{N}\) is a mapping that is strictly increasing.  It essentially provides a rule for selecting which elements of the sequence \(a_n\) to include in the subsequence \(a_{n_k}\), and does so by specifing the indices of the elements we want.  
</p>

<p>
For example, suppose \(a_n = n\).  Then, the subsequence \(a_{n^2}\) would have elements \(1, 4, 9, 16, ...\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass-Bolzano-Weierstrass Theorem-Theorem: Bolzano-Weierstrass."></span><strong id="Theorem: Bolzano-Weierstrass.">Theorem: Bolzano-Weierstrass.</strong>  If \(a_n\) is a sequence in a bounded interval, then there exists a convergent subsequence.  
</p>

<p>
Suppose we had the sequence \(a_n\) with elements \(\frac{1}{4}, \frac{3}{4}, \frac{1}{4}, \frac{3}{4}, ...\).  Clearly, it does not converge.  However, it is bounded!  So, there is a convergent subsequence, and we indeed find one: \(a_{2n - 1}\) converges to \(\frac{1}{4}\).
</p>

<p>
<em>Proof.</em>
</p>

<p>
<em>Step 1.</em>  Take a fixed number of initial elements, say 5.  We consider \(a_1, a_2, a_3, a_4, a_5\) explicitly.  They do not converge,  Now, we bisect the interval in which they exist.  
</p>

<p>
<em>Step 2.</em>  Now, at least one of the two intervals has infinitely many terms of the sequence.
</p>

<p>
<em>Step 3.</em>  Pick the interval that has infinitely more terms.  Now, pick the next 5 elements after \(a_5\) that exist in that interval.  
</p>

<p>
<em>Step 4.</em>  Repeat steps 2 and 3.  
</p>

<p>
This construct produces a Cauchy sequence, which we know converge.  After some number of iterations, all the elements will be in tiny intervals (see the formal definition of a Cauchy sequence).  
</p>

<div id="Topics in Analysis-Notes-Lecture 9; Supremum, infimum, Bolzano-Weierstrass-Bolzano-Weierstrass Theorem-An interesting application"><h5 id="An interesting application">An interesting application</h5></div>
<p>
Let us consider a bounded sequence derived from \(\pi\).  Consider:
</p>
\begin{align}
    \pi     &amp;= 3.1415926535...
    x_1     &amp;= 0.1415926535...
    x_2     &amp;= 0.415926535...
    x_3     &amp;= 0.15926535...
\end{align}

<p>
So, our rule is \(x_1 = \pi - 3\), and \(x_n = 10x_{n - 1} - \lfloor 10x_{n-1} \rfloor\).  
</p>

<p>
The Bolzano-Weierstrass theorem implies that there exists a subsequence such that \(x_{n_k}\) converges.  So, there exists arbirtrarily long string combinations that repeat infinitely many times in \(\pi\).  
</p>

<div id="Topics in Analysis-Notes-Lecture 10; Continuity"><h3 id="Lecture 10; Continuity">Lecture 10; Continuity</h3></div>
<p>
<em>Wednesday, February 10th</em>
</p>

<p>
Continuity.  What is it?  A property of a function?  Not quite.  It is a <em>local</em> property.  
</p>

<div id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity"><h4 id="Definitions of continuity">Definitions of continuity</h4></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-Sequence definition."></span><strong id="Sequence definition.">Sequence definition.</strong>  We may say \(f: \mathbb{R} \rightarrow \mathbb{R}\) is continuous in \(b^\star\) if for all sequences \(x_n \rightarrow b^\star\), it is true that \(f(x_n) \rightarrow f(b^\star)\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-Epsilon-Delta definition."></span><strong id="Epsilon-Delta definition.">Epsilon-Delta definition.</strong>  We may say that \(f\) is continuous in \(b^\star\) if for all \(\varepsilon &gt; 0\) there exists a \(\delta\) such that \(|x - b^\star| &lt; \delta\) implies \(|f(x) - f(b^\star)| &lt; \varepsilon\).  
</p>

<p>
These definitions are useful in different contexts, so it's nice to have them both.  
</p>

<div id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-A strange example"><h5 id="A strange example">A strange example</h5></div>
<p>
Let us consider a strange example of continuous functions.  Suppose we have a function \(f: \mathbb{R} \rightarrow \mathbb{R}\).  
</p>
\begin{align}
f(x) &amp;= 
\begin{cases}
    0   &amp;\text{if } x = 0       \\
    0   &amp;\text{if } x \text{ irrational}        \\
    \frac{1}{|q|}   &amp;\text{if } x = \frac{p}{q} \text{ where } \gcd(p, q) = 1
\end{cases}
\end{align}

<p>
Note that we may say that \(|f(x)| \leq |x|\).  The interesting part is that this function is continuous in zero.  
</p>

<p>
We use the fact that \(0 \leq |f(x)| \leq |x|\), and the \(\varepsilon\)-\(\delta\) definition of continuity.  
</p>

<p>
<em>Proof.</em>  Choose \(\delta = \varepsilon\).  Suppose \(|x| &lt; \varepsilon\).  Then, we have \(|f(x)| \leq |x| &lt; \varepsilon\).  It follows that \(|f(x) - f(0)| &lt; \varepsilon\).  This concludes our proof.  \(\square\)
</p>

<div id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-Polynomials are continuous"><h5 id="Polynomials are continuous">Polynomials are continuous</h5></div>
<p>
We now present some lemmas that allow us to conclude that polynomials are continuous.  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-Polynomials are continuous-Lemma A."></span><strong id="Lemma A.">Lemma A.</strong>  If \(f, g\) are continuous in \(b^\star\), then so is \(f + g\).  
</p>

<p>
<em>Proof.</em>  Suppose \(x_n \rightarrow b^\star\).  Then we have \(f(x_n) \rightarrow f(b^\star)\), and \(g(x_n) \rightarrow g(b^\star)\).  It follows, then, from the algebraic limit theorems that \(f(x_n) + g(x_n) \rightarrow f(b^\star) + g(b^\star)\).  \(\square\)
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-Polynomials are continuous-Lemma B."></span><strong id="Lemma B.">Lemma B.</strong>  If \(f, g\) are continuous in \(b^\star\), then so is \(f \cdot g\).  
</p>

<p>
<em>Proof.</em>  Suppose \(x_n \rightarrow b^\star\).  Then we have \(f(x_n) \rightarrow f(b^\star)\), and \(g(x_n) \rightarrow g(b^\star)\).  It follows, then, from the algebraic limit theorems that \(f(x_n) \cdot g(x_n) \rightarrow f(b^\star) \cdot g(b^\star)\).  \(\square\)
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-Polynomials are continuous-Lemma C."></span><strong id="Lemma C.">Lemma C.</strong>  If a function is continuous in \(b^\star\) and \(c \in \mathbb{R}\) is some constant, then \(c \cdot f\) is also continuous in \(b^\star\).  
</p>

<p>
<em>Proof.</em>  \(\lim c\cdot a_n = c\lim a_n \rightarrow c \cdot f(b^\star)\)
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-Polynomials are continuous-Lemma D."></span><strong id="Lemma D.">Lemma D.</strong>  The function \(f(x) = x\) is continous.  
</p>

<p>
<em>Proof.</em>  Suppose \(x_n \rightarrow b^\star\).  Then \(f(x_n) \rightarrow f(b^\star)\).  That's all Stefan wrote.  Eh.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 10; Continuity-Definitions of continuity-Polynomials are continuous-Theorem."></span><strong id="Theorem.">Theorem.</strong>  Polynomials are continous.
</p>

<p>
<em>Proof.</em>  This follows from Lemmas A through D.  Using Lemma A, we can treat each of the terms separately.  Using Lemma B, we can treat terms raised to a power greater than 1 by decomposing them into a product of order-one terms.  The coefficients are covered by Lemma C, and the order-one terms are covered by Lemma D.  \(\square\)
</p>

<div id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem"><h3 id="Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem">Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem</h3></div>
<p>
<em>Friday, February 12th</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Continuity of \(e^x\)"><h4 id="Continuity of \(e^x\)">Continuity of \(e^x\)</h4></div>
<p>
This is an example where the \(\varepsilon\)-\(\delta\) definition of a limit is helpful.  We claim that \(e^x\) is continous.  We will use the following definition of \(e^x\):
</p>
\[
e^x := \sum_{k = 0}^\infty \frac{x^k}{k!} = 1 + x + \frac{x^2}{2} + ...
\]
<p>
Because polynomials are continuous, each term of the sum converges independently.  We may hope that \(e^{x^\star} = 1 + x^\star + \frac{x^{2\star}}{2} + ...\) converges, but this proof does not work.  The sum of infinitely many continuous functions need not be continuous.  Why not?  
</p>

<p>
Let us consider the following function:
</p>
\begin{align}
f_n(x) &amp;=
\begin{cases}
    x^n     &amp;\text{ if} 0 \leq x \leq 1         \\
    1       &amp;\text{ if}        x \geq 1         \\
    0       &amp;\text{ otherwise}
\end{cases}
\end{align}

<p>
Now, let us consider \(g_n(x) = f_{x + 1}(x) - f_n(x)\).  This function is continuous.
</p>

<p>
Now, let us make the following non-rigorous statement:
</p>
\[
f_1(x) + \sum_{n = 1}^\infty g_n(x) \text{"}=\text{"} f_\infty(x)
\]

<p>
This function \(f_{\infty}(x)\) is no longer continuous.  We put quotation marks around the equals sign because it is ill-defined.  The sum of infinite polynomials does not work as an argument.
</p>

<p>
Fortunately, we may turn to the \(\varepsilon\)-\(\delta\) definition of continuity.  
</p>

<p>
<em>Proof.</em>
</p>

<p>
Our proof consists of two main steps:
</p>
<ol>
<li>
Demonstrate continuity around \(x = 0\).

<li>
Use the identity \(e^{x + h} - e^x = e^x(e^h - 1)\).

</ol>
<p>
Continuity everywhere else follows.  
</p>

\begin{align}
|e^h - 1|            &amp;=     \left|h + \frac{h^2}{2} + \frac{h^3}{6} + ... \right|                                \\
                     &amp;\leq  |h| + \left|\frac{h^2}{2}\right| + ...                                               \\
                     &amp;\leq  |h| + |h|^2 + ...                                                                    \\
                     &amp;= \frac{|h|}{1 - |h|}                                                                       \\
                     &amp;\leq 2|h|
\end{align}

<p>
Assume \(|h| \leq \frac{1}{2}\), if we want \(|e^h - 1| &lt; \varepsilon\), just take \(\delta \leq \frac{\varepsilon}{2}\).  
</p>

<p>
So, if we would like to have \(|e^{x^\star}(e^h - 1)| \leq \varepsilon\), we simply need to select \(h &lt; \frac{\varepsilon}{2e^{x^\star}}\).  \(\square\)
</p>
<div id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Continuity of \(e^x\)-Commentary"><h5 id="Commentary">Commentary</h5></div>
<p>
The previous proof on the continuity of \(e^x\) may have been rather confusing.  Here's a way of understanding it, and the \(\varepsilon\)-\(\delta\) criterion for continuity, more intuitively.
</p>

<p>
Alice and Bob are walking down the street, pondering the continuity of \(e^x\).  Bob says, if we use the identity \(e^{x + h} - e^x = e^x(e^h - 1)\), then proving continuity at \(x = 0\) means that we have proven continuity everywhere.  To help Bob get started, Alice says, "I will pick a \(\varepsilon\), any \(\varepsilon &gt; 0\), and you must be able to supply me with a \(\delta\) which ensures that if \(h\) is within \(\delta\) of \(0\), then \(e^h - 1\) is within \(\varepsilon\) of \(0\)."  
</p>

<p>
Bob assents.  He says, "ah, I see.  If that's the case, then I should set about finding a \(\delta\) that bounds \(h\) properly.  I will work backwards.  I know that \(|e^h - 1| &lt; \varepsilon\) must be true, so I need to take this information into account to find an \(h\) that ensures this to be true."  He spends a while thinking.  "Okay," he says.  "I've discovered that if we constrain \(|h| \leq \frac{1}{2}\), then we can guarantee \(|e^h - 1| \leq 2|h|\).  
</p>

<p>
"Nice!"  Alice responded.  "With that bound on \(|e^h - 1|\), if you can ensure that \(2|h| &lt; \varepsilon\), then you have can guarantee that \(|e^h - 1| &lt; \varepsilon\) as well!  All you need to do is specify \(\delta = \frac{\varepsilon}{2}\); if you do that, then you will have \(|h|\) small enough."  
</p>

<div id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Some theorems on bounded functions"><h4 id="Some theorems on bounded functions">Some theorems on bounded functions</h4></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Some theorems on bounded functions-Theorem."></span><strong id="Theorem.">Theorem.</strong>  A continuous function on an interval \([a, b]\) is bounded from above and below.  We will only show that \(f\) is bounded from above, as the argument for bounding from below is symmetric (take \(-f\)).  
</p>

<p>
<em>Proof.</em>  Suppose not.  Then for every point in the function you name, you can always choose a point that is bigger.  Then, there exists \(x_n \in [a, b]\) such that \(f(x_n) &gt; n\).  This is a sequence in \([a, b]\).  
By the Bolzano-Weierstrass Theorem, there exists a convergent subsequence \(x_{n_k} \rightarrow c\).  The continuity of \(f\) implies \(f(x_{n_k} \rightarrow c\).  However, this does not work, because \(f(c)\) is a fixed number.  But, \(f(x_{n_k})\) is a sequence that <em>keeps increasing.</em>  We have \(f(x_{n_k}) &gt; n_k \rightarrow \infty\).  It cannot converge.  Contradiction.  \(\square\)
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Some theorems on bounded functions-Discussion."></span><strong id="Discussion.">Discussion.</strong>  But what about \(f(x) = \frac{1}{x}\) on \((0, 1]\)?  This is a continuous function on an inteval, but it is not bounded from above.  Clearly, we need the endpoints (zero is excluded here).  But why?  It turns out that the subsequence may converge to something that's <em>not</em> in the interval.  In fact, the sequence itself converges \(x_n \rightarrow \frac{1}{n}\).  That's why you need the endpoints.  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Some theorems on bounded functions-Theorem."></span><strong id="Theorem.">Theorem.</strong>  A continuous function on [a, b] assumes its \(S = \sup \{f(x) | a \leq x \leq b\}\).  ("Assumes" here means that it takes that value.)
</p>

<p>
<em>Proof.</em>  We want to show that for an arbitrary \(\varepsilon &gt; 0\), there exists a \(a \leq x^\star \leq b\) that fulfills \(f(x^\star) \geq S - \varepsilon\).  If this were not true, then \(S - \varepsilon\) would be the supremum.  Recall that a supremum is the <em>smallest</em> upper bound possible.
</p>

<p>
Now, we find a sequence \(x_n\) in \([a, b]\) such that \(f(x_n) \geq S - \frac{1}{n}\).  Then, we have \(S \geq f(x_n) \geq S - \frac{1}{n}\).  
</p>

<p>
We then apply the Bolzano-Weierstrass Theorem.  There is a subsequence \(x_{n_k}\) that converges to \(c\).  We now have \(S \geq f(x_{n_k}) \geq S - \frac{1}{n_k}\), and \(f(x_{n_k}) \rightarrow c\).  
</p>

<p>
By the sandwich lemma, we may conclude that \(f(c) = \lim_{k \rightarrow \infty} f(x_{k_k}) = S\).  \(\square\)
</p>

<div id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Intermediate Value Property"><h4 id="Intermediate Value Property">Intermediate Value Property</h4></div>
<p>
This is a property for continuous functions \(f\) on the interval \([a, b]\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Intermediate Value Property-Statement."></span><strong id="Statement.">Statement.</strong>  If we have \(f(a) &lt; y &lt; f(b)\), then there exists \(a \leq x \leq b\) for which \(f(x) = y\).  
</p>

<p>
<em>Remark.</em>  We will not prove this, but the number of solutions is always odd.
</p>

<p>
This property is useful for showing the existence of solutions.  For example, suppose wish to know whether or not \(\cos(x) = x\) has a solution.  
</p>

<p>
We define a function \(h(x) = \cos(x) - x\).  Observe that \(h(0) = 1 &gt; 0\), and that \(h(2\pi) = 1 - 2\pi &lt; -5\).  By the Intermediate Value Property, there exists an \(x^\star\) such that \(\cos(x^\star) = x^\star\).
</p>

<div id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Borsuk-Ulam Theorem"><h4 id="Borsuk-Ulam Theorem">Borsuk-Ulam Theorem</h4></div>
<p>
Here we present a special case of the theorem, which is possible through the intermediate value property.  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 11; Continuity, continued, intermediate value property, Borsuk-Ulam Theorem-Borsuk-Ulam Theorem-Theorem."></span><strong id="Theorem.">Theorem.</strong>  There exists at any time two points on the equator that are antipodal with the same temperature.  This can also be said for barometric pressure, humidity -- any continuous function.
So, if you had a ring, and a continuous function on \(2\mathbb{D} = \{x: |x| = 1\}\), then the equation \(f(x) = f(-x)\) always has a solution.  (The ring in this case is the unit circle.)
</p>

<p>
<em>Proof.</em>   Suppose not.  Pick any number \(x^\star\).  Then \(f(x^\star) \neq f(-x^\star)\).  
</p>

<p>
Now, consider \(h(x) = f(x) - f(-x)\).  This function is continous on the boundary of the unit disk.  Then, we have:
</p>
\begin{align}
    h(x^\star)      &amp;= f(x^\star) - f(-x^\star) \neq 0          \\
    h(-x^\star)     &amp;= f(-x^\star) - f(x^\star) = -h(x^\star)
\end{align}

<p>
Observe that because when \(x^\star\) went to \(-x^\star\), \(h(x^\star)\) went to \(-h(x^\star)\), then the function <em>must</em> have crossed zero at some point.  Contradiction.  \(\square\)
</p>

<p>
<em>Remark.</em>  This problem depends on its geometry.  It's a beautiful instance in which geometry and analysis meet.  Though left unproven here, it generalizes to higher dimensions.
</p>

<div id="Topics in Analysis-Notes-Lecture 12; Intermediate value theorem, uniform continuity"><h3 id="Lecture 12; Intermediate value theorem, uniform continuity">Lecture 12; Intermediate value theorem, uniform continuity</h3></div>
<p>
<em>Monday, February 15th</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 12; Intermediate value theorem, uniform continuity-Intermediate value theorem"><h4 id="Intermediate value theorem">Intermediate value theorem</h4></div>
<p>
Recall from last class the Intermediate Value Theorem.  It is useful for studying systems which are highly nonlinear.  A nice application of it is the Borsuk-Ulam Theorem from last class.  Today, we will prove the intermediate value theorem.
</p>

<p>
<em>Motivation.</em>  Suppose we have a function \(f(x) = x^2 - 2\), where we know \(f(0) &lt; 0\), and \(f(2) &gt; 0\).  Then, we wish to show that there exeists an \(x \in \mathbb{R}\), \(0 &lt; x &lt; 2\), such that \(f(2) = 0\).  
</p>

<p>
<em>Proof.</em>  
</p>

<p>
<em>Step 1.</em>  Consider the set \(S = \{a \leq x \leq b: f(x) &lt; y\}\).  We know that is not empty, because we have at least \(a \in S\).  
</p>

<p>
<em>Step 2.</em>  Observe that the set \(S\) is bounded from above by \(b\).  Therefore, a finite smallest upper bound \(c = \sup S\) exists.
</p>

<p>
<em>Step 3.</em>  The existence of \(c\) implies that for all \(\varepsilon &gt; 0\), there exists \(h \in S\) such that \(c - \varepsilon &lt; h &lt; c\).  This just says that there are elements near \(c\), because it is the supremum.
</p>

<p>
<em>Step 4.</em>  This allows us to construct a sequence \(x_n \in S\), \(x_n \rightarrow c\), by choosing subsequently smaller \(\varepsilon\).  
</p>

<p>
<em>Step 5.</em>  We know that \(f(x_n) &lt; y\) for every \(n\).  
</p>

<p>
<em>Step 6.</em>  \(\lim_{n\rightarrow \infty} f(x_n) = f(c)\).  This is just the definition of continuity.  
</p>

<p>
<em>Step 7.</em>  \(f(c) \leq y\).
</p>

<p>
Now, suppose by way of contradiction that \(f(c) &lt; y\).  By the \(\varepsilon\)-\(\delta\) definition of continuity, there exists a \(\delta &gt; 0\) such that \(f(z) &lt; y\) for all \(z \in (c - \delta, c + \delta)\).  This contradicts \(c = \sup s\) beacuse it implies there are elements in \(S\) (the entire \((c - \delta, c + \delta)\)) that are bigger than \(c\).  Basically, we can have elements \(\delta\) away from \(c\) <em>in either direction</em>.  
</p>

<p>
Thus, we may conclude that \(f(c) = y\).  \(\square\)
</p>


<div id="Topics in Analysis-Notes-Lecture 12; Intermediate value theorem, uniform continuity-Uniform continuity"><h4 id="Uniform continuity">Uniform continuity</h4></div>
<p>
Recall the definition of continuity on a domain \(D\), where \(D\) be \((a, b)\), \([a, b]\), or \(\mathbb{R}\).  We may say that a function is <span id="Topics in Analysis-Notes-Lecture 12; Intermediate value theorem, uniform continuity-Uniform continuity-continuous"></span><strong id="continuous">continuous</strong> on a domain \(D\) if, for all \(x \in D\), and given any arbitrarily small \(\varepsilon &gt; 0\), there exists and we may find a \(\delta &gt; 0\) such that for all \(y\),  we have \(|x - y| &lt; \delta\) implies \(|f(x) - f(y)| &lt; \varepsilon\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 12; Intermediate value theorem, uniform continuity-Uniform continuity-Definition: Uniform continuity."></span><strong id="Definition: Uniform continuity.">Definition: Uniform continuity.</strong>  A function \(f\) is uniformly continuous on a domain \(D\) if for all \(\varepsilon &gt; 0\) there exists a \(\delta &gt; 0\) such that for all \(x \in D\) and for all \(y \in D\), we have \(|x - y| &lt; \delta\) implies \(|f(x) - f(y)| &lt; \varepsilon\).  
</p>

<p>
The difference between continuity and uniform continuity is subtle, but it exists.  In normal continuity, the choice of \(\delta\) depends on the \(x\) at which we wish to prove continuity.  In uniform continuity, the choice of \(\delta\) is made regardless of which \(x\) at which we wish to prove continuity, because it must hold true for all \(x\).  
</p>

<p>
In short, uniform continuity says that a function does not go crazy.  
</p>

<div id="Topics in Analysis-Notes-Lecture 12; Intermediate value theorem, uniform continuity-Uniform continuity-Example"><h5 id="Example">Example</h5></div>
<p>
Suppose we wish to prove that \(f(x) = x^2\) is uniformly continuous.  
</p>

<p>
We want to be able to select a \(\delta &gt; 0\) such that if \(|x^* - x| &lt; \delta\), the following holds for any \(\varepsilon &lt; 0\).  
</p>
\begin{align}
|f(x^*)- f(x)| &amp;= |x^{*^2} - x^2|                 \\
               &amp;= |x^* + x||x^* - x|              \\
               &amp;&lt; \varepsilon
\end{align}

<p>
But at this point, we observe that if we want the above to hold, then we must set \(|x^* - x| \leq \frac{\varepsilon}{|x^* + x|}\).  As we can see, the requisite \(\delta\) does depend on the choice of \(x\).  Therefore, \(f(x) = x^2\) is <em>not</em> uniformly continuous.  
</p>

<div id="Topics in Analysis-Notes-Lecture 12; Intermediate value theorem, uniform continuity-Uniform continuity-More remarks on uniform continuity"><h5 id="More remarks on uniform continuity">More remarks on uniform continuity</h5></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 12; Intermediate value theorem, uniform continuity-Uniform continuity-More remarks on uniform continuity-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If \(f:[a, b] \rightarrow \mathbb{R}\) is continuous, then it is uniformly continuous.
</p>

<p>
<em>Proof.</em>  Suppose not.  Then there exists \(\varepsilon &gt; 0\) such that for all \(\delta &gt; 0\), there exists \(x, y \in [a, b]\), we have \(|x - y| &lt; \delta\) and \(|f(x) - f(y)| &gt; \varepsilon\).  
</p>

<p>
We want to <em>fail</em> to prove this claim, and we will do so by leveraging the fact that it must be true for <em>all</em> \(\delta\).  
</p>

<p>
Imagine we have \(\varepsilon = 1\).  We will choose subsequently smaller \(delta &gt; 0\) with the rule that \(\delta_n = 2^{-n}\).  For each subsequent \(\delta\), the distance \(\varepsilon\) is the same.  Once we can prove that there are \(\delta &gt; 0\) such that the jump between \(f(x)\) and \(f(y)\) is smaller than \(\varepsilon\), then we are done.
</p>

<p>
Now, we employ the Bolzano-Weierstrass Theorem!  We have \(x_{n_k} \rightarrow c\).  Then \(|x_{n_k} - y_{n_k}| \leq \frac{1}{2^{n_k}}\), so we also have \(y_{n_k} \rightarrow c\).  This implies that \(f(x_{n_k}) \rightarrow f(c)\) and \(f(y_{n_k}) \rightarrow f(c)\).  We thus have \(\lim_{k \rightarrow \infty} |f(x_{n_k}) - f(y_{n_k})| = 0 \geq \varepsilon\), which violates the claim that it is actually supposed to be \(\geq \varepsilon &gt; 0\).  Contradiction.  \(\square\)
</p>

<div id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums"><h3 id="Lecture 13; Lipschitz continuity, Riemann sums">Lecture 13; Lipschitz continuity, Riemann sums</h3></div>
<p>
<em>Wednesday, February 17th</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Lipschitz continuity"><h4 id="Lipschitz continuity">Lipschitz continuity</h4></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Lipschitz continuity-Definition."></span><strong id="Definition.">Definition.</strong>  A function \(f: \mathbb{R} \rightarrow \mathbb{R}\) is Lipschitz continuous with Lipschitz constant \(\mathcal{L}\) (\(0 \leq \mathcal{L} &lt; \infty\)) if, for all \(x, y \in \mathbb{R}\), \(|f(x) - f(y)| \leq \mathcal{L}|x - y|\).  
</p>

<p>
Here are some examples of Lipschitz continuous functions:
</p>

<table>
<tr>
<th>
\(f\)
</th>
<th>
\(\mathcal{L}\)
</th>
</tr>
<tr>
<td>
\(f(x) = x\)
</td>
<td>
\(1\)
</td>
</tr>
<tr>
<td>
\(f(x) = \sin(2x)\)
</td>
<td>
\(2\)
</td>
</tr>
<tr>
<td>
\(f(x) = x^2\)
</td>
<td>
<em>not Lipschitz continuous</em>
</td>
</tr>
</table>

<p>
<span id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Lipschitz continuity-Theorem."></span><strong id="Theorem.">Theorem.</strong>  Lipchitz continuity implies uniform continuity.  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Lipschitz continuity-Proof."></span><strong id="Proof.">Proof.</strong>  \(\delta = \frac{\varepsilon}{\mathcal{L}}\).
</p>

<p>
Note that the concept of Lipschitz continuity is philosophically very similar to differentiation.  Think of it as the following: \(\frac{|f(x) - f(y)|}{x - y} \leq L\).  
</p>

<p>
In fact, we know that if \(\lim_{y \rightarrow x} \frac{|f(x) - f(y)|}{x - y}\) exists, then \(f'(x) \leq L\).  
</p>

<div id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Integration: an introduction"><h4 id="Integration: an introduction">Integration: an introduction</h4></div>
<p>
We know that at the Italians and at least one Babylonian knew about integration.  What type of integrals are out there?
</p>
<ul>
<li>
Riemann

<li>
Lebesgue

<li>
Bochner

<li>
Daniell

<li>
Ito

<li>
Stratonovich

</ul>

<p>
We begin by focusing on the Riemann integral.
</p>

<div id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Riemann integral"><h4 id="Riemann integral">Riemann integral</h4></div>
<p>
We are familiar with the idea that \(\int_a^b f(x) dx\) denote the area under the curve \(f(x)\) between \(a\) and \(b\).  A natural way to approximate this number is by splitting this interval into smaller partitions \(a = x_0 &lt; a_1 &lt; ... &lt; x_n = b\).  We may now define upper and lower sums:
</p>
\begin{align}
L_P     &amp;= \sum_{k = 1}^n (x_k - x_{k - 1})\inf\{f(x) | x_{k - 1} \leq x \leq x_k\} \\
U_P     &amp;= \sum_{k = 1}^n (x_k - x_{k - 1})\sup\{f(x) | x_{k - 1} \leq x \leq x_k\} 
\end{align}

<p>
<span id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Riemann integral-Lemma."></span><strong id="Lemma.">Lemma.</strong>  Suppose \(P = \{\text{some points}\}\) and \(Q = P \cup \{\text{some more points}\}\).  Then we find that \(L_Q \geq L_P\) and \(U_Q \leq U_P\).  
</p>

<p>
<em>Proof.</em>  There are two cases here:
</p>
<ol>
<li>
We do not add a point; that is \(Q = P\).  Then the computation is the same.

<li>
We add one or more points.  Then we make it slightly more precise.

</ol>

<p>
It can be shown that if \(A \subset B\), then \(\inf\{f(x) | x \in A\} \geq \inf\{f(x) | x \in B\}\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Riemann integral-Lemma."></span><strong id="Lemma.">Lemma.</strong>  Let \(P\) and \(Q\) be arbitrary partitions.  Then, \(L_P \leq U_Q\).  
</p>

<p>
<em>Proof.</em>  Here, we use the previous lemma.  
</p>

\[
L_P \leq L_{P \cup Q} \leq U_{P \cup Q} \leq U_Q
\]
<p>
If the sum exists, the lower sum is always lower than than the area.  That's why this is so useful. 
</p>

<p>
This implies that \(\sup\{L_P | P \text{ partition}\} \leq \inf\{U_Q | Q \text{ partition}\}\).  Recall that if \(a \leq b\) for all \(a \in A, b \in B\), then \(\sup A \leq \inf B\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Riemann integral-Definition."></span><strong id="Definition.">Definition.</strong>  If for two arbitrary partitions \(P\) and \(Q\), \(\sup\{L_P|P \text{ partition}\} = \inf\{U_Q|Q \text{ partition}\}\), then \(f\) is Riemann integrable, on \([a, b]\), and \(\int_a^b f(x) dx = \inf(U_Q) = \sup(L_P)\).  Stefan calls this the "magic number in the middle".  Note that in this definition, there is <em>no notion of area</em>.  
</p>

<div id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Riemann integral-Example: Not Riemann integrable"><h5 id="Example: Not Riemann integrable">Example: Not Riemann integrable</h5></div>
\begin{align}
f(x) = 
    \begin{cases}
    1       &amp;\text{ if } x \in \mathbb{R} \setminus \mathbb{Q}      \\
    0       &amp;\text{ if } x \in \mathbb{Q}
    \end{cases}
\end{align}

<p>
Suppose we consider \(\int_0^1 f(x) dx\).  The computation for all arbitrary partitions \(P, Q\) would be \(L_P = 0\), \(U_Q = 1\).  For every small interval, there is an irrational number and a rational number, causing this behavior.  But the integral <em>should</em> exist, with the value of 1.  Although this function is not Riemann integrable, it <em>is</em> integrable.  To integrate it, we should look towards Lebesegue integration.  
</p>

<div id="Topics in Analysis-Notes-Lecture 13; Lipschitz continuity, Riemann sums-Riemann integral-Example: Riemann integrable"><h5 id="Example: Riemann integrable">Example: Riemann integrable</h5></div>
<p>
Suppose we consider \(\int_0^1 x dx\).  We know that the answer is \(\frac{1}{2}\), but is it Riemann integrable?
</p>

\[
\sum_{k = 1}^n \frac{k}{n^2} = \frac{1}{n^2} \sum_{k = 1}^n k
\]

<div id="Topics in Analysis-Notes-Lecture 15; Riemann sums, improper integrals"><h3 id="Lecture 15; Riemann sums, improper integrals">Lecture 15; Riemann sums, improper integrals</h3></div>
<p>
<em>Monday, February 22nd</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 15; Riemann sums, improper integrals-Riemann sums, reviewed"><h4 id="Riemann sums, reviewed">Riemann sums, reviewed</h4></div>
<p>
Recall the definition of upper and lower sums from last lecture.  If we have:
</p>
\[
\sup_p \texttt{lower sum} = \inf_p \texttt{upper sum}
\]
<p>
Then the value on either side is the Riemann integral.  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 15; Riemann sums, improper integrals-Riemann sums, reviewed-Theorem."></span><strong id="Theorem.">Theorem.</strong>  Continuous functions on closed intervals are Riemann integrable.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 15; Riemann sums, improper integrals-Riemann sums, reviewed-Theorem."></span><strong id="Theorem.">Theorem.</strong>  It does not matter which partitions you take, as long as the largest element goes to zero.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 15; Riemann sums, improper integrals-Riemann sums, reviewed-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If \(f: [a, b] \rightarrow \mathbb{R}\) is monotone and bounded, then it is Riemann integrable.
</p>

<p>
Let us reflect on that last theorem.  Suppose we had a function that was monotone and bounded, but not continuous.  Normally, this would be bad, because the upper and lower do not match; however, it is okay, but the width goes to "zero".  If it is bounded and monotone, it cannot have many jumps.  On the other hand, if it were not monotone, it could have a wild number of jumps.
</p>

<div id="Topics in Analysis-Notes-Lecture 15; Riemann sums, improper integrals-Improper integrals"><h4 id="Improper integrals">Improper integrals</h4></div>
<p>
Suppose you wanted to integrate \(\int_0^1 \frac{1}{\sqrt{x}} dx\).  With our definition of Riemann integrals, we cannot do this, since for the leftmost interval, the infimum would never equal the supremum.  We would always get infinity at zero for \(\texttt{Upper} = \sum_{k = 1}^n (\sup_{I_k} f) |I_k|\).  
</p>

<p>
Instead, we can do the following:
</p>
\begin{align}
\int_\varepsilon^1 \frac{1}{\sqrt{x}}dx         &amp;= 2\sqrt{x}|_\varepsilon^1         \\
                                                &amp;= 2 - 2\sqrt{\varepsilon}          \\
                                                &amp;\overset{\varepsilon \rightarrow 0}{\longrightarrow} 2
\end{align}

<p>
<span id="Topics in Analysis-Notes-Lecture 15; Riemann sums, improper integrals-Improper integrals-Definition."></span><strong id="Definition.">Definition.</strong>  If \(f: (a, b] \rightarrow \mathbb{R}\) and \(\lim_{\varepsilon \rightarrow 0+} \int_{a + \varepsilon}^b f(x) dx\) exists, then we may define: 
</p>
\[
\int_a^b f(x) dx = \lim_{\varepsilon \rightarrow 0+}^b f(x) dx
\]

<div id="Topics in Analysis-Notes-Lecture 15; Riemann sums, improper integrals-Improper integrals-Example"><h5 id="Example">Example</h5></div>
\begin{align}
\int_{\varepsilon}^1 \frac{1}{x^\alpha} dx      &amp;= \frac{x^{-\alpha + 1}}{1 - \alpha}\rvert_\varepsilon^1       \\
                                                &amp;= \frac{1}{1 - \alpha} - \frac{1}{1 - \alpha}\frac{1}{\varepsilon^{\alpha - 1}}
\end{align}
<p>
If \(\alpha &lt; 1\), then \(\int_0^1 \frac{1}{x^\alpha} dx = \frac{1}{1 - \alpha}\).  If \(\alpha = 1\), then \(\int_\varepsilon^1 \frac{1}{x^\alpha} dx = \log(x)\rvert_\varepsilon^1 = -\log(\varepsilon)\), which is bad as \(\varepsilon\) approaches zero. 
</p>

<div id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability"><h3 id="Lecture 16; Improper integrals, cont., differentiability">Lecture 16; Improper integrals, cont., differentiability</h3></div>
<p>
Last lecture, we considered the statement \(\int_0^1 \frac{1}{\sqrt{x}} dx = 2\), which is a true statement, although the integral on the left cannot be computed via a Riemann integral.  To resolve this, we introduced the concept of an improper integral.  
</p>

<p>
Now, suppose you wanted \(\int_0^1 \frac{\cos(x)}{\sqrt{x}} dx\).  How would you show that the limit \(\lim_{\varepsilon \rightarrow 0+} \frac{\cos(x)}{\sqrt{x}}dx\) exists?
</p>

<p>
<em>Proof.</em>  We argue that the limit exists via continuity.
</p>

<p>
Saying that the \(\lim_{\varepsilon \rightarrow 0+} \int_\varepsilon^1 f(x) dx\) exists is equivalent to saying that \(\int_{\varepsilon}^1 f(x) dx\) is continuous in \(\varepsilon = 0\).  This is now a function \(F(\varepsilon)\) of \(\varepsilon\).  
</p>

<p>
We use a one-sided form of continuity, which may be succinctly stated as:  for all \(\varepsilon &gt; 0\), there exists a \(\delta &gt; 0\) such that for all \(a, b \in (0, \delta)\), \(\left|\int_a^1 f(x) dx - \int_b^1f(x)dx\right| &lt; \varepsilon\), which may be rewritten as \(\left|\int_a^b f(x) dx \right| &lt; \varepsilon\).  
</p>

<div id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability-Example"><h5 id="Example">Example</h5></div>
<p>
Consider \(\int_0^1 \frac{\cos(x)}{\sqrt{x}} dx\).  We wish to show that for all \(\varepsilon &gt; 0\), there exists a \(\delta &gt; 0\) such that for all \(a &lt; b \in (0, \delta)\), \(\left|\int_a^b \frac{\cos(x)}{\sqrt{x}} dx \right| &lt; \varepsilon\).  
</p>
\begin{align}
    \left| \int_a^b \frac{\cos(x)}{\sqrt{x}} dx \right|     &amp;\leq   \int_a^b \frac{|\cos(X)|}{\sqrt{x}}dx            \\
                                                            &amp;\leq   \int_a^b \frac{1}{\sqrt{x}}                     \\
                                                            &amp;=      2\sqrt{b} - 2\sqrt{a}                           \\
                                                            &amp;\leq   2\sqrt{b}                                       \\
                                                            &amp;\leq   2\sqrt{\delta}                                  \\
                                                            &amp;&lt;      \varepsilon
\end{align}
<p>
We don't need to compute \(\delta\), we just need to make sure that it is small enough.
</p>

<div id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability-Example"><h5 id="Example">Example</h5></div>
<p>
Consider \(\int_0^1 \frac{1}{x^2}dx\).  This is <em>not</em> Riemann integrable, as you may not write down partitions from one to infinity.
</p>

<p>
However, if we rewrite it with a limit, we are able to derive an expression that <em>is</em> Riemann integrable.
</p>
\begin{align}
    \int_1^\infty \frac{1}{x^2}dx       &amp;=  \lim_{N \rightarrow \infty} \int_1^N \frac{1}{x^2}dx        \\
                                        &amp;=  \lim_{N \rightarrow \infty} \frac{-1}{N} + 1                \\
                                        &amp;=  1
\end{align}

<p>
As we can see, the improper integral works.  
</p>

<p>
<em>Proof.</em>  For all \(\varepsilon &gt; 0\), there exists an \(M\) such that for all \(a, b \in (M, \infty)\), \(\lvert \int_a^b f(x) dx \rvert &lt; \varepsilon\).  
</p>

<p>
As an aside, here is one type of integral that Riemann can do, but Lebesgue cannot.  Consider:
</p>

\[
\int_0^\infty   \sin(x^2) dx
\]

<p>
The oscillations get faster, and the bumps cancel out, apparently.
</p>

<div id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability-Differentiation"><h4 id="Differentiation">Differentiation</h4></div>
<p>
Differentiation, like continuity, is also a local property.  
</p>

<p>
We may say that \(f: \mathbb{R} \rightarrow \mathbb{R}\) is differentiable in \(x_0\) if \(\frac{f(x_0 + h) - f(x_0)}{h}\) as a function of \(h\) is continuous in zero.  
</p>

\[
    f'(x_0) = \lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0)}{h}
\]

<p>
<span id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability-Differentiation-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If \(f\) is differentiable in \(x_0\), then it is continuous in \(x_0\).  
</p>

<p>
<em>Proof.</em>  We want to show that \(f(x_0 + h) - f(x_0)\) is very small, So we can restate it as \(\frac{f(x_0 + h) - f(x_0)}{h} \cdot h\), and say that \(h\) gets sufficiently small...
</p>

<div id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability-Differentiation-Continuous but not differentiable"><h5 id="Continuous but not differentiable">Continuous but not differentiable</h5></div>
<p>
Let us consider a classic example from Weierstrass.  Take \(\sin(x)\), and then add another sine that is smaller in height but larger in oscillation.  Repeat.  Ultimately, you get a very jagged sine curve that is represented as:
</p>
\[
\sum_{n = 1}^\infty \frac{\sin(x 4^n)}{2^n}
\]

<p>
This function is continuous, but not differentiable.  If we were to naively differentiate it, we would get \(\sum_{n = 1}^\infty \frac{\cos(x 4^n) 4^n}{2^n}\), which 'equals' \(\sum_{n = 1}^\infty \cos(x 4^n) 2^n\).  
</p>

<p>
In fact, most continuous functions are not differentiable (Baire Category Theorem).  
</p>

<div id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability-Differentiation-Some theorems on differentiability"><h5 id="Some theorems on differentiability">Some theorems on differentiability</h5></div>
<p>
<span id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability-Differentiation-Some theorems on differentiability-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If \(f, g\) are differentiable, then for all \(\alpha, \beta \in \mathbb{R}\), \(\alpha f + \beta g\) is differentiable, with \((\alpha f + \beta g)' = \alpha f' + \beta g'\).  
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 16; Improper integrals, cont., differentiability-Differentiation-Some theorems on differentiability-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If \(f, g\) are differentiable, then \((f \cdot g)' = f'g + fg'\).  
</p>

<p>
<em>Proof.</em>  
</p>

\begin{align}
    \frac{f(x + h)g(x + h) - f(x)g(x)}{h}       &amp;= \frac{f(x + h) - f(x)}{h}g(x + h) + \frac{g(x + h) - g(x)}{h} f(x)
\end{align}

<p>
We want to show that this is continuous as \(h\) gets small.  We will use the fact that if \(a_n \rightarrow a, b_n \rightarrow b\), then \(\lim_{n \rightarrow \infty} a_n b_n = ab\).  
</p>

<p>
First, note, because \(f\) is differentiable, that:
</p>
\[
    \frac{f(x + h) - f(x)}{h} \rightarrow f'(x)
\]

<p>
And also note, due to continuity of \(g\), that:
</p>
\[
    g(x + h) \rightarrow g(x)
\]
<p>
And also that:
</p>
\[
    \frac{g(x + h) - g(x)}{h} \rightarrow g'(x)
\]

<p>
Thus, we the limits come together to prove the product rule.  \(\square\)
</p>

<p>
The proof for the quotient rule is similar.  
</p>

<p>
Let us now consider chain rule. 
</p>

<p>
If \(f\) is differentiable in \(x_0\), \(f(x + h) \sim f(x) + f'(x)h + ...\), where '...' are smaller order terms of h.  We can say the same for a differentiable function \(g\).  
</p>

\begin{align}
    f(g(x + h))     &amp;=  f(g(x) + hg'(x))            \\
                    &amp;= f(g(x)) + f'(g(x))g'(x)h + ...
\end{align}

<p>
This is an intuitive proof that Newton gave for chain rule.  
</p>

<p>
<em>Aside.</em>  In the spirit of physics, Newton named the lower order terms "fluxions".  
</p>

<div id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem"><h3 id="Lecture 17; Differentiation, cont., mean value theorem">Lecture 17; Differentiation, cont., mean value theorem</h3></div>
<p>
<em>Friday, February 26th</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem-Some remarks on continuity"><h4 id="Some remarks on continuity">Some remarks on continuity</h4></div>
<p>
From the homework, it is interesting to note that \(\sqrt{x}\) is <em>not</em> Lipschitz continuous, though it <em>is</em> uniformly continuous.  The problem arises at zero, where the derivative is "infinite".  
</p>

<p>
From strongest to weakest in assumptions, we have Lipschitz, Uniformly Continuous, Continuous, Functions.
</p>

<div id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem-Differentiation, cont."><h4 id="Differentiation, cont.">Differentiation, cont.</h4></div>
<p>
A way to think of differentiation and differentiability is that it is a 'best linear approximation'.  To show that a function is differentiable in a point \(x\), we want to show that \(\lim_{h \rightarrow 0} \frac{f(x + h) - f(x)}{h}\) exists.  
</p>

<p>
Informally, we may say that \(f(x + h) = f(x) + f'(x)h + ...\) where '...' are smaller order terms of \(h\).  From the definition, as \(h\) approaches zero, the smaller order terms of \(h\) divided by \(h\) approach zero.
</p>

<p>
For more history about differentiation, see William Thurston's <a href="http:&#47;&#47;arxiv.org&#47;abs&#47;math&#47;9404236">On proof and progress of mathematics</a>.  It has a list of over 50 definitions of differentiation.
</p>

<p>
So, why are derivatives so great?
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem-Differentiation, cont.-Theorem."></span><strong id="Theorem.">Theorem.</strong>  If a function \(f: [a, b] \rightarrow \mathbb{R}\) is differentiable in \((a, b)\), and there is a \(c\) such that \(a &lt; c &lt; b\) and \(f(c)\) is the maximum, then \(f'(c) = 0\).  
</p>

<p>
<em>Proof.</em>  Consider \(\lim_{h \rightarrow 0+} \frac{f(c + h) - f(c)}{h}\). Because \(f(c)\) is the maximum, no matter what \(h\) is, this limit is always negative or zero.  
</p>

<p>
Recall that if we have a sequence \(a_n \rightarrow a\), which fulfills \(\forall n a_n \leq L\), then \(a \leq L\).  Therefore, \(\lim_{h \rightarrow 0+} \frac{f(c + h) - f(c)}{h} \leq 0\).  We have \(f'(c) \leq 0\).  
</p>

<p>
Now, let us consider \(\lim_{h \rightarrow 0+} \frac{c - h) - f(c)}{h}\).  We find that \(-f'(c) \leq 0\), implying \(f'(c) \geq 0\).  \(\square\)
</p>

<p>
<em>Note.</em>  Stefan was not especially clear about this proof during class.  For an indirect version of this proof, see the book.
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem-Differentiation, cont.-Rolle&#39;s Theorem."></span><strong id="Rolle&#39;s Theorem.">Rolle's Theorem.</strong>  If a function satisfies \(f: [a, b] \rightarrow \mathbb{R}\), \(f(a) = f(b)\), and \(f\) is differentiable in \((a, b)\), then there exists a \(c \in (a, b)\) such that \(f'(c) = 0\).  
</p>

<p>
<em>Proof.</em>  We divide this into two cases.  Assume without loss of generality that \(f(a) = 0 = f(b)\).  
</p>

<p>
<em>Case 1.</em>  \(f\) is always zero.  Then we may choose \(c\) to be any point in the interval \((a, b)\).  
</p>

<p>
<em>Case 2.</em>  There exists a \(d\) in \((a, b)\) such that \(f(d) \leq 0\).  Then the function has a maximum or minimum.  If it has a minimum, multiply the function by \(-1\), such that \(f(d) &gt; 0\).  This implies that \(f\) has a maximum, which with the previous theorem implies the existence of a \(c\) such that \(f'(c) = 0\).  \(\square\)
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem-Differentiation, cont.-Mean Value Theorem."></span><strong id="Mean Value Theorem.">Mean Value Theorem.</strong>  If a function satisfies \(f: [a, b] \rightarrow \mathbb{R}\), \(f\) differentiable in \((a, b)\), then there exists \(a &lt; c &lt; b\) such that \(f'(c) = \frac{f(b) - f(a)}{b - a}\).  
</p>

<p>
This is a "really cool" theorem, because it depends only on the boundaries to show an "inescapable truth" about teh interior.  
</p>

<p>
For example, look at \(h(x) = f(x) - \frac{f(b) - f(a)}{b - a}(x - a)\).  We can see easily that \(h(a) = f(a)\).  With some math, we can also see that \(h(b) = f(a)\).  By Rolle's Theorem, there exists \(a &lt; c &lt; b\), such that \(h'(c) = 0\).  
</p>

<p>
Let's look at \(h'(x)\).
</p>

\begin{align}
h'(x)       &amp;=  f'(x) - \frac{f(b) - f(a)}{b - a}       \\
h'(0)       &amp;=  0                                       \\
            &amp;=  f'(c) - \frac{f(b) - f(a)}{b - a}       \\
f'(c)       &amp;= \frac{f(b) - f(a)}{b - a}    \qquad\qquad\qquad\square
\end{align}


<div id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem-Differentiation, cont.-Example"><h5 id="Example">Example</h5></div>
<p>
Prove that \(\cos(x)\) is 1-Lipschitz.  That is, we want to show that \(|\cos(x) - \cos(y)| \leq |x - y|\).  
</p>

<p>
We apply to the mean value theorem to \(\cos(x)\).  This implies that there exists \(x &lt; c &lt; y\) such that:
</p>
\begin{align}
    -\sin(c)        &amp;=  \frac{\cos(y) - \cos(x)}{y - x}                 \\
    1 \leq |\sin(c)|&amp;=  \left|\frac{cos(y) - \cos(x)}{y - x}\right|     \\
    \lvert \cos(y) - \cos(x) \rvert     &amp;\leq \lvert y - x \rvert    
\end{align}

<p>
When you want to bound the distance between two functions, one way of doing it is to understand the distance between \(x\) and \(y\) and how the derivative behaves. 
</p>

<div id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem-Differentiation, cont.-Example"><h5 id="Example">Example</h5></div>
<p>
Suppose we are interested in \(\frac{1}{\sqrt{n + 1}}\).  
</p>

<p>
We apply the mean value theorem to \(f(x) = \frac{1}{\sqrt{x}}\).  The mean value theorem implies that there exists a \(c\) such that \(f'(c) = \frac{f(n + 1) - f(n)}{n + 1 - n}\).  
</p>

<p>
Instead of studying the boundary, we know something about the value of the derivative in the interior:
</p>
\[
    f'(x) = -\frac{1}{2}x^{-\frac{3}{2}}
\]

<p>
For all \(x \in (n, n + 1)\):
</p>
\begin{align}
    -\frac{1}{2}\frac{1}{n^{\frac{3}{2}}}       &amp;\leq       -\frac{1}{2}\frac{1}{x^{\frac{3}{2}}}       \\
                                                &amp;\leq       -\frac{1}{2}\frac{1}{(n + 1)^{\frac{3}{2}}}
\end{align}

<p>
This is all you need.  No matter where \(c\) is, we have bounds on \(f'(c)\).  This implies that 
</p>
\[
    -\frac{1}{2}\frac{1}{n^{\frac{3}{2}}}   \leq \frac{1}{\sqrt{n + 1}} - \frac{1}{\sqrt{n}}    \leq -\frac{1}{2}\frac{1}{(n + 1)^{\frac{3}{2}}}
\]

<div id="Topics in Analysis-Notes-Lecture 17; Differentiation, cont., mean value theorem-Differentiation, cont.-Example"><h5 id="Example">Example</h5></div>
<p>
Suppose we wanted to evaluate \((n + \pi)^{0.7} - n^{0.7}\).  
</p>

<p>
Let us define \(f(x) = x^{0.7}\).  The mean value theorem would then imply that there exists a \(c\) such that \(f'(c) = \frac{(n + \pi)^{0.7} - n^{0.7}}{(n + \pi) - n}\).  
</p>

<p>
So we know:
</p>
\[
    \frac{0.7\pi}{(n + \pi)^{0.3}}    \leq    (n + \pi)^{0.7} - n^{0.7}   \leq \frac{0.7\pi}{n^{0.3}}
\]

<div id="Topics in Analysis-Notes-Lecture 18; Fundamental theorem of Calculus"><h3 id="Lecture 18; Fundamental theorem of Calculus">Lecture 18; Fundamental theorem of Calculus</h3></div>
<p>
<em>Monday, February 29th</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 18; Fundamental theorem of Calculus-Review: Mean Value Theorem"><h4 id="Review: Mean Value Theorem">Review: Mean Value Theorem</h4></div>
<p>
Recall from last lecture Rolle's Theorem, which stated that for a continuous function \(f\) on a bounded interval \([a, b]\), if we have \(f(a) = f(b)\), then there exists \(a &lt; c &lt; b\) such that \(f'(c) = 0\).  
</p>

<p>
We then quickly used Rolle's Theorem to prove the Mean Value Theorem, by subtracting off \(\frac{f(b) - f(a)}{b - 1}(x - a)\) from the original function \(f\) and applying Rolle's Theorem.  
</p>

<div id="Topics in Analysis-Notes-Lecture 18; Fundamental theorem of Calculus-Review: Mean Value Theorem-Example"><h5 id="Example">Example</h5></div>
<p>
Suppose we wanted to make some statement about \(\arctan{\frac{1}{n}}\), and we knew its derivative.  
</p>

<p>
We may say that \(\arctan{\frac{1}{n}} - \arctan{0} = \frac{1}{1 + \xi^2} \cdot \frac{1}{n}\) for some \(0 &lt; \xi &lt; \frac{1}{n}\).  This follows from a modified version of the mean valued theorem: \(f'(c) \cdot (b - a) = f(b) - f(a)\).  We don't know \(\xi\), but we may place bounds:
</p>
\[
  \frac{n^2}{n^2 + 1} = \frac{1}{1 + \frac{1}{n^2}}   \leq \frac{1}{1 + \xi^2}  \leq  1
\]
<p>
We then have:
</p>
\[
  \frac{n^2}{n^2 + 1}\frac{1}{n}  \leq  \arctan{\frac{1}{n}}  \leq \frac{1}{n}
\]
<p>
And so we may conclude that \(n\arctan{\frac{1}{n}} \rightarrow 1\).
</p>

<div id="Topics in Analysis-Notes-Lecture 18; Fundamental theorem of Calculus-Review: Mean Value Theorem-Example"><h5 id="Example">Example</h5></div>
<p>
Suppose we wanted to show that \(\lim_{x \rightarrow 0} \frac{\sin{x}}{x} = 1\).  
</p>

<p>
Use the mean value theorem!
</p>

<p>
For some \(0 &lt; \xi &lt; x\), we have:
</p>
\[
    \frac{\sin{x} - \sin{0}}{x - 0}   &amp;=  \cos{\xi}         \\
    \frac{\sin{x}}{x}                 &amp;=  \cos{\xi(x)}    
\]
<p>
In the second line, we write \(\xi\) as a function of \(x\) because by definition it is constrained by \(x\), therefore in depends on \(x\).  But if \(x \rightarrow 0\), then so does \(\xi(x)\), since \(\xi\) falls between \(0\) and \(x\).  And if \(\xi(x) \rightarrow 0\), by continuity, \(\cos(\xi(x)) \rightarrow 1\).  
</p>

<div id="Topics in Analysis-Notes-Lecture 18; Fundamental theorem of Calculus-Fundamental Theorem of Calculus"><h4 id="Fundamental Theorem of Calculus">Fundamental Theorem of Calculus</h4></div>
<p>
All of the theorems presented in the days prior were building up to this theorem, but it is just the mean value theorem applied several times.  Its statement follows:
</p>
\[
    \int_a^b  f'(x) dx    =   f(b) - f(a)
\]

<p>
To show this is true, we just use a lot of small partitions.  First, we rewrite:
</p>
\[
    f(b) - f(a)   =   (f(b) - f(x_n)  +   (f(x_n) - f(x_{n - 1}))   +   ...   -   f(a)
\]
<p>
And for every pair of terms in this telescoping sum, we apply the mean value theorem:
</p>
\[
    f(x_k)  -   f(x_{k - 1})    =     f'(\xi_k)(x_k - x_{k - 1})
\]
<p>
Now, we may rewrite the original telescoping sum as follows:
</p>
\[
    f(b) - f(a)   =   f'(\xi_x)(b - x_n)  +   f'(\xi_n)(x_n - x_{n - 1})  + ... + f'(\xi_1)(x_1 - a)
\]
<p>
This is a Riemann sum!  As long as the supremem equals the infimum, we can taek any partitions, so long as they are sufficiently fine... (and other conditions).  Recall that \(f'\) is continuous, then \(f'\) is Riemann integrable.
</p>

<p>
Ultimately, if the partitions are fine enough, we may recognize the final sum we wrote as:
</p>
\[
    \int_a^b  f'(x) dx
\]
<p>
So really, the Fundamental Theorem of Calculus is just a consequence of Mean Value Theorem.
</p>

<div id="Topics in Analysis-Notes-Lecture 18; Fundamental theorem of Calculus-Dual Formulation"><h4 id="Dual Formulation">Dual Formulation</h4></div>
<p>
This is another formulation of the fundamental theorem of calculus.  Suppose we have \(f(x)\) continuous, and \(F(x) = \int_0^x f(t) dt\).  Then \(F\) is differentiable, and \(F' = f\).  
</p>

<p>
<em>Proof.</em>  
</p>

<p>
From the assumptions, we may state:
</p>
\[
    \frac{F(x + h)  -  F(x)}{h}   =   \frac[1}{h}\int_x}^{x + h} f(t) dt
\]
<p>
In the following step, we look at the mean value theorem from a different perspective.  We claim that there exists a point \(x &lt; \xi &lt; x + h\) such that \(\int_x^{x + h} f(t) dt = h \cdot f(\xi)\).  This would imply  that:
</p>
\[
    \frac{1}{h} \int_x^{x + h}  f(t) dt = f(\xi)
\]
<p>
We have little to no hope of understanding the term on the left; however, we can understand the term on the right.  
</p>

<p>
We proceed using the intermediate value theorem, and consider two cases.
</p>

<p>
The first case is when \(f\) is constant and continuous.  Then our claim is trivially true.
</p>

<p>
In the second case, we assume \(f\) is not constant, and we claim thaht there exists \(x &lt; \xi_1 x + h\) for which:
</p>
\[
    h \cdot f(\xi_1)  &lt;   \int_x^{x + h}  f(t) dt 
\]
<p>
This is easy to prove by contradiction.  Suppose that for all \(x &lt; \xi &lt; x + h\), \(f(\xi) &gt; \frac{1}{n}\int_x^{x + h} f(t) dt\).  Then the stated average would no longer be true.  To formalize, let \(a_1, ..., a_n \in \mathbb{R}\), and define \(\bar a = \frac{1}{n} \sum_{k = 1}^n a_k\).  Either all of the \(a_k = \bar a\), or there exists \(a_i &lt; \bar a &lt; a_j\), which is evident if we assume the claim to be false, and state \(\bar a = \frac{1}{n} \sum a_i &gt; \frac{1}{n} \bar a = \bar a\).  
</p>

<p>
Therefore, we may conclude that with the existence of a necessary \(x &lt; \xi &lt; x + h\), as \(h \rightarrow 0\), we have \(f(\xi) \rightarrow f(x)\), which proves that \(F' = f\).  
</p>

<p>
<em>My record of this proof is a bit shaky.  An authoritative text should be consulted.</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin"><h3 id="Lecture 19; Approximating area, Taylor and MacLaurin">Lecture 19; Approximating area, Taylor and MacLaurin</h3></div>
<p>
<em>Wednesday, March 2nd</em>
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-Approximating area"><h4 id="Approximating area">Approximating area</h4></div>
<p>
Suppose \(\int_a^b f(x) dx\) cannot be computed.  To work around this, we use numerical methods.
</p>

<p>
In the following discussion, we are considering a continuous function \(f\) on the bounded interval \([c, d]\).
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-Approximating area-Endpoint rule"><h5 id="Endpoint rule">Endpoint rule</h5></div>
<p>
Using rectangles, this is one of the cruder approximations (and the most inaccurate we study).  
</p>

\[
    \int_c^d  \approx
      \begin{cases}
        &amp;f(c) (d - c)       \\
        &amp;f(d) (d - c)
      \end{cases}
\]

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-Approximating area-Midpoint rule"><h5 id="Midpoint rule">Midpoint rule</h5></div>
<p>
Instead of using the value of either endpoint, we take the value at the midpoint.  
</p>

\[
  \int_c^d  \approx   f\left(\frac{c + d}{2}\right) \cdot (d - c)
\]

<p>
Note that this rule is exact for a linear function.  On a small enough scale, it is a linear (which is the idea behind differentiability).
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-Approximating area-Trapezoidal rule"><h5 id="Trapezoidal rule">Trapezoidal rule</h5></div>
<p>
Here, we approximate with a linear function with the same value at the endpoints as \(f\), the function we are approximating.  
</p>

<p>
First, we define:
</p>
\[
    l(x)  =   f(c) + \frac{(f(d) - f(c))(x - c)}{d - c}
\]

<p>
Then, we have the approximation:
</p>
\begin{align}
    \int_c^d  f(x)  dx  &amp;\approx   \int_c^d  l(x)  dx                                                       \\
                        &amp;=          \int_c^d f(c) +   \left(\frac{f(d) - f(c)}{d - c}\right)(x - c) dx      \\
                        &amp;=          \frac{f(c) + f(d)}{2}(d - c)
\end{align}

<p>
This approximation works well for periodic functions.
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-Approximating area-Simpson&#39;s Rule"><h5 id="Simpson&#39;s Rule">Simpson's Rule</h5></div>
<p>
Although in English named for Thomas Simpson, this method of approximation was known by Johannes Kepler in 1615 and Torricelli in 1608.  
</p>

<p>
The idea behind this rule is to approximate with quadratic polynomials.  We find \(l(x) = ax^2 + xb + c\), selecting the coefficients such that we have the same endpoints and extrumum.  
</p>

<p>
Perhaps surprisingly, the approximation is a sum of linear functions:
</p>
\[
    \int_c^d  f(x) dx =   \frac{d - c}{6}\left(f(c) + f\left(\frac{c + d}{2}\right) + f(d)\right)
\]

<p>
Legend has it that Kepler devised this approximation on his wedding day, since he was dissatisfied with the wineseller's approximation of the volume of a barrel.  
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-Taylor&#39;s Theorem"><h4 id="Taylor&#39;s Theorem">Taylor's Theorem</h4></div>
<p>
The fundamental idea behind Taylor's theorem is <em>approximation by polynomials</em>.  
</p>

<p>
Suppose that we naively select twenty points and fit a degree twenty polynomial over those points.  This is not a good idea, as we would get perfect approximations at those points, but the polynomial may behave wildly in between (this is referred to as overfitting in statistics).  
</p>

<p>
Instead, we want to match many degrees of <em>derivatives</em> at a point, which would help guaranteed a good approximation near that point.
</p>

<p>
For example, if we had at \(x_0\) an approximating polynomial \(p\), 
</p>
\begin{align}
    p(x_0)    &amp;=  f(x_0)          \\
    p'(x_0)    &amp;=  f'(x_0)          \\
    p''(x_0)    &amp;=  f''(x_0)          \\
              &amp;...                \\
    p^{(n)}(x_0)    &amp;=  f^{(n)}(x_0)          \\
\end{align}
<p>
would yield a Taylor polynomial of degree \(n\) at \(x_0\).  
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-MacLaurin Series"><h4 id="MacLaurin Series">MacLaurin Series</h4></div>
<p>
The only difference between Taylor Series and MacLaurin Series is that MacLaurin evaluated at \(x_0 = 0\), whereas Taylor generalized the theorem.  For brevity, we prove the case for a MacLaurin series and claim that it generalizes.
</p>

<p>
We want a polynomial \(a + bx + cx^2 + ...\) which is a good approximation of \(f\) at \(x_0\).  We make the following evaluations:
</p>

\begin{align}
    p(0)  =  f(0)    &amp;\Rightarrow     p(0) = a = f(0)        \\
    p'(0) = f'(0)    &amp;\Rightarrow     p'(0) = b = f'(0)      \\
    p''(0) = f''(0)  &amp;\Rightarrow         2c = f''(0)        \\
\end{align}

<p>
Thus, we find the MacLaurin series of degree \(n\), we need only solve for \(n\) coefficients.  
</p>

<p>
With \(f(x)\) given, we define 
</p>
\begin{align}
    p(x) &amp;= f(0) + f'(0)x + \frac{f''(0)}{2}x^2 + \frac{f'''(0)}{6}x^3 + \frac{f^{(4)}(0)}{4!}x^4 + ...     \\
         &amp;= \sum_{k = 0}{\infty}  \frac{f^{(k)}(0)}{k!}x^k
\end{align}

<p>
And a Taylor approximation about \(x_0\) is given by:
</p>
\[
    p(x)  &amp;=  \sum_{k = 0}^{\infty} \frac{f^{(k)}(x_0)}{k!}(x - x_0)^k
\]

<p>
But does it converge?
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-MacLaurin Series-Some remarks on Taylor&#39;s theorem"><h6 id="Some remarks on Taylor&#39;s theorem">Some remarks on Taylor's theorem</h6></div>
<p>
Note that \(f: \mathbb{R} \rightarrow \mathbb{R}\) is differentiable as many times as we wish.  For a degree \(n \in \mathbb{N}\), the Taylor approximation is given by:
</p>
\[
  p(t)  = \sum_{k = 0}^{n - 1}  \frac{f^{(k)}(\alpha)}{k!}(t - \alpha)^k
\]

<p>
There is an error that we should be aware of.  How should we quantify it?
</p>

<p>
<span id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-MacLaurin Series-Some remarks on Taylor&#39;s theorem--Theorem."></span><strong id="Theorem.">Theorem.</strong>  There exists \(\alpha &lt; x &lt; \beta\) such that $f(\beta) + p(\beta) + \frac{f<sup><small>{(n)}(x)}{n!} (\beta - \alpha)</small></sup>n
</p>

<p>
Proof?
</p>

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-MacLaurin Series-Example"><h6 id="Example">Example</h6></div>
<p>
Consider \(f(x) = \sin(x)\) on the closed interval \([0, \beta]\).  We may say:
</p>
\[
    \left| \frac{f^{(n)}(0)}{n!}\beta^n \right| \leq  \frac{\beta^n}{n!}  \leq  \varepsilon
\]

<p>
If we took \(n = 1\), then we have:
</p>

\begin{align}
p(t)    &amp;= \sum_{k = 0}^{n - 1} \frac{f^{(k)}(\alpha)}{k!}(t - \alpha)^k        \\
        &amp;= \frac{f^{(0)}(\alpha)}{1}(t - \alpha)^0                              \\
        &amp;= f(\alpha)
\end{align}

<div id="Topics in Analysis-Notes-Lecture 19; Approximating area, Taylor and MacLaurin-MacLaurin Series-Specific case: mean value theorem"><h5 id="Specific case: mean value theorem">Specific case: mean value theorem</h5></div>
<p>
Note that Taylor's theorem implies that if we take the degree to be one, then Taylor's theorem implies that there exists an \(\alpha &lt; x &lt; \beta\) such that:
</p>
\begin{align}
    f(\beta)    &amp;=  p(\beta)  + f'(x) (\beta - \alpha)        \\
                &amp;=  f(\alpha) + f'(x) (\beta - \alpha)        \\
    f'(x)       &amp;=  \frac{f(\beta) - f(\alpha)}{\beta - \alpha}
\end{align}

</body>
</html>
